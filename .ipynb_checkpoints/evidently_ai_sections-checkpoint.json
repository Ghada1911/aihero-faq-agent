[
  {
    "title": "Create Plant",
    "openapi": "POST /plants",
    "filename": "docs-main/api-reference/endpoint/create.mdx",
    "ai_section": "## Section Title\nSection text with all relevant details."
  },
  {
    "title": "Create Plant",
    "openapi": "POST /plants",
    "filename": "docs-main/api-reference/endpoint/create.mdx",
    "ai_section": "## Another Section\nAnother section text."
  },
  {
    "title": "Delete Plant",
    "openapi": "DELETE /plants/{id}",
    "filename": "docs-main/api-reference/endpoint/delete.mdx",
    "ai_section": "## Document Overview\nThis document provides a comprehensive guide on various topics, including user instructions, troubleshooting tips, and frequently asked questions."
  },
  {
    "title": "Delete Plant",
    "openapi": "DELETE /plants/{id}",
    "filename": "docs-main/api-reference/endpoint/delete.mdx",
    "ai_section": "## User Instructions\nIn this section, users will find detailed instructions on how to navigate the system, access features, and utilize the platform effectively."
  },
  {
    "title": "Delete Plant",
    "openapi": "DELETE /plants/{id}",
    "filename": "docs-main/api-reference/endpoint/delete.mdx",
    "ai_section": "## Troubleshooting Tips\nHere, users can find solutions to common issues they may encounter while using the platform, including error messages and performance problems."
  },
  {
    "title": "Delete Plant",
    "openapi": "DELETE /plants/{id}",
    "filename": "docs-main/api-reference/endpoint/delete.mdx",
    "ai_section": "## Frequently Asked Questions (FAQs)\nThis section addresses the most common inquiries users have, providing clear answers to enhance understanding and usage of the document's contents."
  },
  {
    "title": "Delete Plant",
    "openapi": "DELETE /plants/{id}",
    "filename": "docs-main/api-reference/endpoint/delete.mdx",
    "ai_section": "## System Requirements\nInformation about the hardware and software specifications needed to run the system optimally is provided in this section."
  },
  {
    "title": "Delete Plant",
    "openapi": "DELETE /plants/{id}",
    "filename": "docs-main/api-reference/endpoint/delete.mdx",
    "ai_section": "## Support Contact Information\nUsers can find the necessary contact details for technical support, including email addresses, phone numbers, and support hours."
  },
  {
    "title": "Get Plants",
    "openapi": "GET /plants",
    "filename": "docs-main/api-reference/endpoint/get.mdx",
    "ai_section": "Sure! Please provide the document you would like me to split into sections."
  },
  {
    "title": "Introduction",
    "description": "Example section for showcasing API endpoints",
    "filename": "docs-main/api-reference/introduction.mdx",
    "ai_section": "## Welcome\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\n\n<Card\n  title=\"Plant Store Endpoints\"\n  icon=\"leaf\"\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\n>\n  View the OpenAPI specification file\n</Card>"
  },
  {
    "title": "Introduction",
    "description": "Example section for showcasing API endpoints",
    "filename": "docs-main/api-reference/introduction.mdx",
    "ai_section": "## Authentication\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\n\n```json\n\"security\": [\n  {\n    \"bearerAuth\": []\n  }\n]\n```"
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.11 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).\n\nExample notebooks:\n- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)"
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.10 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).\n\nNEW: automated prompt optimization. Read the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\n\nExample notebooks:\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)"
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.9 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.8 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.7 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.6 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.6)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.5 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.4 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.3 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.2 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.1 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.7.0 Release Changes\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.6.7 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.6.6 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.6.5 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.6.4 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.6.3 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.6.2 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.2). We extended support for `litellm`, so you can easily use different providers like Gemini, Anthropic, etc. for LLM-based evaluations."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.6.1 Release Notes\nFull release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.1)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Evidently 0.6.0 API Release\nThe new API is available when you import modules from `evidently.future`. Read more in [Migration guide](/faq/migration). Release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.0)."
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## Editable Datasets in Evidently Cloud\nYou can now hit \"edit\" on any existing dataset, create a copy, and add/delete rows and columns. Use it while working on your evaluation datasets or to leave comments on outputs.\n\n![](/images/changelog/editable_dataset-min.png)"
  },
  {
    "title": "Product updates",
    "description": "Latest releases.",
    "filename": "docs-main/changelog/changelog.mdx",
    "ai_section": "## New Documentation Website\nWe are creating a new Docs website in anticipation of API changes. You can still access old docs for information on earlier API and examples."
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Evaluations Overview\nTo run evaluations, create a `Dataset` object with a `DataDefinition`, mapping column types and roles. This ensures Evidently processes the data correctly. For certain evaluations, specific columns are necessary. The mapping can be defined using the Python API or visually in the Evidently platform when uploading data."
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Basic Flow to Create a Dataset\n**Step 1. Imports:**  \nImport the necessary modules:\n\n```python\nfrom evidently import Dataset\nfrom evidently import DataDefinition\n```\n\n**Step 2. Prepare your data:**  \nUtilize a `pandas.DataFrame`. Your data can have a flexible structure with a mix of column types. \n\n**Step 3. Create a Dataset object:**  \nUse `Dataset.from_pandas` with `data_definition` to initialize the object.\n\n```python\neval_data = Dataset.from_pandas(\n    source_df,\n    data_definition=DataDefinition()\n)\n```\n\n**Step 4. Run evaluations:**  \nOnce the **Dataset** object is set up, add Descriptors and run Reports."
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Special Cases in Dataset Usage\n**Working directly with pandas.DataFrame:**  \nYou may pass a `pandas.DataFrame` directly to `report.run()` for simple checks. However, creating a `Dataset` object is recommended for clarity.\n\n**Working with two datasets:**  \nWhen working with current and reference datasets for drift detection, ensure both have identical data definitions."
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Understanding Data Definition\nThis section outlines mapping options for a `DataDefinition`. Only use the relevant mappings for your evaluation scenario."
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Column Types in Data Definition\nUnderstanding column types ensures correct statistics, visualizations, and default tests.\n\n### Text Data\nSpecify input/output columns as text for LLM evaluations.\n\n```python\ndefinition = DataDefinition(\n    text_columns=[\"Latest_Review\"]\n)\n```\n\n### Tabular Data\nMap numerical, categorical, or datetime columns explicitly to avoid misclassification. \n\n```python\ndefinition = DataDefinition(\n    text_columns=[\"Latest_Review\"],\n    numerical_columns=[\"Age\", \"Salary\"],\n    categorical_columns=[\"Department\"],\n    datetime_columns=[\"Joining_Date\"]\n)\n```"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Default Column Types\nIf no explicit mapping is provided, the following defaults apply:\n\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\n|"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "|"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "- |"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "|\n| `numerical_columns`   | Columns containing numeric values.                                                                                                   | All `np.number` type columns.                       |\n| `datetime_columns`    | Columns containing datetime values.                                                                                                   | All `np.datetime64` type columns.                  |\n| `categorical_columns` | Columns containing categorical values.                                                                                                | All non-numeric/non-datetime columns.               |\n| `text_columns`        | Text columns that require mapping for drift detection.                                                                               | No automated mapping.                               |"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## ID and Timestamp Columns\nIf you have an ID or timestamp column, it's beneficial to identify them within the `DataDefinition`.\n\n```python\ndefinition = DataDefinition(\n    id_column=\"Id\",\n    timestamp=\"Date\"\n)\n```"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## LLM Evaluations and Descriptors\nWhen generating text descriptors, they are automatically included in the Data Definition. If external scores or metadata are computed, map them explicitly in the `DataDefinition`.\n\n```python\ndefinition = DataDefinition(\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\n)\n```"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Regression Checks\nTo run regression checks, map the target and prediction columns.\n\n```python\ndefinition = DataDefinition(\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\n)\n```\n\nDefaults are:\n\n```python\n    target: str = \"target\"\n    prediction: str = \"prediction\"\n```"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Classification Checks\nFor classification checks, target and predicted labels/probabilities must be mapped appropriately.\n\n### Multiclass Example Mapping\n```python\ndata_def = DataDefinition(\n    classification=[MulticlassClassification(\n        target=\"target\",\n        prediction_labels=\"prediction\",\n        prediction_probas=[\"0\", \"1\", \"2\"],\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}\n    )]\n)\n```\n\n### Binary Example Mapping\n```python\ndefinition = DataDefinition(\n    classification=[BinaryClassification(\n        target=\"target\",\n        prediction_labels=\"prediction\")],\n    categorical_columns=[\"target\", \"prediction\"]\n)\n```"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "## Ranking Evaluations for Recommender Systems\nMap columns with prediction and relevance labels for recommender system evaluations. Predictions can either be scores or ranks.\n\n### Example for Score Predictions\n| user_id | item_id | prediction (score) | target (relevance) |\n|"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "|\n| user_1  | item_1  | 1.95               | 0                  |\n| user_1  | item_2  | 0.8                | 1                  |\n\n### Example for Rank Predictions\n| user_id | item_id | prediction (rank) | target (relevance) |\n|"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Data definition",
    "description": "How to map the input data.",
    "filename": "docs-main/docs/library/data_definition.mdx",
    "ai_section": "|\n| user_1  | item_1  | 1                 | 0                  |\n| user_1  | item_2  | 2                 | 1                  |\n\nExample mapping could be:\n\n```python\ndefinition = DataDefinition(\n    ranking=[Recsys()]\n)\n```"
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Evaluating Text Data with Descriptors\nTo evaluate text data, like LLM inputs and outputs, you create **Descriptors**. This universal interface allows various evaluations, ranging from text statistics to LLM judges. Each descriptor computes a score or label for each row in your dataset, and you can combine multiple descriptors with optional pass/fail conditions. Custom descriptors can be created using LLM prompts or Python."
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Basic Flow for Creating Sample Data\nTo generate sample data for testing, you can use the provided Python code snippet with pandas. This snippet creates a DataFrame containing sample questions and answers, which can be utilized for further evaluation.\n\n```python\nimport pandas as pd\n\ndata = [\n    [\"What is the chemical symbol for gold?\", \"The chemical symbol for gold is Au.\"],\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\n    ...\n]\n\ncolumns = [\"question\", \"answer\"]\ndf = pd.DataFrame(data, columns=columns)\n```"
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Importing Required Modules\nFor conducting evaluations, you need to import specific modules from evidently:\n\n```python\nfrom evidently import Dataset\nfrom evidently import DataDefinition\nfrom evidently import Report\n\nfrom evidently.descriptors import *\nfrom evidently.presets import TextEvals\n```\n\n**Note**: Some descriptors may require downloading NLTK dictionaries."
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Adding Descriptors to Dataset\nYou can add descriptors to the Dataset object in two ways: during the creation of the Dataset or afterwards with the `add_descriptors` method.\n\n**Option A**: Create the `Dataset` and add descriptors simultaneously.\n\n```python\neval_dataset = Dataset.from_pandas(\n    df,\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\n    descriptors=[\n        Sentiment(\"answer\", alias=\"Sentiment\"),\n        TextLength(\"answer\", alias=\"Length\"),\n        ...\n    ]\n)\n```"
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Exporting Results from Dataset\nYou can preview the DataFrame with results using the following command:\n\n```python\neval_dataset.as_dataframe()\n```\n\nAdditionally, you can generate a report that summarizes the results for all descriptors."
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Generating Evaluation Reports\nTo summarize results capturing stats and distributions for all descriptors, use the `TextEvals` Preset to configure and run the report for your dataset.\n\n```python\nreport = Report([\n    TextEvals()\n])\nmy_eval = report.run(eval_dataset)\n```\n\nYou can choose to export the report in various formats such as HTML, JSON, or Python dictionary."
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Customizing Descriptors\nEvidently provides several descriptors with optional parameters. Adding an `alias` to each descriptor is recommended for easier reference in visualizations. \n\n```python\neval_dataset.add_descriptors(descriptors=[\n    WordCount(\"answer\", alias=\"Words\"),\n])\n```\n\nYou can also create multi-column descriptors for evaluations that involve more than one text column."
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Implementing Descriptor Tests\nDescriptor Tests allow you to define specific pass/fail conditions for each row. For instance, you can check if the text length is under a certain limit or that sentiment is positive.\n\n**Step 1**: Import necessary modules.\n\n```python\nfrom evidently.descriptors import ColumnTest, TestSummary\n```\n\n**Step 2**: Add tests while creating a descriptor.\n\n```python\neval_dataset = Dataset.from_pandas(\n    df,\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\n    descriptors=[\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[gte(0, alias=\"Sentiment is non-negative\")]),\n        ...\n    ]\n)\n```"
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Creating Summary Reports\nIn addition to using `TextEvals`, you can create custom reports using different metric combinations for enhanced control.\n\n**Imports**: Import the necessary components.\n\n```python\nfrom evidently import Report\nfrom evidently.presets import TextEvals\nfrom evidently.metrics import *\n```\n\nYou can also run advanced checks, such as detecting text length drift by comparing two datasets."
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Dataset-Level Test Suites\nYou can attach tests to your metrics to receive pass/fail results at the dataset report level. Examples of such tests include validating that no response has a sentiment below a certain threshold.\n\n```python\ntests = Report([\n    ...\n])\n```"
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Custom Metrics Evaluation\nYou can create custom evaluations using metrics such as `MeanValue` and `MaxValue` to analyze your dataset. For example, you can compute the mean sentiment score and the length of responses:\n\n```python\ncustom_report = Report([\n    MeanValue(column=\"Length\"),\n    MeanValue(column=\"Sentiment\")\n])\n\nmy_custom_eval = custom_report.run(eval_dataset, None)\nmy_custom_eval\n```"
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Drift Detection\nDrift detection allows you to compare distributions between two datasets to identify changes, such as variations in text length. For instance, you can detect length drift as follows:\n\n```python\ncustom_report = Report([\n    ValueDrift(column=\"Length\"),\n])\n\nmy_custom_eval = custom_report.run(eval_dataset, eval_dataset)\nmy_custom_eval\n```"
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Dataset-level Test Suites\nYou can create test suites attached to your metrics for pass/fail results at the dataset report level. Example tests include:\n\n- No response has sentiment < 0\n- No response exceeds 150 characters\n- No more than 10% of rows fail the summary test\n\n```python\ntests = Report([\n    MinValue(column=\"Sentiment\", tests=[gte(0)]),\n    MaxValue(column=\"Length\", tests=[lte(150)]),\n    CategoryCount(column=\"Test result\", category=False, share_tests=[lte(0.1)])\n])\n\nmy_test_eval = tests.run(eval_dataset, None)\nmy_test_eval\n# my_test_eval.json()\n```\n\nThis produces a Test Suite that provides clear pass/fail results, useful for automated checks and regression testing."
  },
  {
    "title": "Descriptors",
    "description": "How to run evaluations for text data.",
    "filename": "docs-main/docs/library/descriptors.mdx",
    "ai_section": "## Report and Tests API\nFor more detailed information, refer to the guides on [generating Reports](/docs/library/report) and setting [Test conditions](/docs/library/tests)."
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## Core Evaluation Workflow\nThis page shows the core eval workflow with the Evidently library and links to guides."
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## Define and Run the Eval\nTo log the evaluation results to the Evidently Platform, first connect to [Evidently Cloud](/docs/setup/cloud) or your [local workspace](/docs/setup/self-hosting) and [create a Project](/docs/platform/projects_manage). It's optional: you can also run evals locally."
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## Prepare the Input Data\nGet your data in a table like a `pandas.DataFrame`. More on [data requirements](/docs/library/overview#dataset). You can also [load data](/docs/platform/datasets_workflow) from Evidently Platform, like tracing or synthetic datasets."
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## Create a Dataset Object\nCreate a Dataset object with `DataDefinition()` that specifies column role and types. You can also use default type detection. [How to set Data Definition](/docs/library/data_definition).\n\n```python\neval_data = Dataset.from_pandas(\n    source_df,\n    data_definition=DataDefinition()\n)\n```"
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## (Optional) Add Descriptors\nFor text evals, choose and compute row-level `descriptors`. Optionally, add row-level tests to get pass/fail for specific inputs. [How to use Descriptors](/docs/library/descriptors).\n\n```python\neval_data.add_descriptors(descriptors=[\n    TextLength(\"Question\", alias=\"Length\"),\n    Sentiment(\"Answer\", alias=\"Sentiment\")\n])\n```"
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## Configure Report\nFor dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics` or `presets`. How to [configure Reports](/docs/library/report).\n\n```python\nreport = Report([\n    DataSummaryPreset()\n])\n```"
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## (Optional) Add Test Conditions\nAdd dataset-level Pass/Fail conditions, like to check if all texts are in < 100 symbols length. How to [configure Tests](/docs/library/tests).\n\n```python\nreport = Report([\n    DataSummaryPreset(),\n    MaxValue(column=\"Length\", tests=[lt(100)]),\n])\n```"
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## (Optional) Add Tags and Timestamps\nAdd `tags` or `metadata` to identify specific evaluation runs or datasets, or override the default `timestamp`. [How to add metadata](/docs/library/tags_metadata)."
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## Run the Report\nTo execute the eval, `run` the Report on the `Dataset` (or two).\n\n```python\nmy_eval = report.run(eval_data, None)\n```"
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## Explore the Results\n* To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).\n\n```python\nws.add_run(project.id, my_eval, include_data=True)\n```\n\n* To view locally. [All output formats](/docs/library/output_formats).\n\n```python\nmy_eval\n##my_eval.json()\n```"
  },
  {
    "title": "Overview",
    "description": "End-to-end evaluation workflow.",
    "filename": "docs-main/docs/library/evaluations_overview.mdx",
    "ai_section": "## Quickstarts\nCheck for end-to-end examples:\n\n<CardGroup cols={2}>\n  <Card title=\"LLM quickstart\" icon=\"comment-text\" href=\"/quickstart_llm\">\n    Evaluate the quality of text outputs.\n  </Card>\n\n  <Card title=\"ML quickstart\" icon=\"table\" href=\"/quickstart_ml\">\n    Test tabular data quality and data drift.\n  </Card>\n</CardGroup>"
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Metrics Overview\nThis document outlines the metrics used for data quality checks and analysis, particularly in the context of exploratory data analysis and drift monitoring. It discusses how to evaluate the correlation between scores, missing values, and other key data quality indicators."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Correlation Metrics\nCorrelation metrics are crucial for exploratory data analysis and monitoring drift in datasets. These metrics can help check the alignment between scores, such as LLM-based descriptors against human labels."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Column Data Quality Metrics\nWithin the column data quality metrics, several key metrics are outlined, all focusing on dataset-level and column-level checks for missing values, empty columns, and category counts. \n\n### RowsWithMissingValuesCount()\n- **Description**: Counts rows with missing values.\n- **Parameters**: Optional test conditions.\n- **Test Defaults**: Fails if there is at least one row with missing values or if the share differs by more than 10% when a reference is provided."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### AlmostEmptyColumnCount()\n- **Description**: Counts almost empty columns (95% empty).\n- **Parameters**: Optional test conditions.\n- **Test Defaults**: Fails if there is at least one almost empty column or if the count is higher than in the reference."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### NewCategoriesCount()\n- **Description**: Counts new categories compared to a reference.\n- **Parameters**: Requires a specified column; optional test conditions.\n- **Test Defaults**: Expects 0 new categories."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### MissingCategoriesCount()\n- **Description**: Counts missing categories compared to a reference.\n- **Parameters**: Requires a specified column; optional test conditions.\n- **Test Defaults**: Expects 0 missing categories."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### MostCommonValueCount()\n- **Description**: Identifies and provides the count and share of the most common value in the column.\n- **Parameters**: Requires a specified column; optional test conditions.\n- **Test Defaults**: Fails if the most common value share is â‰¥80% or differs by more than 10% from the reference."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Drift Metrics\nDrift metrics are essential for evaluating changes in the dataset over time, particularly regarding embedding and multivariate correlations.\n\n### EmbeddingDrift()\n- **Description**: Calculates data drift for embeddings.\n- **Parameters**: Requires specifying embeddings and method.\n- **Test Defaults**: Defaults for method apply when a reference is provided."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### MultivariateDrift()\n- **Description**: Computes a single score indicating dataset drift.\n- **Parameters**: Optional specification of columns and method.\n- **Test Defaults**: Defaults for method apply when a reference is provided."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### DatasetCorrelations()\n- **Description**: Calculates correlations between all or specified columns in the dataset using various methods (Pearson, Spearman, etc.).\n- **Parameters**: Optional specification of columns and test conditions.\n- **Test Defaults**: Not applicable."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### Correlation()\n- **Description**: Computes the correlation between two specified columns.\n- **Parameters**: Requires specification of two columns; optional method and test conditions.\n- **Test Defaults**: Not applicable."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### CorrelationChanges()\n- **Description**: Checks for significant changes in correlation strength between columns.\n- **Parameters**: Optional specification of columns, method, and correlation difference threshold.\n- **Test Defaults**: Fails if at least one correlation violation is detected with a reference."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Classification Metrics\nClassification metrics aim to evaluate the performance of predictive models, specifically focusing on class distributions and lift calculations.\n\n### LabelCount()\n- **Description**: Analyzes the distribution of predicted classes, useful for visualizing class balance and probability distribution.\n- **Parameters**: Requires at least one visualization type; optional test conditions.\n- **Test Defaults**: Not applicable."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "### Lift()\n- **Description**: Calculates the lift of predictions, which can be visualized through lift curves or tables.\n- **Parameters**: Requires at least one visualization type.\n- **Test Defaults**: Not applicable."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## LabelCount()\nThe **LabelCount()** metric, which is coming soon, helps visualize the distribution of predicted classes. It allows users to view class balance and/or probability distribution.  \n**Required**: Users must set at least one visualization: `class_balance` or `prob_distribution`.  \n**Optional**: Test conditions can be included as referenced in [Test conditions](/docs/library/tests).  \n**Notes**: N/A"
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Lift()\nThe **Lift()** metric, which is coming soon, calculates lift and allows for visualization of the lift curve or table. The metric result is a `value`.  \n**Required**: Users must set at least one visualization: `lift_table` or `lift_curve`.  \n**Optional**: Additional parameters such as `probas_threshold`, `top_k`, and test conditions can be referenced as per [Test conditions](/docs/library/tests).  \n**Notes**: N/A"
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## RecSysPreset()\nThe **RecSysPreset()** metric provides a large preset that includes a range of recommendation system metrics. The metric result encompasses all metrics involved. For detailed information, users can refer to the [Preset page](/metrics/preset_recsys).  \n**Parameters**: There are no specific parameters required for this metric.  \n**Test Defaults**: The defaults are as per individual metrics."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Personalization\nThe **Personalization()** metric (coming soon) calculates the personalization score at the top K recommendations, with the result being a `value`.  \n**Required**: The parameter `k` must be set.  \n**Optional**: Test conditions can be referenced as detailed in [Test conditions](/docs/library/tests).  \n**Test Defaults**:  \n- **No reference**: Tests if Personalization > 0.  \n- **With reference**: Fails if Personalization differs by >10%."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## ARP\nThe **ARP()** metric (coming soon) computes the Average Recommendation Popularity at the top K recommendations and requires a training dataset. The result is a `value`.  \n**Required**: The parameter `k` must be set.  \n**Optional**: Users can include `normalize_arp` (default: `False`) and test conditions as referenced in [Test conditions](/docs/library/tests).  \n**Test Defaults**:  \n- **No reference**: Tests if ARP > 0.  \n- **With reference**: Fails if ARP differs by >10%."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Coverage\nThe **Coverage()** metric (coming soon) calculates coverage at the top K recommendations and requires a training dataset, resulting in a `value`.  \n**Required**: Users must set the parameter `k`.  \n**Optional**: Test conditions can be included as referenced in [Test conditions](/docs/library/tests).  \n**Test Defaults**:  \n- **No reference**: Tests if Coverage > 0.  \n- **With reference**: Fails if Coverage differs by >10%."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## GiniIndex\nThe **GiniIndex()** metric (coming soon) calculates the Gini Index at the top K recommendations, requiring a training dataset, and results in a `value`.  \n**Required**: The parameter `k` must be set.  \n**Optional**: Test conditions can be referenced as detailed in [Test conditions](/docs/library/tests).  \n**Test Defaults**:  \n- **No reference**: Tests if Gini Index < 1.  \n- **With reference**: Fails if Gini Index differs by >10%."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Diversity\nThe **Diversity()** metric (coming soon) calculates diversity at the top K recommendations and requires item features, resulting in a `value`.  \n**Required**: Users must set both `k` and `item_features`.  \n**Optional**: Test conditions can be referenced in [Test conditions](/docs/library/tests).  \n**Test Defaults**:  \n- **No reference**: Tests if Diversity > 0.  \n- **With reference**: Fails if Diversity differs by >10%."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Serendipity\nThe **Serendipity()** metric (coming soon) calculates serendipity at the top K recommendations and requires a training dataset, resulting in a `value`.  \n**Required**: Users must set both `k` and `item_features`.  \n**Optional**: The `min_rel_score` can be defined, along with test conditions from [Test conditions](/docs/library/tests).  \n**Test Defaults**:  \n- **No reference**: Tests if Serendipity > 0.  \n- **With reference**: Fails if Serendipity differs by >10%."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Novelty\nThe **Novelty()** metric (coming soon) calculates novelty at the top K recommendations, requiring a training dataset, with the result being a `value`.  \n**Required**: Users must set the parameter `k`.  \n**Optional**: Test conditions can be referenced from [Test conditions](/docs/library/tests).  \n**Test Defaults**:  \n- **No reference**: Tests if Novelty > 0.  \n- **With reference**: Fails if Novelty differs by >10%."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Relevant Parameters for RecSys Metrics\n- `no_feedback_user: bool = False`: Specifies whether to include users who did not select any items when computing the quality metric. Default: False.\n- `min_rel_score: Optional[int] = None`: Defines the minimum relevance score to consider relevant when calculating quality metrics for non-binary targets (e.g., ratings or custom scores)."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Diversity Explained\n**Evidently Metric**: `Diversity`  \nThis metric measures the average intra-list diversity at K, reflecting the range of items within a single user's recommendation list, averaged across all users.  \n\n### Methodology:\n1. **Measure the difference between recommended items**: Calculate the Cosine distance for each pair of recommendations within the top-K in each user's list, defined by:\n   $$ \\text{Cosine distance} = 1 - \\text{Cosine Similarity} $$\n2. **Intra-list diversity**: Calculate this for each user by averaging the Cosine Distance between pairs.\n3. **Overall diversity**: Average intra-list diversity across all users.\n\n**Range**: From 0 to 2.  \n**Interpretation**: Higher values indicate more varied items shown to users.  \n\n**Requirements**: Must pass an `item_features` list to compare the similarity between different items."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Novelty Explained\n**Evidently Metric**: `Novelty`  \nThis metric measures the average novelty of recommendations at K, reflecting how unexpected top-K items are shown to users.\n\n### Methodology:\n1. **Measure the novelty of recommended items**: Defined by:\n   $$ \\text{novelty}_i = -\\log_2(p_i) $$ \n   where `p` is the probability that item `i` appears in the training set.\n2. **Measure by user**: Compute average item novelty at K.\n3. **Overall novelty**: Average the novelty per user across all users.\n\n**Range**: 0 to infinity.  \n**Interpretation**: Higher values indicate more unusual items shown to users."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Serendipity Explained\n**Evidently Metric**: `Serendipity`  \nThis metric measures how unusual relevant recommendations are within K, averaged across all users.\n\n### Methodology:\n1. **Measure unexpectedness**: Using Cosine distance, compute distance between relevant recommendations and prior user interactions:\n   $$ \\text{serendipity}_i = \\text{unexpectedness}_i \\times \\text{relevance}_i $$\n2. **Calculate averages**: For all relevant recommendations, compute averages.\n\n**Notes**: Higher values indicate relevant items that are unexpected for users."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Serendipity in Recommendations\nSerendipity reflects the ability of a recommender system to show relevant items that are unexpected based on user history. For instance, a user who typically enjoys comedies might be recommended a thriller and upvotes it. \n\n**Implemented Method**:\n- Measure the **unexpectedness** of relevant recommendations using Cosine distance. The higher the distance, the greater the unexpectedness.\n- The formula to calculate serendipity is:  \n  $$\\text{serendipity}_i = \\text{unexpectedness}_i \\times \\text{relevance}_i$$\n- Calculate average unexpectedness for each user and overall by averaging results across all users.\n\n**Range**: 0 to 2. A value of 0 means only expected recommendations, while 2 indicates completely unexpected relevant recommendations. \n\n**Interpretation**: Higher values suggest a better capability of the system to pleasantly surprise users."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Personalization of Recommendations\nPersonalization measures the average uniqueness of each user's recommendations in the top-K list.\n\n**Implemented Method**:\n- Compute the overlap between top-K recommended items for every two users.\n- Calculate the average overlap and derive personalization as:  \n  $$\\text{Personalization} = 1 - \\text{average overlap}$$\n\n**Range**: 0 to 1. A value of 0 signifies identical recommendations, while 1 indicates unique recommendations for each user.\n\n**Interpretation**: Higher values reflect more personalized recommendations."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Average Recommendation Popularity (ARP)\nARP assesses the average popularity of recommended items, revealing any bias toward a few popular choices.\n\n**Implementation**:\n- Calculate item popularity based on training data.\n- Compute average popularity for each user and then overall using the formula:  \n  $$ARP = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{1}{|L_u|} \\sum_{i \\in L_u} \\phi(i)$$\n\n**Range**: 0 to infinity. \n\n**Interpretation**: Higher values indicate more popular recommendations in the top-K list."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Coverage Metric\nCoverage reflects the proportion of items recommended by the system compared to all potential items.\n\n**Implementation**:\n- Compute the share of unique items recommended to users relative to the total number of unique items:  \n  $$\\text{Coverage} = \\frac{\\text{Number of unique items recommended}}{\\text{Total number of unique items}}$$\n\n**Range**: 0 to 1, where 1 means that all items have been recommended.\n\n**Interpretation**: Higher values are generally preferable as they indicate broader representation in recommendations."
  },
  {
    "title": "Leftovers",
    "description": "Description of your new file.",
    "noindex": true,
    "filename": "docs-main/docs/library/leftover_content.mdx",
    "ai_section": "## Gini Index for Recommendation Distribution\nThe Gini index measures the inequality in item distribution recommendations across users.\n\n**Implementation**:\n- The Gini index is computed using the formula:  \n  $$ Gini(L) = 1 - \\frac{1}{|I| - 1} \\sum_{k=1}^{|I|} (2k - |I| - 1) p(i_k | L)$$\n- Here, *p(i|L)* is the occurrence ratio of item *i* across the recommendation lists.\n\n**Range**: 0 to 1. A value of 0 indicates perfect equality, while 1 indicates complete inequality.\n\n**Interpretation**: Lower values represent a more equitable distribution of recommended items among users."
  },
  {
    "title": "Metric generators",
    "description": "How to generate multiple metrics at once.",
    "filename": "docs-main/docs/library/metric_generator.mdx",
    "ai_section": "## Generating Multiple Tests and Metrics\nSometimes you need to generate multiple column-level Tests or Metrics. To simplify this, you can use metric generator helper functions."
  },
  {
    "title": "Metric generators",
    "description": "How to generate multiple metrics at once.",
    "filename": "docs-main/docs/library/metric_generator.mdx",
    "ai_section": "## Pre-requisites\nBefore using the metric generator helper functions, ensure you know how to [generate Reports](/docs/library/report)."
  },
  {
    "title": "Metric generators",
    "description": "How to generate multiple metrics at once.",
    "filename": "docs-main/docs/library/metric_generator.mdx",
    "ai_section": "## Imports\nTo generate toy data for this guide, you can use the following code:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom evidently import Dataset\nfrom evidently import DataDefinition\n\nnp.random.seed(42)\n\ndata = {\n    \"Age\": np.random.randint(18, 60, size=30),\n    \"Salary\": np.random.randint(30000, 120000, size=30),\n    \"Department\": np.random.choice([\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"], size=30),\n    \"YearsExperience\": np.random.randint(1, 15, size=30),  \n    \"EducationLevel\": np.random.choice([\"High School\", \"Bachelor\", \"Master\", \"PhD\"], size=30)  \n}\n\ndummy_df = pd.DataFrame(data)\n\neval_data_1 = Dataset.from_pandas(\n    dummy_df.iloc[:15],\n    data_definition=DataDefinition()\n)\neval_data_2 = Dataset.from_pandas(\n    dummy_df.iloc[15:],\n    data_definition=DataDefinition()\n)\n```"
  },
  {
    "title": "Metric generators",
    "description": "How to generate multiple metrics at once.",
    "filename": "docs-main/docs/library/metric_generator.mdx",
    "ai_section": "## Importing Necessary Libraries\nFor using the functionalities of the metrics and tests, import the following libraries:\n\n```python\nfrom evidently import Report\nfrom evidently.metrics import *\nfrom evidently.generators import ColumnMetricGenerator\n```"
  },
  {
    "title": "Metric generators",
    "description": "How to generate multiple metrics at once.",
    "filename": "docs-main/docs/library/metric_generator.mdx",
    "ai_section": "## Metric Generators\nYou can apply selected metrics to your dataset using `ColumnMetricGenerator`. Here are a few examples:\n\n**Example 1**: Apply the selected metric (`ValueDrift`) to all columns.\n\n```python\nreport = Report([\n    ColumnMetricGenerator(ValueDrift)\n])\n\nmy_eval = report.run(eval_data_1, eval_data_2)\nmy_eval\n```\n\n**Example 2**: Apply `ValueDrift` only to specified columns with parameters.\n\n```python\nreport = Report([\n    ColumnMetricGenerator(ValueDrift, \n                          columns=[\"EducationLevel\", \"Salary\"],\n                          metric_kwargs={\"method\":\"psi\"}),\n])\n\nmy_eval = report.run(eval_data_1, eval_data_2)\nmy_eval\n```\n\n**Example 3**: Apply `UniqueValueCount` only to categorical columns.\n\n```python\nreport = Report([\n    ColumnMetricGenerator(UniqueValueCount, \n                          column_types='cat'),\n])\n\nmy_eval = report.run(eval_data_1, eval_data_2)\nmy_eval\n```\n\nAvailable column types include:\n* `num` - numerical\n* `cat` - categorical\n* `all` - all"
  },
  {
    "title": "Metric generators",
    "description": "How to generate multiple metrics at once.",
    "filename": "docs-main/docs/library/metric_generator.mdx",
    "ai_section": "## Test Generators\nYou can also generate Tests using a similar approach. \n\n**Example**: Generate a test for all numerical columns.\n\n```python\nfrom evidently.future.tests import *\n\nreport = Report([\n    ColumnMetricGenerator(MinValue, \n                          column_types='num',\n                          metric_kwargs={\"tests\":[gt(0)]}), \n])\n\nmy_eval = report.run(eval_data_1, eval_data_2)\nmy_eval\n```\n\nThis example applies the minimum value test to all numerical columns, checking that they are above 0."
  },
  {
    "title": "Output formats",
    "description": "How to export the evaluation results.",
    "filename": "docs-main/docs/library/output_formats.mdx",
    "ai_section": "## Exporting Reports\nYou can view or export Reports in multiple formats.\n\n**Pre-requisites**:\n* You know how to [generate Reports](/docs/library/report)."
  },
  {
    "title": "Output formats",
    "description": "How to export the evaluation results.",
    "filename": "docs-main/docs/library/output_formats.mdx",
    "ai_section": "## Log to Workspace\nYou can save the computed Report in Evidently Cloud or your local workspace.\n\n```python\nws.add_run(project.id, my_eval, include_data=False)\n```\n\n<Info>\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\n</Info>"
  },
  {
    "title": "Output formats",
    "description": "How to export the evaluation results.",
    "filename": "docs-main/docs/library/output_formats.mdx",
    "ai_section": "## View in Jupyter notebook\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\n\nAfter running the Report, simply call the resulting Python object:\n\n```python\nmy_report\n```\n\nThis will render the HTML object directly in the notebook cell."
  },
  {
    "title": "Output formats",
    "description": "How to export the evaluation results.",
    "filename": "docs-main/docs/library/output_formats.mdx",
    "ai_section": "## HTML Export\nYou can also save this interactive visual Report as an HTML file to open in a browser:\n\n```python\nmy_report.save_html(â€œfile.htmlâ€)\n```\n\nThis option is useful for sharing Reports with others or if you're working in a Python environment that doesnâ€™t display interactive visuals."
  },
  {
    "title": "Output formats",
    "description": "How to export the evaluation results.",
    "filename": "docs-main/docs/library/output_formats.mdx",
    "ai_section": "## JSON Format\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\n\nTo view the JSON in Python:\n\n```python\nmy_report.json()\n```\n\nTo save the JSON as a separate file:\n\n```python\nmy_report.save_json(\"file.json\")\n```"
  },
  {
    "title": "Output formats",
    "description": "How to export the evaluation results.",
    "filename": "docs-main/docs/library/output_formats.mdx",
    "ai_section": "## Python Dictionary Output\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\n\nTo get the dictionary:\n\n```python\nmy_report.dict()\n```"
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Introduction to Evidently Python Library\nThe Evidently Python library is an open-source tool designed to evaluate, test, and monitor the quality of AI systems, from experimentation to production. It can be used independently or as part of the Monitoring Platform (self-hosted or Evidently Cloud)."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Core Workflows of Evidently\nEvidently covers four core workflows that can be used together or standalone:\n1. AI/ML Evaluations\n2. Synthetic Data Generation\n3. Prompt Optimization\n4. Tracking and Visualization UI"
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## AI/ML Evaluations\n**TL;DR**: The library provides numerous AI/ML/data metrics out of the box, exportable as scores or visual reports. \n\nEvidently runs evaluations on AI system inputs and outputs using over 100 built-in metrics and checks, including both metrics for predictive ML tasks and generative LLM outputs."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Evaluation Metrics\nThe library supports various types of built-in checks, such as:\n- **Text qualities**: Length, sentiment, special symbols.\n- **LLM output quality**: Semantic similarity, relevance.\n- **Data quality**: Missing values, duplicates.\n- **Data drift**: 20+ tests to detect distribution drift.\n- **Classification**: Accuracy, precision, recall.\n- **Regression**: MAE, RMSE, error distributions.\n- **Ranking**: NDCG, MAP, MRR."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Export Options for Evaluations\nEvidently allows evaluation results to be exported in multiple formats:\n- JSON or Python dictionary for scores.\n- DataFrame with metrics or attached scores.\n- Visual reports in Jupyter, Colab, or as HTML.\n- Upload functionality to the Evidently Platform for long-term tracking."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Synthetic Data Generation\n**TL;DR**: A configuration for structured synthetic data generation using LLMs is now available.\n\nEvidently helps generate synthetic test datasets, useful for creating RAG-style question-answer pairs and other test inputs."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Prompt Optimization\n**TL;DR**: The library includes tools for automated prompt writing using labeled or annotated data.\n\nEvidently provides features for scoring prompt variations based on a target dataset or user feedback, also generating LLM judge prompts for custom evaluations."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Tracking and Visualization UI\n**TL;DR**: The Evidently library includes a minimal UI to store and track evaluation results over time.\n\nThis UI allows for storing, comparing, and visualizing multiple evaluations, offering a historical perspective against previous results."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Core Evaluation Concepts\nTo run an evaluation, prepare data as a pandas DataFrame. This data can include various columns like numerical, categorical, and text. \n\nExamples include:\n- LLM logs with questions and answers.\n- Data tables for ML model performance.\n- Classification and regression logs."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Dataset Preparation\nTo evaluate, create an Evidently `Dataset` object from your prepared data. This captures metadata for proper processing and may require specific columns or types."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Using Multiple Datasets\nYou can prepare a second dataset to compare against the current dataset for:\n- Side-by-side comparisons.\n- Data drift detection.\n- Simplifying test setups with automatic range generation from the reference dataset."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Descriptors for Text Data\nTo evaluate text data and LLM outputs, utilize `Descriptors`, which are row-level scores or labels for analysis. Descriptors enable a more granular evaluation of text-related data."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Test Setup Simplification\nThe test setup can be simplified by automatically generating test conditions, such as min-max ranges, from the reference dataset, eliminating the need for manual configuration."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Data Sampling\nFor large datasets, evaluations can be time-consuming, depending on the evaluation's complexity, dataset size, and infrastructure. When computation is slow, using sampling methods like random or stratified sampling can be more efficient."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Descriptors\nDescriptors are row-level scores or labels assessing specific qualities of text data. They differ from metrics, which evaluate an entire dataset. Descriptors can range from basic metrics like `TextLength` to more complex checks such as `LLMEval`. You can create custom checks and use built-in descriptors to return results in numerical, categorical, or text formats."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Reports\nA Report structures evaluations on datasets or columns. You can generate Reports after obtaining descriptors to summarize text descriptors, analyze data quality, or evaluate AI performance. Each Report visualizes a set of Metrics and conditional Tests, and you can use Presets to simplify configuration."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Metric Presets\nMetric Presets are pre-configured evaluation templates allowing multiple related Metrics to be computed via a single command. They cover various scenarios, including exploratory data analysis and AI quality assessments."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Metrics\nMetrics are individual components within Presets. You can create custom Reports by selecting desired Metrics and include both built-in and custom options. Built-in Metrics range from simple statistics to complex algorithm evaluations."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Test Suites\nTest Suites enable you to run conditional checks against expected outcomes, allowing for automated validation of results. Each Test computes a value, checks against a condition, and reports a pass/fail outcome."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Test Conditions\nYou can establish Test conditions using manual or automatic setups. Manual setups involve adding thresholds directly to Metrics, while automatic setups utilize defaults or heuristics that adjust relative to a reference dataset."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Test Presets\nTest Presets enable quick setup by adding predefined Tests to Reports based on Metric Presets. These Tests check for quality issues, such as missing values or duplicates, employing either static or reference-inferred conditions."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Building Your Workflow\nYou can utilize Reports and Test Suites independently for ad-hoc checks, experiments, and debugging. Alternatively, they can be integrated into production workflows for monitoring and validation purposes using tools like Airflow."
  },
  {
    "title": "Introduction",
    "description": "Core concepts and components of the Evidently Python library.",
    "filename": "docs-main/docs/library/overview.mdx",
    "ai_section": "## Platform Deployment Options\nEvidently provides flexibility in deployment, allowing you to self-host the open-source version or opt for the recommended Evidently Cloud, which includes additional features such as synthetic data and test generation."
  },
  {
    "title": "Prompt optimization",
    "description": "[NEW] Automated prompt optimization.",
    "filename": "docs-main/docs/library/prompt_optimization.mdx",
    "ai_section": "## Overview of Documentation\nMore detailed documentation is coming soon regarding prompt optimization for LLM judges."
  },
  {
    "title": "Prompt optimization",
    "description": "[NEW] Automated prompt optimization.",
    "filename": "docs-main/docs/library/prompt_optimization.mdx",
    "ai_section": "## Release Blog\nRead the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization) for insights and updates."
  },
  {
    "title": "Prompt optimization",
    "description": "[NEW] Automated prompt optimization.",
    "filename": "docs-main/docs/library/prompt_optimization.mdx",
    "ai_section": "## Example Notebooks\nHere are some example notebooks that illustrate different prompt optimization techniques for LLM judges:\n\n- **Code Review Binary LLM Judge Prompt Optimization**: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\n- **Topic Multi-Class LLM Judge Prompt Optimization**: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\n- **Tweet Generation Prompt Optimization**: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)"
  },
  {
    "title": "Report",
    "description": "How to generate Report.",
    "filename": "docs-main/docs/library/report.mdx",
    "ai_section": "## Introduction to Reports\nReports perform evaluations on the Dataset level and/or summarize results of the row-level evaluations. For a general introduction, check [Core Concepts](/docs/library/overview).\n\n**Pre-requisites**:\n- You [installed Evidently](/docs/setup/installation).\n- You created a Dataset with the [Data Definition](/docs/library/data_definition).\n- (Optional) for text data, you added Descriptors.\n\n<Note>\nFor a quick end-to-end example of generating Reports, check the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm).\n</Note>"
  },
  {
    "title": "Report",
    "description": "How to generate Report.",
    "filename": "docs-main/docs/library/report.mdx",
    "ai_section": "## Imports\nImport the Metrics and Presets you plan to use.\n\n```python\nfrom evidently import Report\nfrom evidently.metrics import *\nfrom evidently.presets import *\n```\n\nYou can use Metric Presets, which are pre-built Reports that work out of the box, or create a custom Report selecting Metrics one by one."
  },
  {
    "title": "Report",
    "description": "How to generate Report.",
    "filename": "docs-main/docs/library/report.mdx",
    "ai_section": "## Using Presets\n<Tip>\n**Available Presets**. Check available evals in the [Reference table](/metrics/all_metrics).\n</Tip>\n\nTo generate a template Report, simply pass the selected Preset to the Report and run it over your data. \n\n### Single dataset\nTo generate the Data Summary Report for a single dataset:\n\n```python\nreport = Report([\n    DataSummaryPreset()\n])\n\nmy_eval = report.run(eval_data_1, None)\nmy_eval\n#my_eval.json\n```\n\nAfter you `run` the Report, the resulting `my_eval` contains the computed values for each metric, along with associated metadata and visualizations.\n\n<Note>\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\n</Note>\n\n### Two datasets\nTo generate reports like Data Drift that need two datasets, pass the second one as a reference:\n\n```python\nreport = Report([\n    DataDriftPreset()\n])\n\nmy_eval = report.run(eval_data_1, eval_data_2)\nmy_eval\n#my_eval.json\n```\n\n### Combine Presets\nYou can also include multiple Presets in the same Report:\n\n```python\nreport = Report([\n    DataDriftPreset(), \n    DataSummaryPreset()\n])\n\nmy_eval = report.run(eval_data_1, eval_data_2)\nmy_eval\n#my_eval.json\n```"
  },
  {
    "title": "Report",
    "description": "How to generate Report.",
    "filename": "docs-main/docs/library/report.mdx",
    "ai_section": "## Custom Report Creation\n<Tip>\n**Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\n</Tip>\n\n### Choose Metrics\nTo create a custom Report, list the Metrics one by one. You can combine both dataset-level and column-level Metrics:\n\n```python\nreport = Report([\n    ColumnCount(), \n    ValueStats(column=\"target\")\n])\n\nmy_eval = report.run(eval_data_1, None)\nmy_eval\n#my_eval.json\n```\n\n### Metric Parameters\nMetrics can have optional or required parameters.\n\nTo calculate the Precision at K for a ranking task, you must pass the `k` parameter (Required):\n\n```python\nreport = Report([\n   PrecisionTopK(k=10)\n])\n```"
  },
  {
    "title": "Report",
    "description": "How to generate Report.",
    "filename": "docs-main/docs/library/report.mdx",
    "ai_section": "## Comparing Results\nIf you computed multiple snapshots, you can quickly compare the resulting metrics side-by-side in a dataframe:\n\n```python\nfrom evidently import compare\n\ncompare_dataframe = compare(my_eval_1, my_eval_2, my_eval_3)\n```"
  },
  {
    "title": "Report",
    "description": "How to generate Report.",
    "filename": "docs-main/docs/library/report.mdx",
    "ai_section": "## Grouping by Categories\nYou can calculate metrics separately for different groups in your data using a column with categories. Use the `GroupBy` metric as shown below.\n\n### Example\nThis will compute the maximum value of salaries by each label in the \"Department\" column:\n\n```python\nfrom evidently.metrics.group_by import GroupBy\n\nreport = Report([\n    GroupBy(MaxValue(column=\"Salary\"), \"Department\"),\n])\nmy_eval = report.run(data, None)\nmy_eval.dict()\n```\n\nNote: You cannot use auto-generated Test conditions when you use GroupBy."
  },
  {
    "title": "Report",
    "description": "How to generate Report.",
    "filename": "docs-main/docs/library/report.mdx",
    "ai_section": "## What's Next?\nYou can also add conditions to Metrics: check the [Tests guide](/docs/library/tests)."
  },
  {
    "title": "Synthetic data generation",
    "description": "[NEW] Code-first synthetic data generation.",
    "filename": "docs-main/docs/library/synthetic_data_api.mdx",
    "ai_section": "## Generating Synthetic Test Data\nYou can generate synthetic test data from the RAG knowledge base or by using a simple configuration."
  },
  {
    "title": "Synthetic data generation",
    "description": "[NEW] Code-first synthetic data generation.",
    "filename": "docs-main/docs/library/synthetic_data_api.mdx",
    "ai_section": "## Documentation Availability\nMore detailed documentation on synthetic test data generation will be available soon."
  },
  {
    "title": "Synthetic data generation",
    "description": "[NEW] Code-first synthetic data generation.",
    "filename": "docs-main/docs/library/synthetic_data_api.mdx",
    "ai_section": "## Example Notebooks\nAn example notebook for synthetic data generation can be found at: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)."
  },
  {
    "title": "Add tags and metadata",
    "description": "How to add metadata to evaluations.",
    "filename": "docs-main/docs/library/tags_metadata.mdx",
    "ai_section": "## Adding Timestamps to Reports\nEach Report run has a single timestamp. By default, Evidently assigns `datetime.now()` as the run time based on the user's time zone. You can also specify a custom timestamp by passing it to the `run()` method, allowing for asynchronous logging or backdating of Reports to the relevant time period.\n\n```python\nfrom datetime import datetime\n\nmy_eval_4 = report.run(eval_data_1,\n                       eval_data_2,\n                       timestamp=datetime(2024, 1, 29))\n```"
  },
  {
    "title": "Add tags and metadata",
    "description": "How to add metadata to evaluations.",
    "filename": "docs-main/docs/library/tags_metadata.mdx",
    "ai_section": "## Adding Tags and Metadata to Reports\nYou can add `tags` and `metadata` to Reports for improved searchability and filtering. Tags enable visualization of data subsets on monitoring Panels, marking evaluations by various criteria such as model version or user segment.\n\n### Custom Tags\nPass any custom Tags as a list when creating a Report:\n\n```python\nreport = Report([\n    ClassificationPreset()\n],\ntags=[\"classification\", \"production\"])\n```\n\n### Custom Metadata\nPass metadata as a Python dictionary in key:value pairs:\n\n```python\nreport = Report([\n    ClassificationPreset()\n],\nmetadata = {\n    \"deployment\": \"shadow\",\n    \"status\": \"production\",\n})\n```\n\n### Default Metadata\nUse built-in metadata fields like `model_id`, `reference_id`, `batch_size`, and `dataset_id`:\n\n```python\nreport = Report([\n    ClassificationPreset()\n],\n  model_id=\"model_id\",\n  reference_id=\"reference_id\",\n  batch_size=\"batch_size\",\n  dataset_id=\"dataset_id\"\n)\n```\n\n### Adding Tags to Individual Runs\nYou can tag individual Report runs, which is useful for experiments with varying prompts or hyperparameters:\n\n```python\nmy_eval = report.run(eval_data_1, eval_data_2, tags=[\"prompt_v1\", \"claude\"])\n```"
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "## Overview of Tests\nTests let you validate specific conditions and get Pass/Fail results on the dataset level. Tests are an add-on to the Report and appear in a separate tab.\n\n**Pre-requisites**:\n* You know how to [generate Reports and select Metrics](/docs/library/report).\n\n**Note**: For a quick end-to-end example of generating Tests, check the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm)."
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "## Imports\nTo use Tests, import the following modules:\n\n```python\nfrom evidently import Report\nfrom evidently.metrics import *\nfrom evidently.presets import *\nfrom evidently.tests import *\n```"
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "## Auto-generated Conditions\nThere are 3 ways to run conditional checks:\n\n* **Tests Presets**: Get a suite of pre-selected Tests with auto-generated conditions. \n* **Tests with Defaults**: Pick Tests one by one, with auto-generated conditions.\n* **Custom Tests**: Choose all Tests and set conditions manually."
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "### Test Presets\nTest Presets automatically generate a set of Tests to evaluate your data or AI system. Each Report Preset has this option. \n\nEnable it by setting `include_tests=True` on the Report level. (Default: False).\n\n```python\nreport = Report([\n    DataSummaryPreset(),\n],\ninclude_tests=True)\n```\n\nFor example, while the `DataSummaryPreset()` Report simply shows descriptive stats of your data, adding the Tests will additionally run multiple checks on data quality and expected column statistics.\n\n**Using Reference**: Tests compare the new data against a reference dataset.\n\n```Python\nmy_eval = report.run(eval_data_1, eval_data_2) # eval_data_2 is reference\n```\n\n**Using Heuristics**: Without a reference, Tests use predefined rules.\n\n```Python\nmy_eval = report.run(eval_data_1, None) # no reference data\n```\n\n**Info**: To check Test defaults, consult the [All Metrics](/metrics/all_metrics) reference table."
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "### Individual Tests with Defaults\nYou can select specific Tests while still using default conditions.\n\n**Select Tests**: List individual Metrics and use the `include_Tests` option.\n\n```Python\nreport = Report([\n    MissingValueCount(column=\"Age\"),\n    MinValue(column=\"Age\"),\n], \ninclude_tests=True)\n```\n\n**Exclude Some Tests**: To prevent Test generation for certain Metrics/Presets, set the list of `tests` to `None` or leave it empty.\n\n```Python\nreport = Report([\n    MissingValueCount(column=\"Age\", tests=[]),\n    MinValue(column=\"Age\"),\n], \ninclude_tests=True)\n```"
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "## Custom Test Conditions\nYou can define specific pass/fail conditions for each Test.\n\n**Setting Conditions**: Define a list of `tests` for each Metric you want to validate.\n\n```Python\nreport = Report([\n    MissingValueCount(column=\"Age\", tests=[eq(0)]),\n    MinValue(column=\"Age\", tests=[gte(18)]),\n])\n```\n\n**Info**: Sometimes, you may need to use other parameters to set test conditions. The `tests` parameter applies when a metric returns a single value."
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "### Test Parameters\nHere are the conditions you can set:\n\n| Condition      | Explanation                                       | Example                                                |\n|"
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "- |"
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "|\n| `eq(val)`      | equal to                                         | `MinValue(column=\"Age\", tests=[eq(18)])`             |\n| `not_eq(val)`  | not equal                                       | `MinValue(column=\"Age\", tests=[not_eq(18)])`         |\n| `gt(val)`      | greater than                                    | `MinValue(column=\"Age\", tests=[gt(18)])`             |\n| `gte(val)`     | greater than or equal                           | `MinValue(column=\"Age\", tests=[gte(18)])`            |\n| `lt(val)`      | less than                                       | `MinValue(column=\"Age\", tests=[lt(18)])`             |\n| `lte(val)`     | less than or equal                              | `MinValue(column=\"Age\", tests=[lte(18)])`            |\n| `is_in: list`  | `test_result ==` one of the values              | `MinValue(column=\"Age\", tests=[is_in([18, 21, 30])])`  |\n| `not_in: list` | `test_result !=` any of the values              | `MinValue(column=\"Age\", tests=[not_in([16, 17, 18])])` |\n\n**Info**: To check available parameters, consult the [All Metrics](/metrics/all_metrics) reference table."
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "### Combining Custom and Default Conditions\nYou can use both default and custom conditions across the Report.\n\n```Python\nreport = Report([\n    RowCount(tests=[gt(10)]),\n    MissingValueCount(column=\"Age\"),\n],\ninclude_tests=True) \n```\n\nYou can also add multiple checks to the same Metric.\n\n```python\nreport = Report([\n    MinValue(column=\"Age\", tests=[gte(17), lte(19)]),\n])\n```"
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "### Tests Relative to Reference\n**Testing Against Reference**: Define conditions relative to the reference values.\n\n```python\nreport = Report([\n   RowCount(tests=[gte(Reference(relative=0.1))]),\n])\n```\n\nYou can also define the absolute difference from reference.\n\n```python\nreport = Report([\n   RowCount(tests=[gte(Reference(absolute=5))]),\n])\n```"
  },
  {
    "title": "Tests",
    "description": "How to run conditional checks.",
    "filename": "docs-main/docs/library/tests.mdx",
    "ai_section": "### Setting Test Criticality\nBy default, failed Tests return Fail. To get a Warning instead, set `is_critical=False`. \n\n```python\nreport = Report([\n    MissingValueCount(column=\"Age\", share_tests=[eq(0, is_critical=False)]),\n])\n```\n\nThis helps manage alert fatigue. For layered conditions, you can also set:\n\n```python\nreport = Report([\n    MissingValueCount(column=\"Age\", \n                      share_tests=[eq(0, is_critical=False), \n                                   lte(0.1, is_critical=True)]),\n])\n```"
  },
  {
    "title": "Alerts",
    "description": "How to set up alerts.",
    "filename": "docs-main/docs/platform/alerts.mdx",
    "ai_section": "## Built-in Alerting Overview\nBuilt-in alerting is a Pro feature available in the **Evidently Cloud** and **Evidently Enterprise**."
  },
  {
    "title": "Alerts",
    "description": "How to set up alerts.",
    "filename": "docs-main/docs/platform/alerts.mdx",
    "ai_section": "## How to Enable Alerts\nTo enable alerts, open the Project and navigate to the \"Alerts\" in the left menu. You must set:\n\n* A notification channel.\n* An alert condition."
  },
  {
    "title": "Alerts",
    "description": "How to set up alerts.",
    "filename": "docs-main/docs/platform/alerts.mdx",
    "ai_section": "## Notification Channels\nYou can choose between the following options for notification channels:\n\n* **Email**: Add email addresses to send alerts to.\n* **Slack**: Add a Slack webhook.\n* **Discord**: Add a Discord webhook."
  },
  {
    "title": "Alerts",
    "description": "How to set up alerts.",
    "filename": "docs-main/docs/platform/alerts.mdx",
    "ai_section": "## Alert Conditions\nThere are two types of alert conditions you can set:\n\n### Failed Tests\nIf you use Tests (conditional checks) in your Project, you can tie alerting to the failed Tests in a Test Suite. Toggle this option on the Alerts page. Evidently will set an alert to the defined channel if any of the Tests fail.\n\n<Tip>\n  **How to avoid alert fatigue?** Use the `is_critical` parameter to mark non-critical Tests as Warnings. Setting it to `False` prevents alerts for those checks even if they fail.\n</Tip>"
  },
  {
    "title": "Alerts",
    "description": "How to set up alerts.",
    "filename": "docs-main/docs/platform/alerts.mdx",
    "ai_section": "### Custom Conditions\nYou can also set alerts on individual Metric values. For example, you can generate Alerts when the share of drifting features is above a certain threshold. Click on the plus sign below the â€œAdd new Metric alertâ€ and follow the prompts to set an alert condition."
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Adding Panels via Python API\nYou can add Panels in the user interface or using the Python API. This section describes how to use the Python API for adding panels to your dashboard. \n\n### Pre-requisites\nTo get started, you must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage)."
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Dashboard Management\nDashboards as code are available in Evidently OSS, Cloud, and Enterprise.\n\n### Adding Tabs\nTo add a new Tab, use the following command:\n```python\nproject.dashboard.add_tab(\"Another Tab\")\n```\nYou can also create a new Tab while adding a Panel. If the destination Tab doesnâ€™t exist, it will be created; if it does, the Panel will be added below existing ones.\n\n### Deleting Tabs\nTo delete a Tab, execute:\n```python\nproject.dashboard.delete_tab(\"Another Tab\")\n```\n\n### Deleting Panels\nTo delete a specific Panel, use:\n```python\nproject.dashboard.delete_panel(\"Dashboard title\", \"My new tab\")\n```\n(Note: List the Panel name first, then the Tab name.)\n\n### Clearing the Dashboard\nTo delete all Tabs and Panels on the Dashboard, use:\n```python\nproject.dashboard.clear_dashboard()\n```\nNote: This does **not** delete the underlying Reports or dataset; it only clears the Panels."
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Adding Different Types of Panels\n### Text Panels\nText-only panels are perfect for titles. To add a text panel:\n```python\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Dashboard title\",\n        size=\"full\", \n        values=[], #leave empty\n        plot_params={\"plot_type\": \"text\"},\n    ),\n    tab=\"My new tab\", #will create a Tab if there is no Tab with this name\n)\n```\n\n### Counter Panels\nCounter panels display a value with optional supporting text. Hereâ€™s how to add counters for the `RowCount` metric with different aggregations:\n```python\n# Sum\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Row count\",\n        subtitle=\"Total number of evaluations over time.\",\n        size=\"half\",\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"sum\"},\n    ),\n    tab=\"My tab\",\n)\n\n# Average\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Row count\",\n        subtitle=\"Average number of evaluations per Report.\",\n        size=\"half\",\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"avg\"},\n    ),\n    tab=\"My tab\",\n)\n\n# Last\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Row count\",\n        subtitle=\"Latest number of evaluations.\",\n        size=\"half\",\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\n        plot_params={\"plot_type\": \"counter\", \"aggregation\": \"last\"},\n    ),\n    tab=\"My tab\",\n)\n```\n\n### Pie Charts\nTo add pie charts, you can use the aggregation parameters similar to counters:\n```python\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Row count\",\n        subtitle=\"Total number of evaluations over time.\",\n        size=\"half\",\n        values=[PanelMetric(legend=\"Row count\", metric=\"RowCount\")],\n        plot_params={\"plot_type\": \"pie\", \"aggregation\": \"sum\"},\n    ),\n    tab=\"My tab\",\n)\n```\n\n### Plots\nPanels can also display values as bar or line plots. To add time series panels for the `RowCount` metric:\n```python\n# line chart\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Row count\",\n        subtitle=\"Number of evaluations over time.\",\n        size=\"half\",\n        values=[\n            PanelMetric(\n                legend=\"row count\",\n                metric=\"RowCount\",\n            ),\n        ],\n        plot_params={\"plot_type\": \"line\"},\n    ),\n    tab=\"My tab\",\n)\n\n# bar chart\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Row count\",\n        subtitle=\"Number of evaluations over time.\",\n        size=\"half\",\n        values=[\n            PanelMetric(\n                legend=\"row count\",\n                metric=\"RowCount\",\n            ),\n        ],\n        plot_params={\"plot_type\": \"bar\", \"is_stacked\": False}, \n    ),\n    tab=\"My tab\",\n)\n```\n\n### Multiple Values in a Single Panel\nA Panel can show multiple values. For example, this adds multiple lines on a Line chart:\n```python\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Text Length\",\n        subtitle=\"Text length stats (symbols).\",\n        size=\"full\",\n        values=[\n            PanelMetric(legend=\"max\", metric=\"MaxValue\", metric_labels={\"column\": \"length\"}),\n            PanelMetric(legend=\"mean\", metric=\"MeanValue\", metric_labels={\"column\": \"length\"}),\n            PanelMetric(legend=\"min\", metric=\"MinValue\", metric_labels={\"column\": \"length\"}),\n        ]\n    )\n)\n```"
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Dashboard Panel Options\nA summary of all parameters for Panels is as follows:\n\n| Parameter              | Type   | Required | Default  | Description                                                                               |\n|"
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "- |"
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "|"
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "-- |\n| `title`                | `str`  | âŒ        | `None`   | Title of the panel.                                                                       |\n| `description`          | `str`  | âŒ        | `None`   | Optional panel description shown as a subtitle.                                           |\n| `size`                 | `str`  | âŒ        | `\"full\"` | Panel size: `\"full\"` (100% width) or `\"half\"` (50%).                                      |\n| `values`               | `list` | âœ…        | â€”        | List of `PanelMetric` objects to display.                                                 |\n| `tab`                  | `str`  | âŒ        | `None`   | Dashboard tab name. If not set, defaults to the first tab or creates a new \"General\" tab. |\n| `create_if_not_exists` | `bool` | âŒ        | `True`   | If `True`, creates the tab if it doesn't exist. Throws exception if `False`.              |\n| `plot_params`          | `dict` | âŒ        | `{}`     | Panel visualization settings like `\"plot_type\"`: `\"text\"`, `\"line\"`, `\"counter\"`.         |"
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Configuring Panel Values\n### Metric\nTo define which value the Panel displays, reference the name of the corresponding Evidently Metric. This metric must be present in the Reports logged to your Project.\n\n- **Dataset-level Metrics**: Pass the Metric name directly to `PanelMetric`, e.g., `\"RowCount\"`.\n  \nExample:\n```python\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Row count\",\n        subtitle=\"Number of evaluations over time.\",\n        size=\"half\",\n        values=[\n            PanelMetric(\n                legend=\"Row count\",\n                metric=\"RowCount\",\n            ),\n        ],\n        plot_params={\"plot_type\": \"line\"},\n    ),\n    tab=\"My tab\",\n)\n```\n\n### Presets\nWhen logging Reports using a Preset, refer to the specific metric inside it, such as `Accuracy` or `Recall`.\n\n**Need help finding metric names?** See the [All Metrics Reference Table](/metrics/all_metrics) for a full list of Metrics."
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Metric Labels\nSome Metrics require additional context, especially when they:\n\n- Operate at the column level\n- Return multiple values (metric results)\n- Have user-defined custom parameters\n\nThese cases require you to use `metric_labels`.\n\n**Example**: To plot the share of categories inside the \"Denials\" column:\n```python\nproject.dashboard.add_panel(\n    DashboardPanelPlot(\n        title=\"Denials\",\n        subtitle=\"Number of denials.\",\n        size=\"half\",\n        values=[\n            PanelMetric(\n                legend=\"\"\"{{label}}\"\"\",\n                metric=\"UniqueValueCount\",\n                metric_labels={\"column\": \"denials\", \"value_type\": \"share\"} \n            ),\n        ],\n        plot_params={\"plot_type\": \"bar\", \"is_stacked\": True},\n    ),\n    tab=\"My tab\",\n)\n```\n\n### Column / Descriptor\nFor metrics that operate at the column level, use the `column` label to specify the descriptor it refers to.\n\n**Example**: To plot the min value from the \"Text Length\" column:\n```python\nvalues=[\n    PanelMetric(\n        legend=\"Min text length\",\n        metric=\"MinValue\",\n        metric_labels={\"column\": \"TextLength\"},\n    )\n]\n```\n\n### Value Type\nMost evidential Metrics return a single `value`. Some metrics return multiple results, in which case you must specify which value to plot using the `value_type` key.\n\n**Example**:\n```python\nvalues = [\n    PanelMetric(\n        legend=\"Share\",\n        metric=\"DriftedColumnsCount\", \n        metric_labels={\"value_type\": \"share\"}\n    )\n]\n```\n\n**Metrics with Extra Parameters**: If a metric has configurable options, these must also be included accordingly."
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Helpful Resources\n**How to verify the metric result for a specific metric?**\n- Look up expected outputs in the [All Metrics Table](/metrics/all_metrics).\n- Generate a Report with the target `metric` and inspect its structure via `report.dict()` or `report.json()`."
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Metric Results Explained\nSome metrics produce multiple results, such as:\n\n- `CategoryCount`: returns both `share` and `count`\n- `MAE`: returns both `mean` and `std`\n\nTo specify which value you want, use the `value_type` key, for example: `{\"value_type\": \"share\"}`.\n\n```python\nvalues = [\n    PanelMetric(\n        legend=\"Share\",\n        metric=\"DriftedColumnsCount\",  # <- metric from Data Drift Preset that returns `count` or `share` of drifting columns\n        metric_labels={\"value_type\": \"share\"}  # <- plot relative share\n    ),\n]\n```"
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Verifying Metric Results\n**How to verify the metric result for a specific metric?**\n\n- Look up the expected outputs in the [All Metrics Table](/metrics/all_metrics).\n- Generate a report with the target `metric` and inspect its structure via `report.dict()` or `report.json()`."
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## Metrics with Extra Parameters\nFor metrics that have configurable options (like drift method), you must also include those in `metric_labels`."
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "## PanelMetric Options\nA summary of all parameters for `PanelMetric`:\n\n| Parameter       | Type   | Required | Default | Description                                                                      |\n|"
  },
  {
    "title": "Add dashboard panels (API)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels.mdx",
    "ai_section": "-- |\n| `legend`        | `str`  | âŒ        | `None`  | Legend name in the panel. If `None`, one is auto-generated.                     |\n| `tags`          | `list` | âŒ        | `[]`    | Optional tags to select values from a subset of Reports in the Project.         |\n| `metadata`      | `dict` | âŒ        | `{}`    | Optional metadata to select values from a subset of Reports in the Project.     |\n| `metric`        | `str`  | âœ…        | â€”       | Metric name (e.g., `\"RowCount\"`).                                              |\n| `metric_labels` | `dict` | âŒ        | `{}`    | Parameters like `column` names (applies to descriptors too) or `value_type`.   |"
  },
  {
    "title": "Add dashboard panels (UI)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels_ui.mdx",
    "ai_section": "## Dashboards Overview\nDashboards let you create Panels to visualize evaluation results over time. To populate these panels, you must first add Reports with evaluation results to the Project. No-code Dashboards are available in the Evidently Cloud and Enterprise."
  },
  {
    "title": "Add dashboard panels (UI)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels_ui.mdx",
    "ai_section": "## Adding Tabs\nBy default, new Panels appear on a single Dashboard. You can add multiple Tabs to organize them.\n\n**To add a Tab**:\n\n- Enter \"Edit\" mode on the Dashboard (top right corner).\n- Click the plus sign with \"add Tab\" on the left.\n- To create a custom Tab, select \"empty\" and enter a name.\n\nPre-built Tabs are available as dashboard templates with preset Panel combinations. These rely on having related Metrics within the Project, and if the necessary data is not available, Panels will appear empty until Reports containing those Metrics are added.\n\nAvailable Tabs include:\n| Template    | Description                                                                                                                    | Data source                                                                 |\n|"
  },
  {
    "title": "Add dashboard panels (UI)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels_ui.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Add dashboard panels (UI)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels_ui.mdx",
    "ai_section": "|"
  },
  {
    "title": "Add dashboard panels (UI)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels_ui.mdx",
    "ai_section": "|\n| **Columns** | Shows the results of text evaluations over time OR plots column distributions over time for categorical and numerical columns. | `TextEvals()`, `DataSummaryPreset()` or `ValueStats()` for individual columns. |\n\n**To delete a Tab**: enter \"Edit\" mode, choose \"edit Tabs\" next to Tab names, and select the Tab to delete."
  },
  {
    "title": "Add dashboard panels (UI)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels_ui.mdx",
    "ai_section": "## Adding Panels\nYou can add any number of Panels to your Dashboard, including text panels, counters, pie charts, line plots, and bar plots (grouped and stacked). When you create a Panel, you pull the corresponding value from multiple Reports and display it over time or using specified aggregation methods (sum, average, last).\n\n**How to add a Panel:**\n\n- Enter \"Edit\" mode on the Dashboard (top right corner).\n- Click on the \"Add Panel\" button next to it.\n- Follow the prompts to configure the panel.\n- Use the preview to review your setup.\n- Click \"Save\" and select the Tab where you want to add the Panel.\n\n**Key Configuration Options**:\n- **Select Metrics**: Ensure you choose the correct Metric name that matches the name logged inside the Reports.\n- **Filter by Tag**: Use the \"From\" field to filter metrics by Tags, which must be attached to the Reports.\n- **Filter by Metric label**: Specify further keys like Column and Value type for Metrics that return both.\n- **Set Legend and Panel Type**: Modify legend display and specify the plot type and aggregation level."
  },
  {
    "title": "Add dashboard panels (UI)",
    "description": "How to design your Dashboard with custom Panels.",
    "filename": "docs-main/docs/platform/dashboard_add_panels_ui.mdx",
    "ai_section": "## Deleting/Editing Panels\nTo delete or edit a Panel, enter Edit mode and hover over a specific Panel to choose an action."
  },
  {
    "title": "Overview",
    "description": "Introduction to Dashboard.",
    "filename": "docs-main/docs/platform/dashboard_overview.mdx",
    "ai_section": "## What is a Dashboard?\nA Dashboard provides a clear view of your AI application performance. You can use it:\n\n- to track evaluation results across multiple experiments;\n- to track live production quality over time.\n\nEach Project has its own Dashboard. It's empty at first. To populate it, you need to run an evaluation and **save at least one Report** to the Project. You can then choose values from Reports to plot."
  },
  {
    "title": "Overview",
    "description": "Introduction to Dashboard.",
    "filename": "docs-main/docs/platform/dashboard_overview.mdx",
    "ai_section": "## Dashboard Tabs\nMultiple Tabs are available in **Evidently Cloud** and **Evidently Enterprise**. You can logically organize Panels within the same Dashboard into different Tabs."
  },
  {
    "title": "Overview",
    "description": "Introduction to Dashboard.",
    "filename": "docs-main/docs/platform/dashboard_overview.mdx",
    "ai_section": "## Dashboard Panels\nA Panel is a visual element in the Dashboard that displays specified values in a single widget. Panels can be counters, line plots, bar plots, etc. You can add multiple Panels to the Dashboard and customize their type and values shown.\n\nYou can add Panels in two ways:\n- Using the Python API â€“ define your Dashboard as code.\n- Through the UI â€“ add Panels directly from the interface (Cloud and Enterprise only).\n\nTo create a Panel, you need to specify:\n- Value â€“ choose an individual metric to plot.\n- Parameters â€“ such as title, panel type, and size.\n- Tags (optional) â€“ use to filter and visualize subsets of your data."
  },
  {
    "title": "Overview",
    "description": "Introduction to Dashboard.",
    "filename": "docs-main/docs/platform/dashboard_overview.mdx",
    "ai_section": "## From Dashboard to Reports\nBy clicking on any individual value on the Dashboard, you can open the associated Report and source Dataset for further debugging."
  },
  {
    "title": "Overview",
    "description": "Introduction to Dashboard.",
    "filename": "docs-main/docs/platform/dashboard_overview.mdx",
    "ai_section": "## Data Source\nDashboards rely on having **Reports** in the Project as a data source. \n\nWhen adding a Panel, you select a **Metric**, and Evidently pulls the corresponding value(s) from all Reports in the Project to plot them.\n\nFor example, if you log multiple Data Drift Reports (each includes the `DriftedColumnsCount` for the corresponding batch), you can plot how this Metric value changes over time.\n\nThe Panel time resolution depends on logged Report frequency. For instance, if you log Reports daily, you'll see values at daily granularity.\n\nYou can use **Tags** to filter data from specific Reports. For example, you can plot the accuracy of Model A and Model B on separate Panels. To achieve this, you must first [add relevant Tags](/docs/library/tags_metadata) to the Report, and then filter by these Tags when creating a Panel."
  },
  {
    "title": "Overview",
    "description": "Introduction to Dashboard.",
    "filename": "docs-main/docs/platform/dashboard_overview.mdx",
    "ai_section": "## Whatâ€™s Next?\n- See how to [customize dashboard via API](/docs/platform/dashboard_add_panels).\n- See how to [customize dashboard via UI](/docs/platform/dashboard_add_panels_ui)."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Monitoring Panel Overview\nA monitoring Panel is an individual plot or counter on the Monitoring Dashboard. You can add multiple Panels and organize them by **Tabs**, customizing Panel type, values shown, titles, and legends. When adding a Panel, you choose a **Test** or **Metric** to pull corresponding values from Reports in the Project. You can use **Tags** to filter data from specific Reports."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Panel Types\nThere are three main panel types:\n\n1. **Metric Panels**: Plot individual values from Reports.\n2. **Test Panels**: Show pass/fail Test outcomes over time.\n3. **Distribution Panels**: Plot distributions over time."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Metric Panels\nMetric Panels (`DashboardPanel`) display individual values from Reports over time. For example, this can include mean, max, min values from Data Summary Reports or Data Drift Reports. \n\n**Panel time resolution** varies with Report frequencyâ€”daily logged Reports allow daily granularity.\n\n### Counter\nClass `DashboardPanelCounter` shows a value with supporting text, making it useful for dashboard titles.\n\n### Plot\nClass `DashboardPanelPlot` displays values as bar, line, scatter plot, or histogram."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Test Panels\nTest Panels visualize Test results, allowing you to see pass/fail outcomes or result counts over repeated Tests. These panels work only with Test Suites.\n\n### Test Counter\nClass `DashboardPanelTestSuiteCounter` shows a counter of Tests with specified status.\n\n### Test Plot\nClass `DashboardPanelTestSuite` presents individual results or aggregates based on selected Test types."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Distribution Panel\nClass `DashboardPanelDistribution` displays the distribution of values over time. It can be used to track how the frequency of categories changes in captured evaluations.\n\n**Comparison with Histogram**: A Histogram shows the distribution of selected values, while a Distribution Panel demonstrates how this distribution alters over time."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Adding Dashboard Panels\nThis section describes how to add panels one by one and mentions using [pre-built Tabs](/docs/platform/dashboard_tabs) for a quick start.\n\n### Adding Tabs\nMultiple Tabs are available in Evidently Cloud and Enterprise. You can add Tabs to organize Panels. \n\n**User Interface**: Enter \"Edit\" mode and click the plus sign to add Tabs.\n\n**Python**: Use `create_tab` to add a Tab programmatically."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Adding Panels (User Interface)\nTo add Panels in the User Interface:\n\n1. Enter \"Edit\" mode.\n2. Click on \"Add panel\".\n3. Configure dashboard name, type, etc.\n4. Preview and publish.\n\nTo delete/edit a Panel, hover over the Panel in Edit mode."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Adding Panels (Python API)\nYou can also add Panels using the Python API. Here's the general flow:\n\n1. Connect to your Project using `ws.get_project(\"YOUR PROJECT ID HERE\")`.\n2. Use the `add_panel` method to configure and add a new Panel.\n3. Save with `project.save()`.\n\n**Delete Panels**: To remove all Panels, use `project.dashboard.panels = []`."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Panel Parameters\nGeneral parameters for all Panel types include:\n\n- `title`: The name visible at the header.\n- `filter`: Defines a subset of Reports from which to display data.\n- `size`: Sets the Panel size to half-width or full-width."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Counter Panel Details\nClass `DashboardPanelCounter` can display both a value count or simply text. Example usage allows creating text-only panels or counting panels based on Test results."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Filters\nFilters define a subset of Reports from which to display the data. Tags or metadata values you list must be added when logging Reports. See [docs](/docs/library/tags_metadata)."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Widget Sizes\nThe `size` parameter sets the Panel size to either half-width or full-sized. Default size is `WidgetSize.FULL`."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Counter Panel\nThe `DashboardPanelCounter` shows a value count or functions as a text-only Panel."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Counter Panel - Text Only\nTo create a Panel with only the Dashboard title, you can use the following example:\n\n```python\nproject.dashboard.add_panel(\n        DashboardPanelCounter(\n            title=\"LLM chatbot monitoring\",\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\n            agg=CounterAgg.NONE,\n            size=WidgetSize.FULL,\n        ),\n        tab=\"Overview\"\n    )\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Counter Panel - Value Sum\nTo create a Panel that sums up the number of rows over time, use the following example:\n\n```python\nproject.dashboard.add_panel(\n        DashboardPanelCounter(\n            title=\"Model calls\",\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\n            value=PanelValue(\n                metric_args={\"metric.metric_id\": RowCount().metric_id},\n                field_path=\"value\",\n                legend=\"count\",\n            ),\n            text=\"count\",\n            agg=CounterAgg.SUM,\n            size=WidgetSize.HALF,\n        ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Counter Panel - Last Value\nTo show the number of rows in the last Report, use this example:\n\n```python\nproject.dashboard.add_panel(\n        DashboardPanelCounter(\n            title=\"Row number: last run\",\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\n            value=PanelValue(\n                metric_args={\"metric.metric_id\": RowCount().metric_id},\n                field_path=\"value\",\n                legend=\"count\",\n            ),\n            text=\"count\",\n            agg=CounterAgg.LAST,\n            size=WidgetSize.HALF,\n        ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Counter Panel Parameters\nThe parameters for the `DashboardPanelCounter` are as follows:\n\n| Parameter                     | Description                                                                                                                                                                                                                                                   |\n|"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "- |\n| `value: Optional[PanelValue]` | Specifies the value to display. If left empty, displays a text-only panel.                                                                                                                                                                                |\n| `text: Optional[str]`         | Supporting text to display.                                                                                                                                                                                                                                   |\n| `agg: CounterAgg`             | Data aggregation options: `SUM`, `LAST`, `NONE`.                                                                                                                                                                                                             |"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Plot Panel\nThe `DashboardPanelPlot` shows individual values over time."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Plot Panel - Single Value\nTo plot a single row count as a LINE plot, use the following example:\n\n```python\nproject.dashboard.add_panel(\n        DashboardPanelPlot(\n            title=\"Row count\",\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\n            values=[\n                PanelValue(\n                    metric_args={\"metric.metric_id\": RowCount().metric_id},\n                    field_path=\"value\",\n                    legend=\"count\",\n                ),\n            ],\n            plot_type=PlotType.LINE,\n            size=WidgetSize.HALF,\n        ),\n        tab=\"Overview\"\n    )\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Plot Panel - Multiple Values\nTo plot min, max, and mean values on the same plot, use this example:\n\n```python\nproject.dashboard.add_panel(\n        DashboardPanelPlot(\n            title=\"Length\",\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\n            values=[\n                PanelValue(\n                    metric_args={\"metric.metric_id\": MinValue(column=\"length\").metric_id},\n                    field_path=\"value\",\n                    legend=\"min\",\n                ),\n                PanelValue(\n                    metric_args={\"metric.metric_id\": MeanValue(column=\"length\").metric_id},\n                    field_path=\"value\",\n                    legend=\"mean\",\n                ),\n                PanelValue(\n                    metric_args={\"metric.metric_id\": MaxValue(column=\"length\").metric_id},\n                    field_path=\"value\",\n                    legend=\"max\",\n                ),\n            ],\n            plot_type=PlotType.LINE,\n            size=WidgetSize.HALF,\n        ),\n        tab=\"Overview\"\n    )\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Plot Panel Parameters\nThe parameters for the `DashboardPanelPlot` are as follows:\n\n| Parameter                  | Description                                                                                                                                                                                                                                                                                                                                                                                |\n|"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "|\n| `values: List[PanelValue]` | Specifies the value(s) to display in the Plot. You can point to a named **Metric** and a specific **result**. You can pass multiple values for display together.                                                                                                                                                                                                             |\n| `plot_type: PlotType`      | Specifies the plot type (e.g., `SCATTER`, `BAR`, `LINE`, `HISTOGRAM`).                                                                                                                                                                                                                                                                                                                      |"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Distribution Panel\nThe `DashboardPanelDistribution` shows changes in the distribution over time, relevant for categorical columns."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Example of Distribution Panel\nTo plot the distribution of the \"refusals\" column, use this example:\n\n```python\nproject.dashboard.add_panel(\n        DashboardPanelDistribution(\n            title=\"Is the context valid? (group)\",\n            value=PanelValue(\n                field_path=\"counts\", \n                metric_args={\"metric.metric_id\": UniqueValueCount(column=\"context quality\").metric_id}\n                ),\n            filter=ReportFilter(metadata_values={}, tag_values=[]),\n            barmode=\"group\",\n            size=WidgetSize.FULL,\n        ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Distribution Panel Parameters\nThe parameters for the `DashboardPanelDistribution` are as follows:\n\n| Parameter              | Description                                                                                                                                                                 |\n|"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "- |"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "|\n| `value: PanelValue`    | Specifies the distribution to display on the Panel, pointing to a named **Metric** with `field_path=\"counts\"`.                                                           |\n| `barmode: HistBarMode` | Specifies the distribution plot type (`stack`, `group`, `overlay`, `relative`).                                                                                           |"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Test Counter Panel\nThe `DashboardPanelTestSuiteCounter` shows a counter with Test results."
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Test Counter Panel - All Tests\nTo display results of the latest Test Suite, use the following example:\n\n```python\nproject.dashboard.add_panel(\n    DashboardPanelTestSuiteCounter(\n        title=\"Latest Test suite: results\",\n        agg=CounterAgg.LAST,\n    ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Test Counter Panel - Specific Test (All Time)\nTo show all failures of a specific Test over time, use this example:\n\n```python\nproject.dashboard.add_panel(\n    DashboardPanelTestSuiteCounter(\n        title=\"Empty Rows Test (Total Failed)\",\n        test_filters=[\n            TestFilter(test_args={\"test.metric_fingerprint\": EmptyRowsCount().metric_id})\n            ],\n        statuses=[TestStatus.FAIL]\n    ),\n    tab=\"Tests\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Test Counter Panel - Specific Test (Last Result)\nTo show the last result of a specific Test, use this example:\n\n```python\nproject.dashboard.add_panel(\n    DashboardPanelTestSuiteCounter(\n        title=\"Empty Rows Test (Last result)\",\n        test_filters=[\n            TestFilter(test_args={\"test.metric_fingerprint\": EmptyRowsCount().metric_id})\n            ],\n        agg=CounterAgg.LAST\n    ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Empty Rows Test (Total Failed)\nTo show the total count of failed results for the Empty Rows Test, use the following code snippet. This will save the project with the total failed status displayed on the specified tab.\n\n```python\nproject.dashboard.add_panel(\n    DashboardPanelTestSuiteCounter(\n        title=\"Empty Rows Test (Total Failed)\",\n        test_filters=[\n            TestFilter(test_args={\"test.metric_fingerprint\": EmptyRowsCount().metric_id})\n        ],\n        statuses=[TestStatus.FAIL]\n    ),\n    tab=\"Tests\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Specific Test (Last Result)\nTo display the last result of a specific Test (defaults to showing success status), implement the following code. This saves the project's latest result in a dedicated overview tab.\n\n```python\nproject.dashboard.add_panel(\n    DashboardPanelTestSuiteCounter(\n        title=\"Empty Rows Test (Last result)\",\n        test_filters=[\n            TestFilter(test_args={\"test.metric_fingerprint\": EmptyRowsCount().metric_id})\n        ],\n        agg=CounterAgg.LAST\n    ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Panel Parameters\nThe following parameters can be used when configuring dashboard panels:\n\n| Parameter                                     | Description                                                                                                                                                  |\n|"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "-- |\n| `test_filters: List[TestFilter]=[]`           | Select specific Test(s). Without a filter, the Panel considers results from all Tests.                                                                    |\n| `statuses: List[statuses]`                    | Status filters for selecting Tests, such as `TestStatus.ERROR`, `TestStatus.FAIL`, `TestStatus.SUCCESS` (default), `TestStatus.WARNING`, `TestStatus.SKIPPED`. |\n| `agg: CounterAgg`                             | Data aggregation options, e.g., `SUM` (default) calculates the sum of Test results and `LAST` displays the last available Test result.                     |"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Test Plot Overview\nThe `DashboardPanelTestSuite` component portrays Test results over time and can be used in various configurations.\n\n<CardGroup cols={2}>\n  <Card title=\"Detailed plot\" img=\"/images/dashboard/panel_tests_detailed_hover_example.png\">\n    `TestSuitePanelType.DETAILED`. Displays individual Test results.\n  </Card>\n  <Card title=\"Aggregated plot\" img=\"/images/dashboard/panel_tests_aggregated_hover_example.png\">\n    `TestSuitePanelType.AGGREGATE`. Shows total number of Tests by status.\n  </Card>\n</CardGroup>"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Detailed Test Display \nTo show detailed results of all Tests in a Project, the following code snippet can be used:\n\n```python\nproject.dashboard.add_panel(\n    DashboardPanelTestSuite(\n        title=\"All tests: detailed\",\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\n        size=WidgetSize.FULL,\n        panel_type=TestSuitePanelType.DETAILED\n    ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Selected Test Results\nFor displaying the results of selected Tests with detail, use this code snippet:\n\n```python\nproject.dashboard.add_panel(\n    DashboardPanelTestSuite(\n        title=\"Selected tests (missing)\",\n        test_filters=[\n            TestFilter(test_args={\"test.metric_fingerprint\": MissingValueCount(column=\"length\").metric_id}),\n            TestFilter(test_args={\"test.metric_fingerprint\": MissingValueCount(column=\"sentiment\").metric_id})\n        ],\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\n        size=WidgetSize.HALF,\n        panel_type=TestSuitePanelType.DETAILED,\n    ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Aggregated Test Results\nTo show the aggregate results for all Tests, this code can be deployed:\n\n```python\nproject.dashboard.add_panel(\n    DashboardPanelTestSuite(\n        title=\"All tests: aggregated\",\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\n        size=WidgetSize.FULL,\n        panel_type=TestSuitePanelType.AGGREGATE\n    ),\n    tab=\"Overview\"\n)\nproject.save()\n```"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## All Panel Parameters\nHere are the parameters applicable to test panels:\n\n| Parameter                                                     | Description                                                                                                                   |\n|"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "- |\n| `test_filters: List[TestFilter]`                             | Select specific Test(s). Without a filter, the Panel displays results from all Tests.                                         |\n| `statuses: List[statuses]`                                   | Status filters for selecting Tests with specific outcomes. Defaults display all Test statuses.                                  |\n| `panel_type=TestSuitePanelType`                              | Defines Panel type: **Detailed** shows individual Test results, while **Aggregate** shows total number of Tests by status.     |\n| `time_agg: Optional[str] = None`                             | Groups all Test results by specific periods, e.g. `1D` for daily results.                                                     |"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Metric ID Usage\nTo specify the Metric or Test for plotting on a Panel, use `test_filters` or `metric_args` with `metric_id` or `metric_fingerprint`. Be sure to use the Metric name that was originally logged to the Project.\n\n**Field Path**: In Metric Panels, the `field_path` specifies which result to focus on, with options including `value`, `share`/`count`, or `values`.\n\n| Field Path        | Description                                           | Applicable Metrics                                        | Applicable Panels                       |\n|"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "|"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "|\n| `value`           | Refers to a single result from the Metric.            | Most Metrics                                            | Counter, Plot                           |\n| `share` / `count` | Refers to an absolute count or percentage value.    | Metrics like `MissingValueCount`                        | Counter, Plot                           |\n| `shares` / `counts`| Refers to histogram visualizations in Metrics.     | Metrics with histogram visuals like `UniqueValueCount`. | Distribution                             |"
  },
  {
    "title": "Dashboard panel types [Legacy]",
    "description": "Overview of the available monitoring Panels.",
    "filename": "docs-main/docs/platform/dashboard_panel_types.mdx",
    "ai_section": "## Verifying Metric Results\nTo verify the result of a specific Metric, check the [All Metrics table](/metrics/all_metrics), or generate a Report using the Metric, export it to JSON, and verify the value it returns. \n\nWhen using Evidently Cloud, available fields show in a drop-down menu as you add a new Panel."
  },
  {
    "title": "Synthetic data",
    "description": "Generating synthetic data.",
    "filename": "docs-main/docs/platform/datasets_generate.mdx",
    "ai_section": "## Synthetic Data Generation Overview\nSynthetic data generation is an add-on available on some Evidently Cloud and Enterprise plans. For detailed information, refer to the [pricing](https://www.evidentlyai.com/pricing) page. To explore extended trial access, you can [request a demo](https://www.evidentlyai.com/get-demo) or contact sales at sales@evidentlyai.com. Additionally, there is an option to apply for a [startup discount](https://www.evidentlyai.com/sign-up-startups)."
  },
  {
    "title": "Synthetic data",
    "description": "Generating synthetic data.",
    "filename": "docs-main/docs/platform/datasets_generate.mdx",
    "ai_section": "## How to Use Synthetic Data Feature\nTo utilize the synthetic data feature, follow these steps:\n1. Create a Project.\n2. Set up an API key for Open AI.\n3. Open \"Datasets\" and choose \"Generate Dataset.\"\n\nSynthetic data can be employed to augment your test scenarios as you assess the performance of your AI system."
  },
  {
    "title": "Synthetic data",
    "description": "Generating synthetic data.",
    "filename": "docs-main/docs/platform/datasets_generate.mdx",
    "ai_section": "## Documentation for Synthetic Data\nFor further exploration of the synthetic data functionality, you can visit the separate [docs section](/synthetic-data/introduction)."
  },
  {
    "title": "Synthetic data",
    "description": "Generating synthetic data.",
    "filename": "docs-main/docs/platform/datasets_generate.mdx",
    "ai_section": "## Video Resource\nCheck out the video demonstrating the basic flow from our **LLM evaluation course**. You can watch it here:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gKp9K7Etv4A\" title=\"No-code data generation\" frameborder=\"0\" allowfullscreen />"
  },
  {
    "title": "Overview",
    "description": "Introduction to Datasets.",
    "filename": "docs-main/docs/platform/datasets_overview.mdx",
    "ai_section": "## Availability of Datasets\nDatasets are available in **Evidently Cloud** and **Evidently Enterprise**."
  },
  {
    "title": "Overview",
    "description": "Introduction to Datasets.",
    "filename": "docs-main/docs/platform/datasets_overview.mdx",
    "ai_section": "## What is a Dataset?\n**Datasets** are collections of data from your application used for analysis and automated checks. You can bring in existing datasets, capture live data, or create synthetic datasets."
  },
  {
    "title": "Overview",
    "description": "Introduction to Datasets.",
    "filename": "docs-main/docs/platform/datasets_overview.mdx",
    "ai_section": "## How to Create a Dataset?\nYou can add Datasets to the platform in multiple ways:\n\n* **Upload directly**. Use the UI to upload CSV files or push datasets via the Python API.\n* **Upload with Reports**. Attach datasets to Reports when running local evaluations. This is optional â€” you can also upload only summary metrics.\n* **Generate synthetic data**. Use built-in platform features to generate synthetic evaluation datasets.\n* **Create from Traces**. During tracing, Evidently automatically generates tabular datasets that can be used for evaluations.\n\n<Tip>\n  **Where do I find the data?** To view all datasets (uploaded, synthetic, or evaluation results), go to the \"Dataset\" page in your Project menu. For raw tracing datasets, check the Tracing section.\n</Tip>"
  },
  {
    "title": "Overview",
    "description": "Introduction to Datasets.",
    "filename": "docs-main/docs/platform/datasets_overview.mdx",
    "ai_section": "## Synthetic Data\nYou can synthesize evaluation datasets directly in the Evidently Platform:\n\n* **Generate from examples or description**. Describe specific test scenarios and generate matching datasets.\n* **Generate from source documents**. Generate Q&A pairs from source documents like PDF, CSV, or markdown files (great for RAG evaluations).\n\nAfter creating or uploading datasets, you can edit or diversify them further using the \"more like this\" feature."
  },
  {
    "title": "Overview",
    "description": "Introduction to Datasets.",
    "filename": "docs-main/docs/platform/datasets_overview.mdx",
    "ai_section": "## When Do You Need Datasets?\nHere are common use cases for datasets in Evidently:\n\n* **Organize evaluation datasets**. Save curated datasets with expected inputs and optional ground truth outputs. You can bring in domain experts to collaborate on these datasets in the UI, and access them programmatically for CI/CD checks.\n* **Debug evaluation results**. After you run an evaluation, view the dataset to identify and debug specific failures. E.g., you can sort all text outputs by added scores.\n* **Store ML inference logs or LLM traces**. Collect raw data from production or experimental runs, use it as a source of truth, and run evaluations over it."
  },
  {
    "title": "Work with datasets",
    "description": "How to create, upload and manage Datasets.",
    "filename": "docs-main/docs/platform/datasets_workflow.mdx",
    "ai_section": "## Connection and Project Setup\nBefore you can upload any datasets, you must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage)."
  },
  {
    "title": "Work with datasets",
    "description": "How to create, upload and manage Datasets.",
    "filename": "docs-main/docs/platform/datasets_workflow.mdx",
    "ai_section": "## Upload a Dataset\n### Python Method\nPrepare your dataset as an Evidently Dataset with the corresponding data definition. To upload a Dataset to the specified Project in workspace `ws`, use the `add_dataset` method:\n\n```python\neval_data = Dataset.from_pandas(\n    source_df,\n    data_definition=DataDefinition()\n)\nws.add_dataset(\n    dataset = eval_data, \n    name = \"dataset_name\",\n    project_id = project.id, \n    description = \"Optional description\")\n```\n\nYou must always specify the dataset `name` that you will see in the UI. The description is optional.\n\n### UI Method\nTo upload any existing dataset as a CSV file, click on \"Add dataset\". When you upload the Dataset, you must also add a [**data definition**](/docs/library/data_definition). This allows Evidently to understand the role of specific columns and prepare your Dataset for future evaluations."
  },
  {
    "title": "Work with datasets",
    "description": "How to create, upload and manage Datasets.",
    "filename": "docs-main/docs/platform/datasets_workflow.mdx",
    "ai_section": "## How to Create an Evidently Dataset\nTo learn how to create an Evidently Dataset, refer to the [Data Definition docs](../library/data-definition)."
  },
  {
    "title": "Work with datasets",
    "description": "How to create, upload and manage Datasets.",
    "filename": "docs-main/docs/platform/datasets_workflow.mdx",
    "ai_section": "## Download the Dataset\nYou can pull the Dataset stored or generated on the platform to your local environment, which is useful for CI/CD testing scripts. Use the `load_dataset` method:\n\n```python\neval_dataset = ws.load_dataset(dataset_id = \"YOUR_DATASET_ID\") \n\n# to create a pandas dataframe\ndf = eval_dataset.as_dataframe()\n```"
  },
  {
    "title": "Work with datasets",
    "description": "How to create, upload and manage Datasets.",
    "filename": "docs-main/docs/platform/datasets_workflow.mdx",
    "ai_section": "## Include the Dataset in Reports\nYou can include Datasets when uploading Reports to the platform. This way, after running an evaluation locally, you can simultaneously upload:\n\n- the Report with evaluation result,\n- the Dataset it was generated for, with new added scores if applicable.\n\nBy default, only the Report is uploaded. To include the Dataset, use the `include_data` parameter:\n\n```python\nws.add_run(project.id, data_report, include_data=True)\n```\n\nFor more details, check the docs on [running evals via API](/docs/platform/evals_api)."
  },
  {
    "title": "Run evals via API",
    "description": "How to run evals and log them on the platform",
    "filename": "docs-main/docs/platform/evals_api.mdx",
    "ai_section": "## Overview of Evaluation API\nThis section provides a brief overview of the core evaluation API of the Evidently Python library. For a more detailed understanding, refer to the [detailed guide](/docs/library/evaluations_overview)."
  },
  {
    "title": "Run evals via API",
    "description": "How to run evals and log them on the platform",
    "filename": "docs-main/docs/platform/evals_api.mdx",
    "ai_section": "## Simple Example\nTo run a single evaluation with text evaluation results uploaded to a workspace, you must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).\n\nHere's how you can execute the evaluation:\n\n```python\neval_data = Dataset.from_pandas(\n    source_df,\n    data_definition=DataDefinition()\n)\n\nreport = Report([\n    TextEvals()\n])\n\nmy_eval = report.run(eval_data, None)\nws.add_run(project.id, my_eval, include_data=True)\n```"
  },
  {
    "title": "Run evals via API",
    "description": "How to run evals and log them on the platform",
    "filename": "docs-main/docs/platform/evals_api.mdx",
    "ai_section": "## Workflow Steps\nThe complete workflow for executing evaluations consists of several steps:\n\n1. **Run a Report**: Configure the evals and run the [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions.\n2. **Upload to the platform**: Upload the raw data or only the evaluation results.\n3. **Explore the results**: Navigate to the Explore view inside your Project to debug results and compare outcomes between runs. Understand the [Explore view](/docs/platform/evals_explore).\n4. **(Optional) Set up a Dashboard**: Create a Dashboard to monitor metric changes over time. Refer to the [Dashboard](/docs/platform/dashboard_overview) documentation.\n5. **(Optional) Configure alerts**: Optionally, set up alerts on failed Tests. Learn more about [Alerts](/docs/platform/alerts)."
  },
  {
    "title": "Run evals via API",
    "description": "How to run evals and log them on the platform",
    "filename": "docs-main/docs/platform/evals_api.mdx",
    "ai_section": "## Uploading Data\nWhen uploading a Report, you have the option to include either just the summary Metrics or the raw Dataset you evaluated. \n\n**Important Note**: Raw data upload is available only for Evidently Cloud and Enterprise.\n\nYou can choose to upload:\n\n- Only the resulting Metrics and a summary Report (with distribution summaries, etc.)\n- The raw Dataset along with any additional Descriptors for row-level debugging and analysis.\n\nUse `include_data` (default `False`) to specify whether to include the data:\n\n```python\nws.add_run(project.id, my_eval, include_data=False)\n```"
  },
  {
    "title": "Explore view",
    "description": "Reviewing the evaluation results on the Platform.",
    "filename": "docs-main/docs/platform/evals_explore.mdx",
    "ai_section": "## Evaluation Report Overview\nThe result of each evaluation is a Report, which is a summary of metrics and includes visuals. There is also an optional Test Suite that can accompany the Report, containing pass/fail results based on set conditions."
  },
  {
    "title": "Explore view",
    "description": "Reviewing the evaluation results on the Platform.",
    "filename": "docs-main/docs/platform/evals_explore.mdx",
    "ai_section": "## Browsing Evaluation Results\nTo access the results of your evaluations, enter your Project and navigate to the \"Reports\" section in the left menu. Here, you can view all your evaluation artifacts and browse them by Tags, time, or metadata. Additionally, you have the option to download the reports in HTML or JSON formats."
  },
  {
    "title": "Explore view",
    "description": "Reviewing the evaluation results on the Platform.",
    "filename": "docs-main/docs/platform/evals_explore.mdx",
    "ai_section": "## Exploring Individual Reports\nTo see and compare the evaluation results of a specific Report, click on \"Explore\" next to the individual Report. This will provide you with the Report or Test Suite, and if available, the dataset linked to the evaluation. \n\n- To view just the Report, click on the \"Dataset\" sign at the top to hide the dataset.\n- To focus on the Dataset, choose \"Go to dataset\"."
  },
  {
    "title": "Explore view",
    "description": "Reviewing the evaluation results on the Platform.",
    "filename": "docs-main/docs/platform/evals_explore.mdx",
    "ai_section": "## Comparing Multiple Reports\nYou can analyze multiple evaluation results side by side by selecting them from the Report list and clicking the \"Compare\" button. This feature allows you to identify differences in performance, quality, or behavior across different model versions or configurations.\n\nWhen in the Compare view, you can explore various metric scores or pass/fail test results side by side."
  },
  {
    "title": "Explore view",
    "description": "Reviewing the evaluation results on the Platform.",
    "filename": "docs-main/docs/platform/evals_explore.mdx",
    "ai_section": "## Duplicating Snapshots for Comparison\nWhile viewing a specific Report, you can click on \"duplicate snapshot\" to maintain the current Metric in view. Afterwards, you can select a different Report for comparison, facilitating a detailed analysis."
  },
  {
    "title": "Explore view",
    "description": "Reviewing the evaluation results on the Platform.",
    "filename": "docs-main/docs/platform/evals_explore.mdx",
    "ai_section": "## Tracking Progress Over Time\nAs you conduct multiple evaluations, you can build a Dashboard to track your progress and observe performance improvements over time. This Dashboard will enable you to visualize results from multiple Reports within a Project, helping you monitor how tests are performing over time."
  },
  {
    "title": "No code evals",
    "description": "How to evaluate your data in a no-code interface.",
    "filename": "docs-main/docs/platform/evals_no_code.mdx",
    "ai_section": "## Prepare the Dataset\nBefore you start, create a Project and prepare the Dataset to evaluate. There are two options:\n\n- **Upload a CSV**: Enter the \"Dataset\" menu, click on \"Create new dataset from CSV\". Drag and drop your Dataset. You must also specify the data definition when you upload it.\n- **Use an existing Dataset**: Select a Dataset you previously uploaded to the platform or one collected through [Tracing](tracing_overview).\n\n<Note>\n  **What are Datasets?** Learn how to manage and upload [Datasets](datasets_overview) to the platform.\n</Note>\n\n<Note>\n  **What is Data Definition?** Understand how to set your dataset schema in the [Data Definition](../library/data-definition).\n</Note>"
  },
  {
    "title": "No code evals",
    "description": "How to evaluate your data in a no-code interface.",
    "filename": "docs-main/docs/platform/evals_no_code.mdx",
    "ai_section": "## Start an Evaluation\nWhile you are viewing the Dataset, you can click on \"Add descriptors\" on the right.\n\n**(Optional) Add the LLM provider API key**: Add a token in the â€œSecretsâ€ menu section if you plan to use an LLM for evaluations. You can proceed without it, using other types of evals."
  },
  {
    "title": "No code evals",
    "description": "How to evaluate your data in a no-code interface.",
    "filename": "docs-main/docs/platform/evals_no_code.mdx",
    "ai_section": "## Configure the Evaluation\nYou must choose which column to evaluate and how. You can choose from the following methods:\n\n- **Model-based**: use built-in machine learning models, like sentiment analysis.\n- **Regular expressions**: check for specific words or patterns.\n- **Text stats**: measure stats like the number of symbols or sentences.\n- **LLM-based**: use external LLMs to evaluate your text data.\n\nSelect specific checks one by one. Each evaluation result is called a **Descriptor**. No matter the method, youâ€™ll get a label or score for every evaluated text. Some descriptors, like â€œSentiment,â€ work instantly, while others may need setup.\n\n<Note>\n  **What other evaluators are there?** Check the list of [All Descriptors](../metrics/all_descriptors).\n</Note>"
  },
  {
    "title": "No code evals",
    "description": "How to evaluate your data in a no-code interface.",
    "filename": "docs-main/docs/platform/evals_no_code.mdx",
    "ai_section": "## Words Presence Descriptor\n**Include Words**: This Descriptor checks for listed words and returns \"True\" or \"False.\"\n\nSet up these parameters:\n- Add a list of words.\n- Choose whether to check for â€œanyâ€ or â€œallâ€ of the words present.\n- Set the **lemmatize** parameter to check for inflected and variant words automatically.\n- Give your check a name so you can easily find it in your results.\n\nExample setup:\n\n![](/images/nocode_includes_words-min.png)"
  },
  {
    "title": "No code evals",
    "description": "How to evaluate your data in a no-code interface.",
    "filename": "docs-main/docs/platform/evals_no_code.mdx",
    "ai_section": "## Semantic Similarity Descriptor\n**Semantic Similarity**: This Descriptor converts texts to embeddings and calculates Cosine Similarity between your evaluated column and another column. It scores from 0 to 1 (0: completely different, 0.5: unrelated, 1: identical). Itâ€™s useful for checking if responses are semantically similar to a question or reference.\n\nSelect the column to compare against:\n\n![](/images/nocode_semantic_similarity-min.png)"
  },
  {
    "title": "No code evals",
    "description": "How to evaluate your data in a no-code interface.",
    "filename": "docs-main/docs/platform/evals_no_code.mdx",
    "ai_section": "## LLM as a Judge\n**Custom LLM evaluator**: If you've added your token, use LLM-based evaluations (built-in or custom) to send your texts to LLMs for grading or scoring. You can choose a specific LLM model from the provider.\n\nFor example, you can create a custom evaluator to classify texts as â€œcheerfulâ€ or â€œneutral.â€ Fill in the parameters, and Evidently will generate the evaluation prompt.\n\nFor a binary classification template, you can configure:\n- **Criteria**: define custom criteria in free text to clarify the classification task.\n- **Target/Non-target Category**: labels you want to use.\n- **Uncertain Category**: how the model should respond when it canâ€™t decide.\n- **Reasoning**: choose to include an explanation (Recommended).\n- **Category** and/or **Score**: have the LLM respond with the category (Recommended) or score.\n- **Visualize as**: when both Category and Score are computed, choose which to display in the Report.\n\nTo add evaluations for another column in the same Report, click â€œAdd Preset,â€ select â€œText Evals,â€ and follow the same steps for the new column. You can include evals for multiple columns at once."
  },
  {
    "title": "No code evals",
    "description": "How to evaluate your data in a no-code interface.",
    "filename": "docs-main/docs/platform/evals_no_code.mdx",
    "ai_section": "## Run the Evaluation\nClick â€œRun calculation,â€ and the calculation will start! It may take some time to process, especially on a large dataset. You can check the status of the evaluation in the â€œTasksâ€ (use the left menu to navigate).\n\nOnce your evaluation is complete, you can view the new dataset with the results."
  },
  {
    "title": "Overview",
    "description": "Running evals on the platform.",
    "filename": "docs-main/docs/platform/evals_overview.mdx",
    "ai_section": "## Evaluations in AI Product Development\nYou may need evaluations at different stages of your AI product development. These include:\n\n* **Ad hoc analysis:** Spot-check the quality of your data or AI outputs.\n* **Experiments:** Test different parameters, models, or prompts and compare outcomes.\n* **Safety and adversarial testing:** Evaluate how your system handles edge cases and adversarial inputs, including on synthetic data.\n* **Regression testing:** Ensure the performance does not degrade after updates or fixes.\n* **Monitoring:** Track the response quality for production systems.\n\nEvidently supports all these workflows. You can run evaluations locally or directly on the platform."
  },
  {
    "title": "Overview",
    "description": "Running evals on the platform.",
    "filename": "docs-main/docs/platform/evals_overview.mdx",
    "ai_section": "## Evaluations via API\nEvaluations can be performed via API and are supported in `Evidently OSS`, `Evidently Cloud`, and `Evidently Enterprise`. This method is ideal for experiments, CI/CD workflows, or custom evaluation pipelines.\n\n**How it works**:\n* Run Python-based evaluations on your AI outputs by generating Reports.\n* Upload results to the Evidently Platform.\n* Use the Explore feature to compare and debug results between runs.\n\n**Next step:** check the Quickstart for [ML](/quickstart_ml) or [LLM](/quickstart_llm)."
  },
  {
    "title": "Overview",
    "description": "Running evals on the platform.",
    "filename": "docs-main/docs/platform/evals_overview.mdx",
    "ai_section": "## No-code Evaluations\nNo-code evaluations are supported in `Evidently Cloud` and `Evidently Enterprise`. This option lets you run evaluations directly in the user interface, making it great for non-technical users or for those who prefer to utilize Evidently's infrastructure.\n\n**How it works**:\n* **Analyze CSV datasets:** Drag and drop CSV files and evaluate their contents on the Platform.\n* **Evaluate uploaded datasets:** Assess collected [traces](/docs/platform/tracing_overview) from instrumented LLM applications or any [Datasets](/docs/platform/datasets_overview) you previously uploaded or generated.\n\nNo-code workflows create the same Reports or Test Suites you'd generate using Python, with the subsequent workflow remaining the same. After running your evaluations through any method, you can access results in the Explore view for further analysis.\n\n**Next step:** check the Guide for [No-code evals](/docs/platform/evals_no_code)."
  },
  {
    "title": "Batch monitoring",
    "description": "How to run batch evaluation jobs.",
    "filename": "docs-main/docs/platform/monitoring_local_batch.mdx",
    "ai_section": "## Overview of Batch Monitoring\nBatch monitoring relies on the core evaluation API of the Evidently Python library. For a comprehensive understanding, refer to the [detailed guide](/docs/library/evaluations_overview)."
  },
  {
    "title": "Batch monitoring",
    "description": "How to run batch evaluation jobs.",
    "filename": "docs-main/docs/platform/monitoring_local_batch.mdx",
    "ai_section": "## Simple Example\nTo get the dataset statistics for a single batch and upload it to the workspace, you must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage). Hereâ€™s a simple code example to achieve this:\n\n```python\neval_data = Dataset.from_pandas(\n    pd.DataFrame(source_df),\n    data_definition=DataDefinition()\n)\n\nreport = Report([\n    DatasetStats()\n])\n\nmy_eval = report.run(eval_data, None)\nws.add_run(project.id, my_eval, include_data=False)\n```"
  },
  {
    "title": "Batch monitoring",
    "description": "How to run batch evaluation jobs.",
    "filename": "docs-main/docs/platform/monitoring_local_batch.mdx",
    "ai_section": "## Workflow Steps\nThe complete workflow for batch monitoring involves several key steps:\n\n### 1. Configure the metrics\nDefine an [Evidently Report](/docs/library/report) with optional [Test](/docs/library/tests) conditions to outline the evaluations.\n\n### 2. Run the evals\nIndependently execute Reports on a chosen schedule. Consider using tools like Airflow. You can send Reports from various pipeline steps, such as sending data quality checks, data drift, and prediction drift checks first, and later sending ML quality checks results after acquiring delayed labels. Custom timestamps can be used for backdating your Reports.\n\n### 3. Upload to the platform\nDecide whether to store raw inferences or just upload metric summaries. For more details, refer to [how to upload / delete results](/docs/platform/evals_api).\n\n### 4. Configure the Dashboard\nSet up a Dashboard to monitor results over time. Utilize pre-built Tabs or customize your monitoring Panels. Refer to the [Dashboard guide](/docs/platform/dashboard_overview).\n\n### 5. Configure alerts\nEstablish alerts based on Metric values or Test failures. More information is available in the section on [Alerts](/docs/platform/alerts)."
  },
  {
    "title": "Batch monitoring",
    "description": "How to run batch evaluation jobs.",
    "filename": "docs-main/docs/platform/monitoring_local_batch.mdx",
    "ai_section": "## Tips on Running Tests vs Reports\nStructuring your evaluations as Tests instead of monitoring numerous metrics simultaneously can reduce alert fatigue and simplify configuration. This approach allows for efficient verification that all columns in the input data are within a specified min-max range."
  },
  {
    "title": "Overview",
    "description": "How production AI quality monitoring works.",
    "filename": "docs-main/docs/platform/monitoring_overview.mdx",
    "ai_section": "## AI Observability Overview\nAI observability lets you evaluate the quality of the inputs and outputs of your AI application while it runs in production. This provides an up-to-date view of your system's behavior, helping to spot and fix issues. Evidently offers various ways to set up monitoring for your AI applications."
  },
  {
    "title": "Overview",
    "description": "How production AI quality monitoring works.",
    "filename": "docs-main/docs/platform/monitoring_overview.mdx",
    "ai_section": "## Batch Monitoring Jobs\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that donâ€™t need instant quality evaluations.\n\n**How it works**:\n\n* **Build your evaluation pipeline**. Create a pipeline to run monitoring jobs; this can be a Python script, cron job, or orchestration tool like Airflow. Schedule it regularly (e.g., hourly, daily) or trigger it with new data.\n\n* **Run metric calculations**. Use the Evidently Python library to implement the evaluation step, selecting evals and computing `Reports` that summarize metrics and test results.\n\n* **Store and visualize the results**. Store Report runs in Evidently Cloud or a self-hosted workspace, and visualize results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**. Only aggregated data summaries and test results are retained by default to protect data privacy.\n\n* **Full control over the evaluation pipeline**. You choose when evaluations happen, which is ideal for batch ML models.\n\n* **Fits most ML evaluation scenarios**. Evaluations like data drift detection often work better in batches, allowing for meaningful analysis at intervals.\n\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch)."
  },
  {
    "title": "Overview",
    "description": "How production AI quality monitoring works.",
    "filename": "docs-main/docs/platform/monitoring_overview.mdx",
    "ai_section": "## Tracing with Scheduled Evals\n<Check>\n  Supported in: `Evidently Cloud` and `Evidently Enterprise`. Scheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\n</Check>\n\n**Best for**: LLM-powered applications\n\n**How it works:**\n\n* **Instrument your app**. Use the `Tracely` library to capture all relevant data, including inputs and outputs. \n\n* **Store raw data**. Evidently Platform retains all raw data, providing a comprehensive activity record.\n\n* **Schedule evaluations**. Set evaluations to run automatically, generating Reports or running Tests on the platform, with options to run them manually anytime.\n\n**Benefits of this approach**:\n\n* **Solves the data capture**. It compiles complex traces and production data, making it easier to manage and analyze.\n\n* **Easy to re-run evals**. With stored raw traces, evaluations can be quickly re-run or updated with new metrics.\n\n* **No-code**. After setting up trace instrumentation, everything can be managed through the UI.\n\n**Next step:** check the [Tracing Quickstart](/quickstart_tracing)."
  },
  {
    "title": "Scheduled evals",
    "description": "Running managed evaluations over traces on a platform.",
    "filename": "docs-main/docs/platform/monitoring_scheduled_evals.mdx",
    "ai_section": "## Scheduled Evaluations on Evidently Cloud\nScheduled evaluations are currently in beta on Evidently Cloud. If you're interested in trying it out, please contact our team for more information."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Overview of Evidently Platform\nEvidently Platform helps you manage AI quality across the AI system lifecycle, from pre-deployment testing to production monitoring. It supports evaluations of open-ended LLM outputs, predictive tasks like classification, and complex workflows like AI agents."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Key Features\nEvidently Platform offers a lightweight open-source version for evaluation tracking and monitoring, as well as a Cloud/Enterprise version that includes extra features. Users can check feature availability through a provided link."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Evaluations\nRun evaluations locally with the Evidently Python library or use the no-code interface on the platform. The system provides over 100 built-in evaluations and templates to track, compare, and debug experiments."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Datasets\nManage and organize testing and production datasets. Store datasets on the platform alongside relevant evaluations and collaborate to curate test cases effectively."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Synthetic Data\nGenerate synthetic data for various use cases, including RAG and Q&A. Users can design test scenarios, edge cases, and adversarial inputs for safety evaluations and stress-testing."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Regression Testing\nCombine evaluations in conditional Test Suites with Pass/Fail outcomes. Users can set alerts for failed Tests and track results over time using the built-in dashboard."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Monitoring\nRun evaluations for live systems in batch or real-time. Track results using a dashboard, connect back to raw data as necessary, and set alerts for any violations detected."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Tracing\nInstrument your AI application to collect inputs, outputs, and any intermediate steps. This feature automatically generates a structured dataset for analysis, ready for use."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Programmatic Access\nWhile many workflows can be executed no-code directly on the platform, programmatic access is often needed for tasks like uploading datasets or running local experimental evaluations. Users can utilize the Evidently Python library for these interactions."
  },
  {
    "title": "Introduction",
    "description": "Evidently Platform at a glance.",
    "filename": "docs-main/docs/platform/overview.mdx",
    "ai_section": "## Collecting Input-Outputs\nTo collect inputs and outputs from production AI systems, users must install Tracely, a lightweight tool based on OpenTelemetry, which facilitates the collection process."
  },
  {
    "title": "Manage Projects",
    "description": "Set up an evaluation or monitoring Project.",
    "filename": "docs-main/docs/platform/projects_manage.mdx",
    "ai_section": "## Connecting to Evidently Cloud\nYou must first connect to [Evidently Cloud](/docs/setup/cloud) (or your [local workspace](/docs/setup/self-hosting))."
  },
  {
    "title": "Manage Projects",
    "description": "Set up an evaluation or monitoring Project.",
    "filename": "docs-main/docs/platform/projects_manage.mdx",
    "ai_section": "## Create a Project\n### Python\nTo create a Project inside a workspace `ws` and Organization with an `org_id`:\n\n```python\nproject = ws.create_project(\"My test project\", org_id=\"YOUR_ORG_ID\")\nproject.description = \"My project description\"\nproject.save()\n```\n\nIn self-hosted open-source installation, you do not need to pass the Org ID. To create a Project:\n\n```python\nproject = ws.create_project(\"My test project\")\nproject.description = \"My project description\"\nproject.save()\n```\n\n### UI\n* **Create a Project.** Click on the â€œplusâ€ sign on the home page, set a Project name and description.\n* **Edit a Project.** To change the Project name or description, hover on the existing Project, click \"edit\" and make the changes."
  },
  {
    "title": "Manage Projects",
    "description": "Set up an evaluation or monitoring Project.",
    "filename": "docs-main/docs/platform/projects_manage.mdx",
    "ai_section": "## Connect to a Project\n### Project ID\nYou can see the Project ID above the monitoring Dashboard inside your Project.\n\nTo connect to an existing Project from Python, use the `get_project` method:\n\n```python\nproject = ws.get_project(\"PROJECT_ID\")\n```"
  },
  {
    "title": "Manage Projects",
    "description": "Set up an evaluation or monitoring Project.",
    "filename": "docs-main/docs/platform/projects_manage.mdx",
    "ai_section": "## Working with a Project\n### Save Changes\nAfter making any changes to the Project (like editing description or adding monitoring Panels), always use the `save()` command:\n\n```python\nproject.save()\n```\n\n### Browse Projects\nYou can see all available Projects on the monitoring homepage, or request a list programmatically. To get a list of all Projects in a workspace `ws`, use:\n\n```python\nws.list_projects()\n```\n\nTo find a specific Project by its name, use the `search_project` method:\n\n```python\nws.search_project(\"project_name\")\n```\n\n### [DANGER] Delete Project\n<Warning>\n  Deleting a Project deletes all the data inside it.\n</Warning>\n\n#### Python\nTo delete the Project:\n\n```python\n# ws.delete_project(\"PROJECT ID\")\n```\n\n#### UI\nHover on the existing Project and click \"delete\"."
  },
  {
    "title": "Manage Projects",
    "description": "Set up an evaluation or monitoring Project.",
    "filename": "docs-main/docs/platform/projects_manage.mdx",
    "ai_section": "## Project Parameters\nEach Project has the following parameters.\n\n| Parameter                                       | Description                                                                                     | Example                                                                                                                                                     |\n|"
  },
  {
    "title": "Manage Projects",
    "description": "Set up an evaluation or monitoring Project.",
    "filename": "docs-main/docs/platform/projects_manage.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Manage Projects",
    "description": "Set up an evaluation or monitoring Project.",
    "filename": "docs-main/docs/platform/projects_manage.mdx",
    "ai_section": "-- |\n| `name: str`                                     | Project name.                                                                                   | -                                                                                                                                                           |\n| `id: UUID4 = Field(default_factory=uuid.uuid4)` | Unique identifier of the Project. Assigned automatically.                                       | -                                                                                                                                                           |\n| `description: Optional[str] = None`             | Optional description. Visible when you browse Projects.                                         | -                                                                                                                                                           |\n| `dashboard: DashboardConfig`                    | Dashboard configuration that describes the composition of the monitoring Panels.                | See [Dashboard Design](dashboard_add_panels) for details. You don't need to explicitly pass `DashboardConfig` if you use the `.dashboard.add_panel` method. |\n| `date_from: Optional[datetime.datetime] = None` | Start DateTime of the monitoring Dashboard. By default it shows data for all available periods. | `datetime.now() + timedelta(-30)`                                                                                                                           |\n| `date_to: Optional[datetime.datetime] = None`   | End DateTime of the monitoring Dashboard.                                                       | Works the same as `date_from`.                                                                                                                              |"
  },
  {
    "title": "Overview",
    "description": "Introduction to Projects.",
    "filename": "docs-main/docs/platform/projects_overview.mdx",
    "ai_section": "## What is a Project?\nA **Project** helps you organize data and evaluations for a specific use case. You can view all your Projects on the home page.\n\nEach Project:\n\n* Stores its own **datasets**, **reports**, and **traces**.\n* Has a dedicated **dashboard** and **alerting** rules.\n* Provides a **unique ID** for connecting via the **Python API** to send data, edit dashboards, and manage configurations. You can also manage everything through the UI."
  },
  {
    "title": "Overview",
    "description": "Introduction to Projects.",
    "filename": "docs-main/docs/platform/projects_overview.mdx",
    "ai_section": "## What to put in one Project?\nYou can structure projects to suit your workflow. Here are some ideas:\n\n* **By Application or Model.** Create individual Projects for each LLM app or ML model.\n* **By App Component.** For complex systems like AI agents, set up Projects for specific components, such as testing intent classification independently of other features.\n* **By Test Scenario.** Use separate Projects for distinct test scenarios, like isolating safety or adversarial datasets from other evaluations.\n* **By Phase.** Manage different development stages of the same app with separate Projects for experimentation/testing and production monitoring.\n* **By Use Case.** Group data and evaluations for multiple ML models in one Project, organizing them with tags (e.g., \"version,\" \"location\")."
  },
  {
    "title": "Overview",
    "description": "Introduction to Tracing.",
    "filename": "docs-main/docs/platform/tracing_overview.mdx",
    "ai_section": "## What are Pro Features in Evidently?\nTrace store and viewer are Pro features available in **Evidently Cloud** and **Evidently Enterprise**."
  },
  {
    "title": "Overview",
    "description": "Introduction to Tracing.",
    "filename": "docs-main/docs/platform/tracing_overview.mdx",
    "ai_section": "## What is LLM tracing?\nTracing lets you instrument your AI application to collect data for evaluation and analysis. It captures detailed records of how your LLM app operates, including inputs, outputs, and any intermediate steps and events (e.g., function calls). You define what to include. Evidently provides multiple ways to explore tracing data."
  },
  {
    "title": "Overview",
    "description": "Introduction to Tracing.",
    "filename": "docs-main/docs/platform/tracing_overview.mdx",
    "ai_section": "## What views are available for tracing data?\nEvidently offers multiple tabs for exploring tracing data:\n\n- **Trace view**: See a timeline of execution steps with input-output details and latency.\n  \n  ![](/images/tracing.png)\n\n- **Dataset view**: Automatically generate a tabular view for easier evaluation or labeling.\n  \n  ![](/images/examples/tracing_tutorial_dataset_view.png)\n\n- **Dialogue view**: For conversational applications, browse traces by user or session to focus on chat flows.\n  \n  ![](/images/examples/tracing_tutorial_session_view.png)"
  },
  {
    "title": "Overview",
    "description": "Introduction to Tracing.",
    "filename": "docs-main/docs/platform/tracing_overview.mdx",
    "ai_section": "## Do I always need tracing?\nTracing is optional on the Evidently Platform. You can also:\n\n- Upload tabular datasets using Dataset API.\n- Run evals locally and send results to the platform without tracing.\n\nHowever, tracing is especially useful for understanding complex LLM chains and execution flows, both in experiments and production monitoring."
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Quickstart\nFor a simple end-to-end example, check the [Tutorial.](../../quickstart_tracing)"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Installation\nInstall the `tracely` package from PyPi:\n\n```bash\npip install tracely\n```"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Initialize tracing\n<Tip>\n  You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).&#x20;\n</Tip>\n\nTo start sending traces, use `init_tracing`:\n\n```python\nfrom tracely import init_tracing\n\ninit_tracing(\n   address=\"https://app.evidently.cloud/\",\n   api_key=\"YOUR_EVIDENTLY_TOKEN\",\n   project_id=\"YOUR_PROJECT_ID\",\n   export_name=\"YOUR_TRACING_DATASET_NAME\",\n)\n```\n\nYou can also set parameters using environment variables with the specified names.\n\n### `init_tracing()` Function Arguments\n\n| Parameter       | Description                                                                                                                        | Environment Variable                    |\n|"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "|"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "- |"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "|\n| `address`       | Trace collector address. Defaults to `https://app.evidently.cloud/`.                                                               | `EVIDENTLY_TRACE_COLLECTOR`             |\n| `api_key`       | Evidently Cloud API key.                                                                                                           | `EVIDENTLY_TRACE_COLLECTOR_API_KEY` or `EVIDENTLY_API_KEY`    |\n| `export_name`   | Tracing dataset name. Traces with the same name are grouped into a single dataset.                                                 | `EVIDENTLY_TRACE_COLLECTOR_EXPORT_NAME` |\n| `project_id`    | Destination Project ID in Evidently Cloud.                                                                                         | `EVIDENTLY_TRACE_COLLECTOR_PROJECT_ID`  |\n| `exporter_type` | Trace export protocol: `grpc` or `http`.                                                                                           | -                                       |\n| `as_global`     | Registers the tracing provider globally (`True`) or locally (`False`). Default: `True`. Set to false if you want to initiate tracing to multiple datasets from the same environment.| -                                       |"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Tracing dataset ID\nTo get the `export_id` of the tracing dataset, run:\n\n```python\nfrom tracely import get_info\n\nget_info()\n```\n\nYou can use the `export_id` as a dataset ID for download. See [datasets API](datasets_workflow)."
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Decorator\nOnce `Tracely` is initialized, you can decorate your functions with `trace_event` to start collecting traces for a specific function:\n\n```python\nfrom tracely import init_tracing\nfrom tracely import trace_event\n\n@trace_event()\ndef process_request(question: str, session_id: str):\n    # do work\n    return \"work done\"\n```\n\nYou can also specify which function arguments should be included in the trace.\n\n**Example 1.** To log all arguments of the function:\n\n```python\n@trace_event()\n```\n\n**Example 2.** To log only input arguments of the function:\n\n```python\n@trace_event(track_args=[])\n```\n\n**Example 3.** To log only \"arg1\" and \"arg2\":\n\n```python\n@trace_event(track_args=[\"arg1\", \"arg2\"])\n```\n\n### `trace_event` Decorator Arguments\n\n| **Parameter**                      | **Description**                                                                                                            | **Default**                     |\n|"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "- |\n| `span_name: Optional[str]`         | The name of the span to send in the event.                                                                                 | Function name                   |\n| `track_args: Optional[List[str]]`  | A list of function arguments to include in the event.                                                                      | `None` (all arguments included) |\n| `ignore_args: Optional[List[str]]` | A list of function arguments to exclude, e.g., arguments that contain sensitive data.                                      | `None` (no arguments ignored)   |\n| `track_output: Optional[bool]`     | Indicates whether to track the function's return value.                                                                    | `True`                          |\n| `parse_output: Optional[bool]`     | Indicates whether the result should be parsed, e.g., `dict`, `list`, and `tuple` types will be split into separate fields. | `True`                          |"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Nested events (Spans)\nMany LLM workflows involve multiple steps â€” such as retrieval followed by generation, or extraction followed by summarization. In these cases, it's useful to trace all steps as part of a single parent trace, with each step recorded as a nested child span.\n\nYou can trace multi-step workflows using the `@trace_event` decorator and nesting the functions. If a traced function is called inside another traced function, it will automatically appear as a nested child span, as long as it's executed in the same call context (same thread).\n\nFor example:\n\n```python\n@trace_event(span_name=\"extraction\")\ndef extract_info(document):\n    â€¦\n\n@trace_event(span_name=\"summarization\")\ndef summarize_info(document):\n    â€¦\n\n@trace_event(span_name=\"document_processing\")\ndef process_document(document):\n    extract_output = extract_info(document)\n    summary_output = summarize_info(document)\n    return {\n        \"document\": document,\n        \"extraction_output\": extract_output,\n        \"summary_output\": summary_output\n    }\n```\n\nThis results in the following trace structure:\n\n```python\ndocument_processing\nâ”œâ”€â”€ extraction\nâ””â”€â”€ summarization\n```"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Context manager\nTo create a trace event without using a decorator (e.g., for a specific piece of code), you can use the context manager:\n\n```python\nimport uuid\n\nfrom tracely import init_tracing\nfrom tracely import create_trace_event\n\ninit_tracing()\n\nsession_id = str(uuid.uuid4())\n\nwith create_trace_event(\"external_span\", session_id=session_id) as event:\n    event.set_attribute(\"my-attribute\", \"value\")\n    # do work\n    event.set_result({\"data\": \"data\"})\n```\n\nYou can also trace multi-step workflows using context blocks. This gives you fine-grained control â€” useful when tracing inline code or scripts. For example, you can nest multiple `create_trace_event()` calls inline inside the same function, using `with` blocks.\n\n```python\ndef process_document(document):\n    with create_trace_event(\"document_processing\", document=document):\n        with create_trace_event(\"extraction\"):\n            ...\n        with create_trace_event(\"summarization\"):\n            ...\n```\n\n### `create_trace_event` Function Arguments\n\n| Parameter      | Description                                                                            | Default |\n|"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "- |\n| `name`         | Span name.                                                                             | -       |\n| `parse_output` | Whether to parse the result into separate fields for `dict`, `list`, or `tuple` types. | `True`  |\n| `params`       | Key-value parameters to set as attributes.                                             | -       |\n\n### `event` Object Methods\n\n| Method          | Description                                                        |\n|"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "|\n| `set_attribute` | Sets a custom attribute for the event.                             |\n| `set_result`    | Sets a result for the event. Only one result can be set per event. |"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Sessions\nIf your trace events are created in separate functions or threads you can also pass a shared `session_id`. In this case, traces will be separate but you can view the session in the UI to join them together - e.g., to read the chat conversation.\n\nSee the example above the \"Context Manager\" session."
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Add event attributes\nIf you want to add a new attribute to an active event span, you can use `get_current_span()` to get access to the current span:\n\n```python\nimport uuid\n\nfrom tracely import init_tracing\nfrom tracely import create_trace_event\nfrom tracely import get_current_span\n\ninit_tracing()\n\nsession_id = str(uuid.uuid4())\n\nwith create_trace_event(\"external_span\", session_id=session_id):\n    span = get_current_span()\n    span.set_attribute(\"my-attribute\", \"value\")\n    # do work\n    span.set_result({\"data\": \"data\"})\n```\n\n### `get_current_span()` Object  Methods\n\n| **Method**      | **Description**                                                                                              |\n|"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "|\n| `set_attribute` | Adds a new attribute to the active span.                                                                     |\n| `set_result`    | Sets a result field for the active span. (*Has no effect in decorated functions that define return values).* |"
  },
  {
    "title": "Set up tracing",
    "description": "How to collect data from a live LLM app.",
    "filename": "docs-main/docs/platform/tracing_setup.mdx",
    "ai_section": "## Connecting event into a trace\nSometimes events happen across different systems, but itâ€™s helpful to link them all into a single trace. You can do this using `tracely.bind_to_trace`:\n\n```python\n@tracely.trace_event()\ndef process_request(question: str, session_id: str):\n    # do work\n    return \"work done\"\n\n# trace id is unique 128-bit integer representing single trace\ntrace_id = 1234;\n\nwith tracely.bind_to_trace(trace_id):\n    process_request(...)\n```\n\nIn this example, instead of creating a new trace ID for each event, all events will be attached to the existing trace with the given `trace_id`.\n\n<Warning>\n  In this case, you manage the `trace_id` yourself, so you need to make sure itâ€™s truly unique. If you reuse the same `trace_id`, all events will be joined, even if they donâ€™t belong together.\n</Warning>"
  },
  {
    "title": "Evidently Cloud",
    "description": "How to set up Evidently Cloud account.",
    "icon": "cloud",
    "filename": "docs-main/docs/setup/cloud.mdx",
    "ai_section": "## Account Creation\nIf not yet, sign up for a [free Evidently Cloud account](https://app.evidently.cloud/signup). After logging in, create an **Organization** and name it."
  },
  {
    "title": "Evidently Cloud",
    "description": "How to set up Evidently Cloud account.",
    "icon": "cloud",
    "filename": "docs-main/docs/setup/cloud.mdx",
    "ai_section": "## Connecting from Python\nYou need this for programmatic tasks like tracing or logging local evaluations. Many other tasks can be done directly on the platform."
  },
  {
    "title": "Evidently Cloud",
    "description": "How to set up Evidently Cloud account.",
    "icon": "cloud",
    "filename": "docs-main/docs/setup/cloud.mdx",
    "ai_section": "## Get a Token\nClick the **Key** menu icon to open the [Token page](https://app.evidently.cloud/token). Generate and save the token securely."
  },
  {
    "title": "Evidently Cloud",
    "description": "How to set up Evidently Cloud account.",
    "icon": "cloud",
    "filename": "docs-main/docs/setup/cloud.mdx",
    "ai_section": "## Install Evidently\n[Install](/docs/setup/installation) the Evidently Python library using the following command:\n\n```python\npip install evidently ## or pip install evidently[llm]\n```"
  },
  {
    "title": "Evidently Cloud",
    "description": "How to set up Evidently Cloud account.",
    "icon": "cloud",
    "filename": "docs-main/docs/setup/cloud.mdx",
    "ai_section": "## Connect to Cloud Workspace\nImport the cloud workspace and pass your API token to connect:\n\n```python\nfrom evidently.ui.workspace import CloudWorkspace\n\nws = CloudWorkspace(\ntoken=\"API_KEY\",\nurl=\"https://app.evidently.cloud\")\n```\n\n<Note>\nFor Evidently 0.6.7 and Evidently Cloud v1, use `from evidently.ui.workspace.cloud import CloudWorkspace`. [Read more](/faq/cloud_v2).\n</Note>"
  },
  {
    "title": "Evidently Cloud",
    "description": "How to set up Evidently Cloud account.",
    "icon": "cloud",
    "filename": "docs-main/docs/setup/cloud.mdx",
    "ai_section": "## API Key Management\nYou can also provide the API key by setting the environment variable `EVIDENTLY_API_KEY`."
  },
  {
    "title": "Evidently Cloud",
    "description": "How to set up Evidently Cloud account.",
    "icon": "cloud",
    "filename": "docs-main/docs/setup/cloud.mdx",
    "ai_section": "## Next Steps\nYou are all set! Create a Project and run your first [evaluation](/quickstart_llm)."
  },
  {
    "title": "Installation",
    "description": "How to install the open-source Python library.",
    "icon": "play",
    "filename": "docs-main/docs/setup/installation.mdx",
    "ai_section": "## Installing Evidently\n`Evidently` is available as a Python package. Install it using the **pip package manager**:\n\n```python\npip install evidently\n```\n\nTo install `evidently` using **conda installer**, run:\n\n```sh\nconda install -c conda-forge evidently\n```"
  },
  {
    "title": "Installation",
    "description": "How to install the open-source Python library.",
    "icon": "play",
    "filename": "docs-main/docs/setup/installation.mdx",
    "ai_section": "## Installing Evidently LLM\nTo run evaluations specific to LLMs that include additional dependencies, run:\n\n```python\npip install evidently[llm]\n```"
  },
  {
    "title": "Installation",
    "description": "How to install the open-source Python library.",
    "icon": "play",
    "filename": "docs-main/docs/setup/installation.mdx",
    "ai_section": "## Installing Tracely\nTo use tracing based on OpenTelemetry, install the sister package **tracely**:\n\n```sh\npip install tracely\n```"
  },
  {
    "title": "Self-hosting",
    "description": "How to self-host the open-source Evidently UI service.",
    "icon": "window-restore",
    "filename": "docs-main/docs/setup/self-hosting.mdx",
    "ai_section": "## Overview of Self-Hosting Evidently\nThis page explains how to self-host the lightweight open-source platform. [Contact us](https://www.evidentlyai.com/get-demo) for the Enterprise version with extra features and support. Compare [OSS vs. Enterprise/Cloud](/faq/oss_vs_cloud)."
  },
  {
    "title": "Self-hosting",
    "description": "How to self-host the open-source Evidently UI service.",
    "icon": "window-restore",
    "filename": "docs-main/docs/setup/self-hosting.mdx",
    "ai_section": "## Creating a Workspace\nOnce you [install Evidently](/docs/setup/installation), you will need to create a `workspace` to store the evaluation results (JSON Reports called `snapshots`). \n\n### Local Workspace\nBoth the UI Service and data storage are local. You can create a local Workspace like this:\n\n```python\nws = Workspace.create(\"evidently_ui_workspace\")\n```\n\n### Remote Workspace\nHere, you send the snapshots to a remote server, running the Monitoring UI on the same server. Create a remote Workspace with:\n\n```python\nworkspace = RemoteWorkspace(\"http://localhost:8000\")\n```\n\n### Remote Snapshot Storage\nYou can store snapshots in a remote data store (e.g., an S3 bucket). To read snapshots from an S3 bucket, set the following environment variables:\n\n```\nFSSPEC_S3_ENDPOINT_URL=http://localhost:9000/\nFSSPEC_S3_KEY=my_key FSSPEC_S3_SECRET=my_secret\nevidently ui --workspace s3://my_bucket/workspace\n```\n\n### [DANGER] Delete Workspace\nTo delete a Workspace, run the command:\n```bash\ncd src/evidently/ui/\nrm -r workspace\n```\n**Warning:** This will delete all data stored in the workspace folder."
  },
  {
    "title": "Self-hosting",
    "description": "How to self-host the open-source Evidently UI service.",
    "icon": "window-restore",
    "filename": "docs-main/docs/setup/self-hosting.mdx",
    "ai_section": "## Launching the UI Service\nTo launch the Evidently UI service, run the command in the Terminal.\n\n**Option 1:** Run the following from the Workspace directory:\n```bash\nevidently ui\n```\n\n**Option 2:** Specify the project in a different Workspace:\n```bash\nevidently ui --workspace . /workspace\n```\n\n**Option 3:** Run the UI service on a specific port:\n```bash\nevidently ui --workspace ./workspace --port 8080\n```\n\nTo view the interface, go to http://localhost:8000 or the specified port."
  },
  {
    "title": "Self-hosting",
    "description": "How to self-host the open-source Evidently UI service.",
    "icon": "window-restore",
    "filename": "docs-main/docs/setup/self-hosting.mdx",
    "ai_section": "## Demo Projects\nTo launch the service with demo projects, run:\n```\nevidently ui --demo-projects all\n```"
  },
  {
    "title": "Self-hosting",
    "description": "How to self-host the open-source Evidently UI service.",
    "icon": "window-restore",
    "filename": "docs-main/docs/setup/self-hosting.mdx",
    "ai_section": "## Additional Resources\nCheck this tutorial for an end-to-end example:\n[Evidently UI tutorial](https://github.com/evidentlyai/evidently/blob/main/examples/service/workspace_tutorial.ipynb) - How to create a workspace, project, and run Reports."
  },
  {
    "title": "Evidently and GitHub actions",
    "description": "Testing LLM outputs as part of the CI/CD flow.",
    "filename": "docs-main/examples/GitHub_actions.mdx",
    "ai_section": "## Integration with GitHub Actions\nYou can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application as part of every code push or pull request."
  },
  {
    "title": "Evidently and GitHub actions",
    "description": "Testing LLM outputs as part of the CI/CD flow.",
    "filename": "docs-main/examples/GitHub_actions.mdx",
    "ai_section": "## How the Integration Works\n- You define a test dataset of inputs (e.g., test prompts with or without reference answers). You can store it as a file or save the dataset at Evidently Cloud callable by Dataset ID.\n- Run your LLM system or agent against those inputs inside CI.\n- Evidently automatically evaluates the outputs using the user-specified config, which defines the Evidently descriptors, tests, and Report composition. This includes methods like:\n  - LLM judges (e.g., tone, helpfulness, correctness)\n  - Custom Python functions\n  - Dataset-level metrics like classification quality\n- If any test fails, the CI job fails.\n- You receive a detailed test report with pass/fail status and metrics.\nResults are stored locally or pushed to Evidently Cloud for deeper review and tracking."
  },
  {
    "title": "Evidently and GitHub actions",
    "description": "Testing LLM outputs as part of the CI/CD flow.",
    "filename": "docs-main/examples/GitHub_actions.mdx",
    "ai_section": "## Benefits of Using CI-Native Testing\nThe final result is CI-native testing for your LLM behavior, allowing you to safely tweak prompts, models, or logic without breaking things silently."
  },
  {
    "title": "Evidently and GitHub actions",
    "description": "Testing LLM outputs as part of the CI/CD flow.",
    "filename": "docs-main/examples/GitHub_actions.mdx",
    "ai_section": "## Code Example and Tutorial\nðŸ‘‰ Check the full tutorial and example repo: [Evidently CI Example](https://github.com/evidentlyai/evidently-ci-example)"
  },
  {
    "title": "Evidently and GitHub actions",
    "description": "Testing LLM outputs as part of the CI/CD flow.",
    "filename": "docs-main/examples/GitHub_actions.mdx",
    "ai_section": "## GitHub Marketplace Action\nAction is also available on GitHub Marketplace: [Run Evidently Report](https://github.com/marketplace/actions/run-evidently-report)"
  },
  {
    "title": "LLM evaluations",
    "description": "Run different LLM evaluation methods.",
    "noindex": "true",
    "filename": "docs-main/examples/LLM_evals.mdx",
    "ai_section": "## LLM Evaluation Methods Overview\nThis tutorial provides an overview of evaluation methods for LLMs (Language Learning Models). It is divided into three parts: \n- **Part 1**: Anatomy of a single evaluation, covering the basic LLM evaluation API and setup.\n- **Part 2**: Reference-based evaluation, which includes exact match, semantic similarity, BERTScore, and LLM judge.\n- **Part 3**: Reference-free evaluation, discussing text statistics, regex, ML models, LLM judges, and session-level evaluators."
  },
  {
    "title": "LLM evaluations",
    "description": "Run different LLM evaluation methods.",
    "noindex": "true",
    "filename": "docs-main/examples/LLM_evals.mdx",
    "ai_section": "## Code Example\nYou can explore the code related to LLM evaluation methods through the following notebook: [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb)."
  },
  {
    "title": "LLM evaluations",
    "description": "Run different LLM evaluation methods.",
    "noindex": "true",
    "filename": "docs-main/examples/LLM_evals.mdx",
    "ai_section": "## Video Tutorials\nTo aid your understanding of LLM evaluation methods, there are several video tutorials available:\n- [Video 1](https://www.youtube.com/watch?v=6JGRdMGbNCI&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=3)\n- [Video 2](https://www.youtube.com/watch?v=yD20c-KAImE&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=4)\n- [Video 3](https://www.youtube.com/watch?v=-zoIqOpt2DA&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=5)"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Tutorial Overview\nIn this tutorial, we'll demonstrate how to evaluate text for custom criteria using an LLM (Large Language Model) as the judge. The tutorial will cover two methods: reference-based evaluation and open-ended evaluation, allowing you to use the LLM evaluator in various contexts like regression testing or prompt comparisons."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Prerequisites\nTo follow this tutorial, you will need:\n- Basic Python knowledge.\n- An OpenAI API key to use the LLM evaluator.\nWe recommend using Jupyter Notebook or Google Colab for a rich interactive experience."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Installation and Imports\nTo set up, ensure you have the Evidently library installed:\n\n```python\npip install evidently\n```\n\nThen, import the required modules and pass your OpenAI API key as an environment variable:\n\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n```\n\nFor additional evaluator LLMs, refer to the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm)."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Creating the Dataset\nWe'll create a toy Q&A dataset simulating customer support questions. It includes:\n- Questions (inputs to the LLM)\n- Target responses (approved answers)\n- New responses (imitated outputs)\n- Manual labels (judging the accuracy)\n\nThis process will help create better criteria for the LLM judge and formulate a \"ground truth\" for evaluation.\n\nHereâ€™s a sample structure for generating the dataset:\n\n```python\ndata = [\n    [\"Hi there, how do I reset my password?\", ...],\n    ...\n]\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Evaluation Criteria\nIn this section, we will focus on why it's beneficial to add manual labels to your dataset. The labels help:\n- Improve the criteria, leading to a more effective prompt.\n- Establish a baseline of \"ground truth,\" making it easier to evaluate the LLM judge.\n\nUltimately, treating the LLM judge as a small machine learning system requires its own evaluations to ensure accuracy."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Tutorial Steps\nThe tutorial will involve:\n1. Creating an evaluation dataset.\n2. Designing and running an LLM as a judge.\n3. Comparing the LLM judge's evaluations against manual labels.\n\nWe will first focus on a reference-based evaluator followed by a simpler judge that measures verbosity."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Additional Resources\nIf you prefer video tutorials, there is an extended code tutorial available that walks through the iterative improvement of the LLM judge prompt. You can watch it [here](https://www.youtube.com/watch?v=kP_aaFnXLmY).\n\nAdditionally, sample notebooks for this tutorial can be accessed through:\n- [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb)\n- [Open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb)."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Contact Methods\nTo contact us, please select 'Contact Us' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Reporting a Lost or Stolen Card\nIf your card is lost or stolen, immediately log into your account, go to 'Card Management', and select 'Report Lost/Stolen'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Enabling Two-Factor Authentication (2FA)\nTo enable two-factor authentication, log into your account, go to 'Security Settings', and select 'Enable 2FA'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in. You can also set up backup codes in case you lose access to the 2FA app."
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Creating an Evidently Dataset Object\nTo create an Evidently dataset object, pass the dataframe and map the column types. Use the following code snippet:\n\n```python\ndefinition = DataDefinition(\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\n    categorical_columns=[\"label\"]\n)\neval_dataset = Dataset.from_pandas(\n    golden_dataset,\n    data_definition=definition)\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Previewing the Dataset\nTo preview the dataset, set the display option for pandas and call the head method:\n\n```python\npd.set_option('display.max_colwidth', None)\ngolden_dataset.head(5)\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Setting Up a Correctness Evaluator\nTo set up a correctness evaluator, configure the evaluator prompt using the LLMEval Descriptor. Here's how to define the prompt template for correctness:\n\n```python\ncorrectness = BinaryClassificationPromptTemplate(\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\n        REFERENCE:\n        =====\n        {target_response}\n        =====\"\"\",\n        target_category=\"incorrect\",\n        non_target_category=\"correct\",\n        uncertainty=\"unknown\",\n        include_reasoning=True,\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\n)\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Scoring Your Data\nTo add a new descriptor to your dataset, run the following code:\n\n```python\neval_dataset.add_descriptors(descriptors=[\n    LLMEval(\"new_response\",\n            template=correctness,\n            provider = \"openai\",\n            model = \"gpt-4o-mini\",\n            alias=\"Correctness\",\n            additional_columns={\"target_response\": \"target_response\"}),\n])\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Generating a Report\nTo generate a report summarizing the results, use the following code:\n\n```python\nreport = Report([\n    TextEvals()\n])\nmy_eval = report.run(eval_dataset, None)\nmy_eval\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Evaluating the LLM Eval Quality\nTo evaluate the quality of the LLM evaluator itself, map the structure of the dataset accordingly. Use the following data definition:\n\n```python\ndefinition_2 = DataDefinition(\n    classification=[BinaryClassification(\n        target=\"label\",\n        prediction_labels=\"Correctness\",\n        pos_label = \"incorrect\")],\n    categorical_columns=[\"label\", \"Correctness\"])\nclass_dataset = Dataset.from_pandas(\n    pd.DataFrame(df),\n    data_definition=definition_2)\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Creating a Verbosity Evaluator\nTo create a verbosity evaluator that checks for conciseness, set up the prompt template as follows:\n\n```python\nverbosity = BinaryClassificationPromptTemplate(\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\n            A concise response should:\n            - Provide the necessary information without unnecessary details or repetition.\n            - Be brief yet comprehensive enough to address the query.\n            - Use simple and direct language to convey the message effectively.\"\"\",\n        target_category=\"concise\",\n        non_target_category=\"verbose\",\n        uncertainty=\"unknown\",\n        include_reasoning=True,\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\n)\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Integration with Evidently Cloud\nTo integrate your evaluator into Evidently Cloud, import the necessary components and create a project. Use the following code:\n\n```python\nfrom evidently.ui.workspace import CloudWorkspace\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Sending Your Eval to Cloud\nOnce your eval is created, you can upload it to the Evidently Cloud using:\n\n```python\nws.add_run(project.id, my_eval, include_data=True)\n```"
  },
  {
    "title": "LLM as a judge",
    "description": "How to create and evaluate an LLM judge.",
    "filename": "docs-main/examples/LLM_judge.mdx",
    "ai_section": "## Reference Documentation\nFor complete documentation on LLM judges, please refer to the [documentation on LLM judges](https://docs.yourwebsite.com/metrics/customize_llm_judge)."
  },
  {
    "title": "LLM-as-a-jury",
    "description": "Evaluate the LLM outputs with multiple LLMs.",
    "filename": "docs-main/examples/LLM_jury.mdx",
    "ai_section": "## Evaluation Approach Overview\nThis evaluation approach uses multiple LLMs to evaluate the same output. You can obtain an aggregate evaluation result and explicitly surface disagreements among LLMs. For more details, refer to the blog on the concept of LLM jury: [Blog on LLM judges and jury](https://www.evidentlyai.com/blog/llm-judges-jury)."
  },
  {
    "title": "LLM-as-a-jury",
    "description": "Evaluate the LLM outputs with multiple LLMs.",
    "filename": "docs-main/examples/LLM_jury.mdx",
    "ai_section": "## Preparation\nTo begin, you'll need to install Evidently with the following command:\n\n```python\npip install evidently litellm \n```\n\nAlternatively, you can install using the `evidently[llm]` option.\n\nNext, import the necessary components you'll use in your analysis:\n\n```python\nimport pandas as pd\nfrom evidently import Dataset\nfrom evidently import DataDefinition\nfrom evidently import Report\nfrom evidently.presets import TextEvals\nfrom evidently.tests import eq, is_in, not_in\nfrom evidently.descriptors import LLMEval, TestSummary, ColumnTest\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\nfrom evidently.core.datasets import DatasetColumn\nfrom evidently.descriptors import CustomColumnDescriptor\n\nfrom evidently.ui.workspace import CloudWorkspace\n```"
  },
  {
    "title": "LLM-as-a-jury",
    "description": "Evaluate the LLM outputs with multiple LLMs.",
    "filename": "docs-main/examples/LLM_jury.mdx",
    "ai_section": "## Step 1: Set Up Evaluator LLMs\nPass the API keys for the LLMs you'll use as judges:\n\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\nos.environ[\"GEMINI_API_KEY\"] = \"YOUR KEY\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\n```\n\nYou can also use any other LLMs, including self-hosted ones. Optional: Set up an Evidently Cloud workspace:\n\n```python\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\n```\n\nCreate a project to store your evaluation results:\n\n```python\nproject = ws.create_project(\"LLM jury\", org_id=\"YOUR ORG ID\")\nproject.description = \"Evals with different LLMs\"\n```"
  },
  {
    "title": "LLM-as-a-jury",
    "description": "Evaluate the LLM outputs with multiple LLMs.",
    "filename": "docs-main/examples/LLM_jury.mdx",
    "ai_section": "## Step 1: Toy Data\nDefine a small dataset of user intents and generated emails. This dataset simulates user instructions for an email generation tool, aimed at evaluating the appropriateness of generated emails.\n\n```python\ndata = [\n    [\"donâ€™t want to attend, say no\", \"Hey,\\nGonna skip the meeting tomorrow â€” nothing new for me there. Loop me in if anything big happens.\\nâ€” A\"],\n    ...\n]\n\ncolumns = [\"user input\", \"generated email\"]\neval_df = pd.DataFrame(data, columns=columns)\n```"
  },
  {
    "title": "LLM-as-a-jury",
    "description": "Evaluate the LLM outputs with multiple LLMs.",
    "filename": "docs-main/examples/LLM_jury.mdx",
    "ai_section": "## Step 2: Define the Evaluation Prompt\nUse `BinaryClassificationPromptTemplate` to set up the evaluation criteria for the LLMs judging the email appropriateness:\n\n```python\nus_corp_email_appropriateness = BinaryClassificationPromptTemplate(\n    pre_messages=[\n        (\"system\", \"\"\"You are an expert in U.S. corporate and workplace communication in tech companies.\n        You will be shown a snippet of an email generated by the assistant.\n        Your task is to judge whether the text would be considered *appropriate* for email communication.\n        \"\"\")\n    ],\n    criteria=\"\"\"An APPROPRIATE email text is one that would be acceptable in real-world professional email communication.\n    An INAPPROPRIATE email text includes tone, language, or content that would be questionable or unacceptable.\n    \"\"\",\n    target_category=\"APPROPRIATE\",\n    non_target_category=\"INAPPROPRIATE\",\n    include_reasoning=True,\n)\n```"
  },
  {
    "title": "LLM-as-a-jury",
    "description": "Evaluate the LLM outputs with multiple LLMs.",
    "filename": "docs-main/examples/LLM_jury.mdx",
    "ai_section": "## Step 3: Create a Panel of LLM Judges\nCreate evaluators from multiple LLM providers using the same evaluation prompt. This section includes the code to score the \"generated email\" column with three different judges:\n\n```python\nllm_evals = Dataset.from_pandas(\n    eval_df,\n    data_definition=DataDefinition(),\n    descriptors=[\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\n                provider=\"openai\", model=\"gpt-4o-mini\",\n                alias=\"OpenAI_judge_US\",\n                tests=[eq(\"APPROPRIATE\", column=\"OpenAI_judge_US\", alias=\"GPT approves\")]),\n        ...\n        TestSummary(success_all=True, success_count=True, success_rate=True, alias=\"Approve\"),\n])\n```\n\nTo explicitly flag disagreements among LLMs, add a custom descriptor:\n\n```python\ndef judges_disagree(data: DatasetColumn) -> DatasetColumn:\n    return DatasetColumn(\n        type=\"cat\",\n        data=pd.Series([\n            \"DISAGREE\" if val not in [0.0, 1.0] else \"AGREE\"\n            for val in data.data]))\n\nllm_evals.add_descriptors(descriptors=[\n    CustomColumnDescriptor(\"Approve_success_rate\", judges_disagree, alias=\"Do LLMs disagree?\"),\n])            \n```"
  },
  {
    "title": "LLM-as-a-jury",
    "description": "Evaluate the LLM outputs with multiple LLMs.",
    "filename": "docs-main/examples/LLM_jury.mdx",
    "ai_section": "## Step 4: Run and View the Report\nTo explore results locally, export them to a DataFrame:\n\n```python\nllm_evals.as_dataframe()\n```\n\nTo generate a summary report with overall metrics, run:\n\n```python\nreport = Report([\n    TextEvals()\n])\nmy_eval = report.run(llm_evals, None)\n```\n\nTo upload results to Evidently Cloud or view locally, use:\n\n```python\nws.add_run(project.id, my_eval, include_data=True)\n```\n\nOr for local viewing:\n\n```python\nmy_eval\n# my_eval.json()\n# my_eval.dict()\n# my_eval.save_html(\"report.html\")\n```\n\nThis will provide insights such as how many emails received mixed judgments from the LLMs. You can filter and inspect individual examples directly."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Introduction to Retrieval-Augmented Generation (RAG)\nIn this tutorial, we'll demonstrate how to evaluate different aspects of Retrieval-Augmented Generation (RAG) using Evidently. Weâ€™ll be working with a **local open-source workflow**, viewing results as a pandas dataframe and a visual report â€” ideal for Jupyter or Colab. By the end of this tutorial, you'll learn to generate structured reports to track RAG performance."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Installation and Imports\nTo get started, you will need to install Evidently and import the necessary modules.\n\n1. **Install Evidently:**\n   ```python\n   !pip install evidently[llm]\n   ```\n\n2. **Import Required Modules:**\n   ```python\n   import pandas as pd\n   from evidently import Dataset\n   from evidently import DataDefinition\n   from evidently.descriptors import *\n   from evidently import Report\n   from evidently.presets import TextEvals\n   from evidently.metrics import *\n   from evidently.tests import *\n   from evidently.ui.workspace import CloudWorkspace\n   ```\n\n3. **Set OpenAI Key:**\n   ```python\n   import os\n   os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n   ```"
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Evaluating Retrieval\nHere we focus on evaluating the quality of context retrieval in two scenarios: single context and multiple contexts.\n\n### Single Context\nEvaluate retrieval quality when a single context is retrieved for each query using a synthetic dataset.\n\n1. **Generate a Synthetic Dataset:**\n   ```python\n   synthetic_data = [\n       [\"Why do flowers bloom in spring?\", \"Context 1\", \"Response 1\"],\n       [\"Why do we yawn when we see someone else yawn?\", \"Context 2\", \"Response 2\"],\n       ...\n   ]\n   ```\n\n2. **Assess Overall Context Quality:**\n   Evaluate if the retrieved context provides sufficient information.\n   ```python\n   context_based_evals = Dataset.from_pandas(synthetic_df, ...)\n   ```"
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "### Multiple Contexts\nWhen multiple chunks are retrieved, assess the relevance of each individual chunk.\n\n1. **Generate a Toy Dataset:**\n   ```python\n   synthetic_data = [\n       [\"Why are bananas healthy?\", [\"Context 1\", \"Context 2\"], \"Response 1\"],\n       ...\n   ]\n   ```\n\n2. **Evaluate Hit Rate:**\n   Aggregating results per query to check if at least one chunk is relevant.\n   ```python\n   context_based_evals = Dataset.from_pandas(synthetic_df_2, ...)\n   ```"
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Evaluating Generation\nThis section focuses on evaluating the quality of generated responses with and without ground truth data.\n\n### With Ground Truth\nIf a ground truth dataset is available, compare the generated responses to known correct answers.\n\n1. **Generate a New Toy Example:**\n   ```python\n   synthetic_data = [\n       [\"Why do we yawn?\", \"Generated Response\", \"Ground Truth\"],\n       ...\n   ]\n   ```\n\n2. **Run Comparisons:**\n   Utilize methods like `CorrectnessLLMEval`, `BERTScore`, and `SemanticSimilarity`.\n   ```python\n   context_based_evals = Dataset.from_pandas(synthetic_df_3, ...)\n   ```"
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "### Without Ground Truth\nIn cases where reference answers are not available, use LLM judges to evaluate response quality.\n\n1. **Evaluate Faithfulness:**\n   Check if the response is faithful to the context.\n   ```python\n   context_based_evals = Dataset.from_pandas(synthetic_df, ...)\n   ```\n\nYou can add other checks for response length, refusal rates, and tone."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Get Reports\nTo summarize the results across evaluated inputs, group your evaluations into a **Report**.\n\n1. **Score Data:**\n   Create an Evidently dataset object.\n   ```python\n   context_based_evals = Dataset.from_pandas(synthetic_df, ...)\n   ```\n\n2. **Generate a Report:**\n   Create a report to visualize the evaluation results.\n   ```python\n   report = Report([TextEvals()])\n   my_eval = report.run(context_based_evals, None)\n   ```\n\n3. **Add Test Conditions:**\n   Set up pass/fail tests based on expected score distributions.\n   ```python\n   report = Report([...])\n   ```\n\nThis methodology ensures a comprehensive evaluation process for RAG systems."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## HTML Report Generation\nThis section explains how to render an HTML report in the notebook cell, allowing users to utilize various export options, like `as_dict()` for Python dictionary output. A visual example of a RAG report is provided to illustrate the evaluation of data retrieval and generation."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Adding Test Conditions\nHere, you'll learn about setting up explicit pass/fail tests based on expected score distributions using the Tests feature. It includes a code snippet to create a report with specific conditions for evaluating faithfulness and context quality."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Upload to Evidently Cloud\nThis section outlines the process for systematically tracking results and interacting with your evaluation dataset using the Evidently Cloud platform. It includes steps for setting up the cloud and the relevant code snippets for integration."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Creating and Retrieving Projects\nLearn how to create a new project or retrieve an existing one in Evidently Cloud. This section provides the code necessary for project management within the cloud environment."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## Sending Evaluations to Cloud\nThis section describes how to upload your evaluation to Evidently Cloud after creating it. The code snippet demonstrates adding a run to a project."
  },
  {
    "title": "RAG evals",
    "description": "Metrics to evaluate a RAG system.",
    "filename": "docs-main/examples/LLM_rag_evals.mdx",
    "ai_section": "## What's Next?\nIn this section, you'll find suggestions for implementing regression testing regularly to monitor changes in retrieval and response quality in your RAG system."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Tutorial Overview\nThis tutorial demonstrates how to perform regression testing for LLM outputs by comparing new and old responses after adjustments to prompts, models, or parameters. The aim is to identify significant changes and ensure confidence in updates."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Prerequisites\nTo complete this tutorial, you will need:\n- Basic Python knowledge.\n- An OpenAI API key for using the LLM evaluator.\n- An Evidently Cloud account for tracking test results. Sign up [here](https://www.evidentlyai.com/register) if you do not have an account."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Installation and Imports\nTo begin, install Evidently and import the necessary modules:\n\n```python\npip install evidently[llm]\n```\n\nImport the required modules:\n\n```python\nimport pandas as pd\nfrom evidently.future.datasets import Dataset\n# additional imports...\n```\n\nSet your OpenAI API key:\n\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\n```"
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Create a Project\nEstablish a connection to Evidently Cloud using your API token:\n\n```python\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\n```\n\nThen, create a project:\n\n```python\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\nproject.description = \"My project description\"\nproject.save()\n```"
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Prepare the Dataset\nCreate a toy dataset that consists of questions and reference answers:\n\n```python\ndata = [\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter...\"],\n    # additional rows...\n]\n\ncolumns = [\"question\", \"target_response\"]\nref_data = pd.DataFrame(data, columns=columns)\n```\n\nYou can preview the data:\n\n```python\npd.set_option('display.max_colwidth', None)\nref_data.head()\n```"
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Generate New Answers\nImitate generating new responses from an LLM by adding a new column to your dataset:\n\n```python\ndata = [\n    [\"Why is the sky blue?\", \"reference answer\", \"new response\"],\n    # additional rows...\n]\n\ncolumns = [\"question\", \"target_response\", \"response\"]\neval_data = pd.DataFrame(data, columns=columns)\n```"
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Design the Test Suite\nTo evaluate new answers, use deterministic or embedding-based metrics, along with custom criteria defined using LLM as a judge. The evaluation metrics include:\n\n1. **Length Check**: New responses must not exceed 200 symbols.\n2. **Correctness**: New answers should align with reference answers.\n3. **Style**: New responses must match the style of the reference.\n\n### Correctness Judge\nDefine an LLM judge for correctness assessments:\n\n```python\ncorrectness = BinaryClassificationPromptTemplate(\n    criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE...\"\"\",\n    # additional parameters...\n)\n```\n\n### Style Judge\nCreate a custom judge for style matching:\n\n```python\nstyle_match = BinaryClassificationPromptTemplate(\n    criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE...\"\"\",\n    # additional parameters...\n)\n```"
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Summary\nIn this tutorial, you explored how to prepare for regression testing of LLM outputs by implementing various components, including dataset preparation, new response generation, and designing test suites. \n\nFeel free to refer back to sections for specifics on installation, project creation, dataset preparation, and evaluations."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Overview of Parameters\nAn explanation of each parameter can be found in the [LLM judge docs](/metrics/customize_llm_judge)."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Style Judge\nUsing a custom judge for style match evaluates whether the style (not the contents) of both responses remains similar. The style judge relies on specific criteria to determine if the responses match."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Code for Style Match\nThe style match uses a BinaryClassificationPromptTemplate to assess whether an answer matches the reference in style. It considers attributes like tone, sentence structure, and verbosity level."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Evaluation Steps\nWe can evaluate responses for correctness, style, and text length in two steps: first, by scoring the data with defined row-level descriptors, and second, by creating a report to summarize the results."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Understanding Descriptors\nDescriptors are built-in metrics that process individual responses and add scores or labels. Check the list of other built-in [descriptors](/metrics/all_descriptors)."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Adding Descriptors to Dataset\nTo enhance the dataset with descriptors, the following code can be executed. It defines descriptors and attaches them to the evaluation dataset."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Creating a Report\nA report is formulated to summarize metrics and validate specific conditions regarding the evaluated responses. This includes tests for length and correctness."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Running the Report\nOnce the report is ready, it can be run on the evaluation dataset and sent to the Evidently Cloud for results analysis."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Viewing Report Results\nThe results of the report can be viewed in the Evidently Platform, where you'll find sections for Metrics and Tests outlining aggregate results and any failures."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Testing with New Data\nAfter making changes to the prompt, new responses can be compared against the reference dataset using a similar evaluation process."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Creating a New Dataset\nA new dataset can be created using updated responses. This is essential for comparing and running tests on the freshly generated data."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Repeating Evaluation\nThe previous evaluation process can be repeated on the new dataset with the defined descriptors and report configurations, allowing for continuous improvement tracking."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Creating a Dashboard\nDashboards can be configured to track test results over time, allowing for visual monitoring of performance and improvements in response evaluations."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Designing the Dashboard\nBy coding the dashboard panels, you can create summary displays that show the success rate of the latest test runs and detailed test results over time."
  },
  {
    "title": "LLM regression testing",
    "description": "How to run regression testing for LLM outputs.",
    "filename": "docs-main/examples/LLM_regression_testing.mdx",
    "ai_section": "## Next Steps\nFuture plans could include integrating the testing suite with CI/CD workflows and setting up alerts for test failures, ensuring ongoing improvements in response quality."
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## Quickstarts\nIf you are new, start here. Explore the quickstart guides for different functionalities:\n\n- **LLM quickstart**: Evaluate the quality of text outputs. [Read more here](https://www.evidentlyai.com/quickstart_llm)\n- **ML quickstart**: Test tabular data quality and data drift. [Read more here](https://www.evidentlyai.com/quickstart_ml)\n- **Tracing quickstart**: Collect inputs and outputs from AI in your app. [Read more here](https://www.evidentlyai.com/quickstart_tracing)"
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## LLM Tutorials\nEnd-to-end examples of specific workflows and use cases related to LLMs:\n\n- **LLM as a judge**: How to create and evaluate an LLM judge against human labels. [Read more here](https://www.evidentlyai.com/examples/LLM_judge)\n- **RAG evaluation**: A walkthrough of different RAG evaluation metrics. [Read more here](https://www.evidentlyai.com/examples/LLM_rag_evals)\n- **LLM as a jury**: Using multiple LLMs to evaluate the same output. \n- **LLM evaluation methods**: A walkthrough of different LLM evaluation methods. [CODE + VIDEO](https://www.evidentlyai.com/LLM_evals)\n- **Descriptor cookbook**: A walkthrough of different descriptors in a single notebook. [Read more here](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/descriptors.ipynb)\n- **LLM judge prompt optimization (1)**: Optimize a multi-class classifier using target labels. [Read more here](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\n- **LLM judge prompt optimization (2)**: Optimize a binary classifier using target labels and free-form feedback. [Read more here](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)"
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## ML Tutorials\nEnd-to-end examples of specific workflows and use cases related to machine learning:\n\n- **Metric cookbook**: Various data/ML metrics including Regression, Classification, Data Quality, and Data Drift. [Read more here](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/metrics.ipynb)"
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## Integrations\nEnd-to-end examples of integrating Evidently with other tools and platforms:\n\n- **GitHub actions**: Running Evidently evals as part of CI/CD workflow with native GitHub action integration for regression testing. [Read more here](https://www.evidentlyai.com/examples/GitHub_actions)\n- **Different LLM providers as judges**: Examples of using different external evaluator LLMs as judges, including OpenAI, Gemini, Google Vertex, Mistral, and Ollama. [Read more here](https://github.com/evidentlyai/evidently/blob/main/examples/future_examples/llm_providers.ipynb)\n- **Evidently + Grafana: LLM evals**: Visualize Evidently LLM evaluation metrics with Grafana using Postgres as a database. [Read more here](https://github.com/evidentlyai/evidently/tree/main/examples/llm_eval_grafana_dashboard)\n- **Evidently + Grafana: Data drift**: Visualize Evidently data drift evaluations on a Grafana dashboard using Postgres as a database. [Read more here](https://github.com/evidentlyai/evidently/tree/main/examples/data_drift_grafana_dashboard)"
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## Deployment\nLearn how to deploy evidently with an open-source UI tutorial:\n\n- **Evidently Open-source UI tutorial**: How to create a workspace, project, and run reports. [Read more here](https://github.com/evidentlyai/evidently/blob/main/examples/service/workspace_tutorial.ipynb)"
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## LLM Evaluation Course - Video Tutorials\nWe have an applied LLM evaluation course that walks through core evaluation workflows:\n\nðŸ“¥ [Sign up for the course](https://www.evidentlyai.com/llm-evaluation-course-practice)\n\nðŸ“¹ [View complete YouTube playlist](https://www.youtube.com/watch?v=K8LLVi5Xrh8&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=2)\n\nTable of tutorials include:\n- **Intro to LLM Evals**: Introduction to concepts and goals of LLM evaluation.\n- **LLM Evaluation Methods**: Overview of various evaluation methods.\n- **LLM as a Judge**: Creating and tuning LLM judges aligned with human preferences.\n- **Classification Evaluation**: Evaluate LLMs and simple predictive ML baselines.\n- **Content Generation with LLMs**: Use LLMs for engaging content.\n- **RAG evaluations**: Evaluate RAG systems theoretically and practically.\n- **AI agent evaluations**: Build and evaluate a Q&A agent.\n- **Adversarial testing**: Scenario-based risk testing on sensitive topics."
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## More Examples\nYou can also find more examples in the [Example Repository](https://github.com/evidentlyai/evidently/tree/main/examples)."
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## Adversarial Testing\nAdversarial testing involves running scenario-based risk testing on forbidden topics and brand risks. This tutorial provides guidance on how to effectively conduct such tests to identify potential vulnerabilities.\n\n[Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Adversarial_Testing.ipynb)"
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## Video Resources\nThis section includes video resources that complement the tutorials and provide visual guidance on the topics discussed. These resources can enhance understanding and help with practical application."
  },
  {
    "title": "Tutorials and guides",
    "description": "End-to-end code examples.",
    "filename": "docs-main/examples/introduction.mdx",
    "ai_section": "## More Examples\nFor additional examples and case studies, you can visit the [Example Repository](https://github.com/evidentlyai/community-examples). This repository includes various practical applications and methodologies relevant to the outlined topics."
  },
  {
    "title": "Evidently Cloud v2",
    "description": "A new version of Evidently Cloud available starting April 10, 2025.",
    "filename": "docs-main/faq/cloud_v2.mdx",
    "ai_section": "## âš ï¸ Breaking Change Notice\nWeâ€™ve launched **Evidently Cloud v2** â€“ a major update that brings significant improvements and **breaking changes** to our cloud platform. Please read this carefully to ensure compatibility."
  },
  {
    "title": "Evidently Cloud v2",
    "description": "A new version of Evidently Cloud available starting April 10, 2025.",
    "filename": "docs-main/faq/cloud_v2.mdx",
    "ai_section": "## ðŸš€ Whatâ€™s New\n- **Redesigned dashboard** â€“ faster, cleaner, and more intuitive.\n- **Improved performance** â€“ lighter and more efficient calculations.\n- **Better LLM evaluation support** â€“ including new features like descriptor calculation directly in the cloud."
  },
  {
    "title": "Evidently Cloud v2",
    "description": "A new version of Evidently Cloud available starting April 10, 2025.",
    "filename": "docs-main/faq/cloud_v2.mdx",
    "ai_section": "## ðŸ†• Who Gets Cloud v2?\n- **All new users** are automatically enrolled in **Evidently Cloud v2**.\n- **Existing Cloud v1 users** can manually **switch** to the new version.\n\n<Warning>\n**Breaking changes:** Cloud v2 is **not compatible** with Evidently library versions below `0.7.0`.\n</Warning>"
  },
  {
    "title": "Evidently Cloud v2",
    "description": "A new version of Evidently Cloud available starting April 10, 2025.",
    "filename": "docs-main/faq/cloud_v2.mdx",
    "ai_section": "## ðŸ§© SDK Compatibility Matrix\n| Cloud Version | Required Evidently Library Version |\n|"
  },
  {
    "title": "Evidently Cloud v2",
    "description": "A new version of Evidently Cloud available starting April 10, 2025.",
    "filename": "docs-main/faq/cloud_v2.mdx",
    "ai_section": "- |"
  },
  {
    "title": "Evidently Cloud v2",
    "description": "A new version of Evidently Cloud available starting April 10, 2025.",
    "filename": "docs-main/faq/cloud_v2.mdx",
    "ai_section": "- |\n| **Cloud v2**  | `evidently>=0.7.0`                 |\n| **Cloud v1**  | `evidently<0.7.0`                  |\n\nMake sure you use the matching version of the Evidently Python library for your Cloud environment."
  },
  {
    "title": "Evidently Cloud v2",
    "description": "A new version of Evidently Cloud available starting April 10, 2025.",
    "filename": "docs-main/faq/cloud_v2.mdx",
    "ai_section": "## ðŸ”„ Switching Between Versions\nYou can switch back to **Cloud v1** from your **Account Settings** if needed. However, we **highly recommend** using **Cloud v2** for the latest and most powerful features.\n\n<Warning>\n**Deprecation Notice: Free users will have access to Evidently Cloud v1 until May 31, 2025.** Please make sure you migrate to Cloud v2 and corresponding SDK version within this period to be able to continue sending data without interruptions. After that, Cloud v1 will enter **read-only mode**.\n</Warning>"
  },
  {
    "title": "Evidently Cloud v2",
    "description": "A new version of Evidently Cloud available starting April 10, 2025.",
    "filename": "docs-main/faq/cloud_v2.mdx",
    "ai_section": "## ðŸ“¦ Need Help Migrating?\nIf you're a **paying customer** and need assistance with:\n\n- Migrating assets\n- Updating your code\n- Any technical support\n\nðŸ“§ Reach out to us at [**support@evidentlyai.com**](mailto:support@evidentlyai.com)"
  },
  {
    "title": "Contact us",
    "description": "How to connect with Evidently team.",
    "filename": "docs-main/faq/contact.mdx",
    "ai_section": "## Discord\nJoin our [Discord community](https://discord.gg/xZjKRaNp8b) to chat and connect with other users and developers."
  },
  {
    "title": "Contact us",
    "description": "How to connect with Evidently team.",
    "filename": "docs-main/faq/contact.mdx",
    "ai_section": "## GitHub\nOpen an issue on [GitHub](https://github.com/evidentlyai/evidently) to report bugs and ask questions related to the project."
  },
  {
    "title": "Contact us",
    "description": "How to connect with Evidently team.",
    "filename": "docs-main/faq/contact.mdx",
    "ai_section": "## Blog\nRead our [blog](https://evidentlyai.com/blog), along with our [guides](https://www.evidentlyai.com/mlops-guides) and [tutorials](https://www.evidentlyai.com/mlops-tutorials), for valuable tutorials and content."
  },
  {
    "title": "Contact us",
    "description": "How to connect with Evidently team.",
    "filename": "docs-main/faq/contact.mdx",
    "ai_section": "## Newsletter\n[Sign up](https://www.evidentlyai.com/sign-up) for our newsletter to receive updates on news, content, and product features."
  },
  {
    "title": "Contact us",
    "description": "How to connect with Evidently team.",
    "filename": "docs-main/faq/contact.mdx",
    "ai_section": "## Twitter\nFollow and connect with us on [Twitter](https://twitter.com/EvidentlyAI) for the latest updates and interactions."
  },
  {
    "title": "Contact us",
    "description": "How to connect with Evidently team.",
    "filename": "docs-main/faq/contact.mdx",
    "ai_section": "## Email\nFor general inquiries, you can reach us at [*hello@evidentlyai.com*](mailto:hello@evidentlyai.com). Please note that we do not provide open-source support via email; for help, please use the Discord community or open an issue on GitHub."
  },
  {
    "title": "Frequently Asked Questions",
    "description": "Popular questions.",
    "filename": "docs-main/faq/introduction.mdx",
    "ai_section": "## Evidently Cloud - Migration Guide\nWhat's new in Evidently Cloud v2."
  },
  {
    "title": "Frequently Asked Questions",
    "description": "Popular questions.",
    "filename": "docs-main/faq/introduction.mdx",
    "ai_section": "## Evidently Library - Migration Guide\nHow to migrate to a new Evidently 0.6 version and above."
  },
  {
    "title": "Frequently Asked Questions",
    "description": "Popular questions.",
    "filename": "docs-main/faq/introduction.mdx",
    "ai_section": "## OSS vs Cloud\nUnderstand feature availability."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## Overview of Evidently 0.6 Changes\nThis guide explains the key changes introduced in Evidently 0.6 and above for existing users who used earlier versions of the Evidently library prior to 2025."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## Transitioning to Evidently 0.6\nVersion 0.6 introduced a new core API with a new Report object. During the transition from versions 0.6 to 0.6.7, both the new and old APIs co-existed. Users could choose either the new API or the legacy API documented in old resources."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## Breaking Changes in Evidently 0.7\nThe Evidently 0.7 release made the new API the default. Users now import it as `from evidently import Report`. This version also includes updates to the platform."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## Data Definition Changes\nThe `column_mapping` has been replaced with `data_definition`. Users now explicitly create an Evidently `Dataset` object, allowing for cleaner mapping of input columns based on their type and role, facilitating the mapping of multiple targets and predictions."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## Descriptors Overview\nDescriptors offer row-level text evaluations with an updated API. Computing descriptors involves two steps: adding them to the source table and aggregating results or running conditional checks. This helps in reusing descriptor outputs efficiently, particularly for LLM evaluations."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## New Reports API\nThe core Report API has been enhanced. Reports now define configuration settings and return a separate result object upon running. Users can also use \"Group by\" to compute metrics for specific segments."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## Unified Test Suites and Reports\nReports and Tests have now been combined, allowing users to view test results within the same HTML file. This integration removes the need for separate Reports, enhancing usability and providing clearer data insights."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## Metric Redesign\nThe Metric object has been simplified, producing a single computation result with a fixed structure. This redesign improves JSON result parsing and UI integration, enabling users to focus on streamlined metrics."
  },
  {
    "title": "Migration Guide",
    "description": "How to migrate to the new Evidently version?",
    "filename": "docs-main/faq/migration.mdx",
    "ai_section": "## Simplified Dashboard API\nWith the redesign of Metrics, the Dashboard API has become simpler. Users can create new panels and point to specific Metric results easily. Custom metrics with custom renders are now viewable in the UI, enhancing the dashboard experience."
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "## Evidently Ecosystem\nEvidently AI develops several products, including the Evidently library, Tracely library, and the Evidently Platform. These offer open-source solutions and commercial options for various data and AI evaluations."
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "## Evidently Library\nThe **Evidently Python library** is designed for users to conduct various data and AI evaluations. It generates reports and test suites based on evaluation results, making it ideal for data scientists and AI engineers working in Python environments. The library is open-source and licensed under **Apache 2.0**."
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "## Tracely Library\nThe **Tracely Python library** enables users to capture near real-time data from AI applications. It utilizes OpenTelemetry and is also open-source, available under the **Apache 2.0** license."
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "## Evidently Platform\nThe **Evidently Platform** is a web application that focuses on AI testing and observability. It facilitates collaboration among teams for AI quality, from experiments to production monitoring. The platform has an open-source and a commercial edition:\n\n- **Open-source edition**: This basic version is included in the Evidently library and offers limited features for lightweight deployment.\n  \n- **Commercial edition**: Includes advanced features for AI quality workflows, with two deployment options:\n  - **Evidently Cloud**: Hosted and managed by Evidently AI for easier startup.\n  - **Evidently Enterprise (Self-Hosted)**: Suitable for teams with strict security needs, offering a full-featured version that can be deployed on private clouds or on-premises."
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "## OSS vs. Cloud / Enterprise\nThe platform editions vary in features, support levels, and maintenance costs:\n\n- All core evaluation features are available in the open-source version.\n- OSS provides a lightweight deployment with basic features.\n- The commercial version adds functionalities for dataset management, collaboration, and security features like role-based access control."
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "## Feature Availability\nA detailed comparison of features between the open-source and commercial editions highlights:\n\n| Category             | Feature                   | Open-source | Cloud and Enterprise |\n|"
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "- |"
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "-- |\n| **Core features**    | Tracing (instrumentation) | +           | +                    |\n|                      | Evaluations (100+ checks) | +           | +                    |\n|                      | Reports and Test Suites   | +           | +                    |\n|                      | Monitoring dashboard      | +           | +                    |\n|                      | Custom metrics            | +           | +                    |\n| **Premium features** | Trace viewer              | -           | +                    |\n|                      | No-code evaluations       | -           | +                    |\n|                      | Scheduled evaluations     | -           | +                    |\n|                      | Dataset management        | -           | +                    |\n|                      | Synthetic data generation | -           | +                    |\n|                      | Adversarial testing       | -           | +                    |\n|                      | AI agent testing          | -           | +                    |\n|                      | No-code dashboards        | -           | +                    |\n|                      | Alerts                    | -           | +                    |\n| **Access control**   | Authentication            | -           | +                    |\n|                      | Role-based access control | -           | +                    |\n\nFull details on commercial plans can be found on the [Pricing page](https://www.evidentlyai.com/pricing)."
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "## Support\nCommercial versions of the platform include dedicated support, while:\n\n- **Evidently OSS**: Limited to documentation and community forums. Users must be self-sufficient in troubleshooting. For more support, upgrading to Evidently Cloud or Enterprise is recommended.\n  \n- **Evidently Cloud / Enterprise**: Offers direct support from developers with varying tiers based on the chosen plan, including onboarding and training for the Enterprise Plan."
  },
  {
    "title": "Open-source vs. Cloud",
    "description": "Deployment options and feature overview.",
    "filename": "docs-main/faq/oss_vs_cloud.mdx",
    "ai_section": "## Hosting and Maintenance\nThe maintenance requirements vary by setup:\n\n- **Evidently OSS**: Users are responsible for deployment, management, backups, and scaling, requiring engineering resources.\n  \n- **Evidently Cloud**: Fully managed by the Evidently team, allowing users to focus on AI product development, including automatic updates and scalability.\n\n- **Evidently Enterprise (Self-Hosted)**: Large organizations retain data on-premises, with dedicated implementation support, but ongoing maintenance is the team's responsibility."
  },
  {
    "title": "Telemetry",
    "description": "What data is collected when you use Evidently open-source.",
    "filename": "docs-main/faq/telemetry.mdx",
    "ai_section": "## What is Telemetry?\nTelemetry refers to the collection of usage data. We collect some data to understand how many users we have and how they interact with Evidently open-source. This helps us improve the tool and prioritize implementing new features."
  },
  {
    "title": "Telemetry",
    "description": "What data is collected when you use Evidently open-source.",
    "filename": "docs-main/faq/telemetry.mdx",
    "ai_section": "## What data is collected?\nTelemetry is collected in Evidently starting from **version 0.4.0**. We only collect telemetry when you use **Evidently Monitoring UI**. We DO NOT collect any telemetry when you use the tool as a library, such as running it in a Jupyter notebook or in a Python script to generate Evidently Reports.\n\nWe collect **anonymous** usage data only about **environment** and **service** use. Sensitive information or data about the datasets you process, including schema, parameters, variable names, or anything related to the contents of the data or your code, is not collected. The specific types of data collected include:\n\n**Environment data**:\n- `timestamp`\n- `user_id`\n- `os_name`\n- `os_version`\n- `python_version`\n- `tool_name`\n- `tool_version`\n- `source_ip`\n\n**Service usage data** includes actions like:\n- `Startup`\n- `Index`\n- `List_projects`\n- `Get_project_info`\n- `Add_project`\n- and others."
  },
  {
    "title": "Telemetry",
    "description": "What data is collected when you use Evidently open-source.",
    "filename": "docs-main/faq/telemetry.mdx",
    "ai_section": "## How to enable/disable telemetry?\nBy default, telemetry is enabled. After starting up the service, you will see a message in the terminal:\n\n```\nAnonymous usage reporting is enabled. To disable it, set env variable {DO_NOT_TRACK_ENV} to any value\n```\n\nTo disable telemetry, set the environment variable `DO_NOT_TRACK` to any value:\n\n```\nexport DO_NOT_TRACK=1\n```\n\nThis will change the terminal message to:\n\n```\nAnonymous usage reporting is disabled.\n```\n\nTo enable telemetry again, unset the environment variable:\n\n```\nunset DO_NOT_TRACK\n```"
  },
  {
    "title": "Telemetry",
    "description": "What data is collected when you use Evidently open-source.",
    "filename": "docs-main/faq/telemetry.mdx",
    "ai_section": "## Event log examples\nEvent log examples provide insights into telemetry data collected during specific actions. Some examples include:\n\n- **Action: startup**:\n  ```\n  {\n    \"_timestamp\": \"2023-07-07T14:08:44.332528Z\",\n    \"action\": \"startup\",\n    ...\n  }\n  ```\n\n- **Action: index**:\n  ```\n  {\n    \"_timestamp\": \"2023-07-07T14:10:54.355143Z\",\n    \"action\": \"index\",\n    ...\n  }\n  ```\n\n- Additional actions include `list_projects`, `get_project_info`, `project_dashboard`, etc."
  },
  {
    "title": "Telemetry",
    "description": "What data is collected when you use Evidently open-source.",
    "filename": "docs-main/faq/telemetry.mdx",
    "ai_section": "## Should I opt out?\nWhile we appreciate any choice to opt out of telemetry, we encourage users to keep it enabled. Being open-source, we lack visibility into tool usage unless someone contacts us or files a GitHub issue. Keeping telemetry enabled helps us answer questions like:\n\n- How many people are actively using the tool?\n- Which features are being used most?\n- What environments are users running Evidently in?\n\nThis information guides our priorities in developing new features and testing performance across popular environments. However, if you prefer not to share any telemetry data, we respect that choice, and you can follow the steps above to disable data collection."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## Building Reliable AI Products\nWeâ€™re building Evidently AI to help teams ship reliable AI products: whether itâ€™s an ML model, an LLM app, or a complex agent workflow. Our tools are model-, framework-, and application-agnostic, so you can build and evaluate AI systems your way without limitations."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## We are Open-Source\n[**Evidently**](https://github.com/evidentlyai/evidently) is an open-source library with over 25 million downloads, 5000+ GitHub stars, and a thriving community. It's licensed under Apache 2.0. This gives full transparency - you can see exactly how every metric works and trust the implementation. It also delivers an intuitive API designed for a great developer experience. The **Evidently Platform** builds on the library with additional UI features and workflows for team collaboration. For enterprise users, we offer both Cloud and self-hosted options for full data privacy and control."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## Evidently is Very Modular\nEvidently is built to adapt to your needs without lock-ins or complex setups. Itâ€™s modular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval. You can:\n\n* Start with local ad hoc checks.\n* Add a UI to track evaluations over time.\n* Choose to upload raw data or only evaluation results when running evals.\n* Add monitoring as you are ready to move to production workflows.\n\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. It also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## 100+ Built-in Evaluations\nEvidently puts evaluations and quality testing first. We ship **100+ built-in evaluations** that cover many ML and LLM use cases. From ranking metrics to data drift algorithms and LLM judges, weâ€™ve done the hard work by implementing metrics and ways to visualize them. You can also extend Evidently by adding custom metrics. Evidently Cloud provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## Complete Feature Set\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows, including tracing, synthetic data, rich dashboards, and built-in alerting. \n\nGet the [Platform overview](/docs/platform/overview)."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## Loved by Community\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews). Weâ€™re also known for openly sharing knowledge that helps developers succeed. Resources include the [LLM evaluation course](https://www.evidentlyai.com/llm-evaluations-course), open-source [ML observability course](https://www.evidentlyai.com/ml-observability-course), [guides](https://www.evidentlyai.com/mlops-guides), and [blogs](https://www.evidentlyai.com/blog)."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## Handles Both ML and LLM\nEvidently supports both ML and LLM tasks. This versatility is important because real-world AI systems often overlap different workflows. For example, an LLM-based chatbot may need **classification** steps like detecting user intent, and if you are building with RAG, you are solving a **ranking** problem first. The Evidently Platform supports both complex nested workflows and structured tabular data, providing relevant metrics and views for each."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## Built for Collaboration\nEvidently started as an open-source project loved by data scientists and AI/ML engineers. Weâ€™re building a platform where domain experts and engineers can work together easily. Reliable AI systems require teamwork on curating test data, gathering feedback, and running evaluations. Our platform combines **no-code** workflows for non-technical users with an intuitive **API**, ensuring everyone has the tools they need to excel."
  },
  {
    "title": "Why Evidently?",
    "description": "Why choose Evidently.",
    "filename": "docs-main/faq/why_evidently.mdx",
    "ai_section": "## Trusted Partner\nFounded in 2021, Evidently AI is built by a team with 10+ years of experience deploying AI in high-scale, critical scenarios. We are backed by world-class investors like Y Combinator, Fly Ventures, Runa Capital, Nauta Capital, and angel investors. Our core Evidently library has a stable history of development and has earned trust from the community and enterprise users alike."
  },
  {
    "filename": "docs-main/images/changelog/readme.md",
    "ai_section": "## Images for Changelog Page\nThis section details the inclusion of images specifically designed for the changelog page."
  },
  {
    "filename": "docs-main/images/concepts/readme.md",
    "ai_section": "## Key Conceptual Guides\nIllustrations used for the key conceptual guides include the Library overview and the Platform overview."
  },
  {
    "filename": "docs-main/images/dashboard/readme.md",
    "ai_section": "## Images for Dashboard Panel Design\nThis section discusses the importance and usage of images in dashboard panel design. Images can enhance visual appeal, convey information quickly, and improve user engagement by providing context to data. Effective use of images involves selecting high-quality visuals that align with the data being presented and maintaining consistency in style across the dashboard."
  },
  {
    "filename": "docs-main/images/examples/readme.md",
    "ai_section": "## Screenshots for Guides\nThis section provides information on how to obtain and use screenshots for specific guides or cookbook examples. It includes tips on capturing clear images and ensuring they are relevant to the content."
  },
  {
    "filename": "docs-main/images/examples/readme.md",
    "ai_section": "## Using Screenshots Effectively\nLearn best practices for using screenshots in guides. This includes recommendations on resolution, formatting, and how to annotate images to enhance understanding."
  },
  {
    "filename": "docs-main/images/examples/readme.md",
    "ai_section": "## FAQ on Screenshot Submission\nThis section addresses frequently asked questions about submitting screenshots. It covers acceptable formats, size limits, and where to submit them for inclusion in guides."
  },
  {
    "filename": "docs-main/images/examples/readme.md",
    "ai_section": "## Troubleshooting Screenshot Issues\nIf you encounter problems while taking or uploading screenshots, this section offers solutions to common issues, including software recommendations and troubleshooting tips."
  },
  {
    "filename": "docs-main/images/metrics/readme.md",
    "ai_section": "## Images for Evaluations and Metrics\nThis section discusses the importance and usage of images for evaluations and metrics within the documentation. It covers how visual aids can enhance understanding and readability, ensuring that users can effectively interpret data and outcomes presented in the documents."
  },
  {
    "filename": "docs-main/images/synthetic/readme.md",
    "ai_section": "## Images for Datagen\nThis section provides information about the types and sources of images used in the datagen process, including considerations for quality, resolution, and diversity."
  },
  {
    "title": "What is Evidently?",
    "description": "Welcome to the Evidently documentation.",
    "mode": "wide",
    "filename": "docs-main/introduction.mdx",
    "ai_section": "## Overview of Evidently\nEvidently helps evaluate, test, and monitor data and AI-powered systems. It is an **open-source Python library** with over 25 million downloads, providing 100+ evaluation metrics, a declarative testing API, and a lightweight visual interface to explore the results. The **Evidently Cloud platform** offers a complete toolkit for AI testing and observability, including tracing, synthetic data generation, dataset management, eval orchestration, alerting, and a no-code interface for domain experts to collaborate on AI quality. The goal of Evidently is to assist teams in building and maintaining reliable, high-performing AI products, ranging from predictive ML models to complex LLM-powered systems."
  },
  {
    "title": "What is Evidently?",
    "description": "Welcome to the Evidently documentation.",
    "mode": "wide",
    "filename": "docs-main/introduction.mdx",
    "ai_section": "## Get Started\nYou can run your first evaluation in just a couple of minutes. Here are two options to get you started:\n\n- **LLM evaluation**: Evaluate the quality of LLM system outputs.\n- **ML monitoring**: Test tabular data quality and data drift."
  },
  {
    "title": "What is Evidently?",
    "description": "Welcome to the Evidently documentation.",
    "mode": "wide",
    "filename": "docs-main/introduction.mdx",
    "ai_section": "## Feature Overview\nEvidently offers a range of features for various applications:\n\n- **Evidently Platform**: Key features of the AI observability platform.\n- **Evidently Library**: Comprehensive information on how the Python evaluation library works."
  },
  {
    "title": "What is Evidently?",
    "description": "Welcome to the Evidently documentation.",
    "mode": "wide",
    "filename": "docs-main/introduction.mdx",
    "ai_section": "## Learn More\nTo expand your knowledge and skills with Evidently, check out the following:\n\n- **Metrics**: Browse the catalogue of 100+ evaluations available for use.\n- **Cookbook**: Access end-to-end code tutorials and examples for practical implementation."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Introductory Resources\nFor an intro, read about [Core Concepts](/docs/library/overview) and check the [LLM Quickstart](/quickstart_llm). For a reference code example, see this [Descriptor cookbook](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/descriptors.ipynb)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Deterministic Evals\nThis section covers programmatic and heuristics-based evaluations."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Pattern Match\nCheck for general pattern matching with several methods discussed below.\n\n| Name             | Description                                                                                                                                                                                   | Parameters                                                                                                                  |\n|"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "- |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "|"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "|\n| **ExactMatch()** | Checks if the column contents match between two provided columns. Returns True/False for every input. Example: `ExactMatch(columns=[\"answer\", \"target\"])`                                  | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias` </li></ul>                                           |\n| **RegExp()**     | Matches the text against a set regular expression. Returns True/False for every input. Example: `RegExp(reg_exp=r\"^I\")`                                                                   | **Required:** <ul><li>`reg_exp`</li></ul>**Optional:** <ul><li> `alias`</li></ul>                                           |\n| **BeginsWith()** | Checks if the text starts with a given combination. Returns True/False for every input. Example: `BeginsWith(prefix=\"How\")`                                                              | **Required:** <ul><li>`prefix`</li></ul>**Optional:** <ul><li>`alias`</li><li> `case_sensitive = True` or `False`</li></ul> |\n| **EndsWith()**   | Checks if the text ends with a given combination. Returns True/False for every input. Example: `EndsWith(suffix=\"Thank you.\")`                                                            | **Required:** <ul><li>`suffix`</li></ul>**Optional:** <ul><li>`alias`</li><li>`case_sensitive = True` or `False`</li></ul>  |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Content Checks\nVerify the presence of specific words, items, or components.\n\n| Name                 | Description                                                                                                                                                                      | Parameters                                                                                             |\n|"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "|\n| **Contains()**       | Checks if the text contains **any** or **all** specified items. Returns True/False for every input. Example: `Contains(items=[\"chatgpt\"])`                                   | **Required:** <ul><li>`items: List[str]`</li></ul>**Optional:** <ul><li>`alias`</li><li> `mode = any` or `all` </li><li> `case_sensitive = True` or `False`</li></ul>  |\n| **DoesNotContain()** | Checks if the text does not contain the specified items. Returns True/False for every input. Example: `DoesNotContain(items=[\"as a large language model\"])`                     | **Required:** <ul><li>`items: List[str]`</li></ul>**Optional:** <ul><li>`alias`</li><li>`mode = all`</li><li> `case_sensitive = True` or `False`</li></ul>           |\n| **IncludesWords()**  | Checks if the text includes **any** or **all** specified words. Considers only vocabulary words. Returns True/False for every input.                                          | **Required:** <ul><li>`words_list: List[str]`</li></ul>**Optional:** <ul><li>`alias`</li><li>`mode = any` or `all`</li><li> `lemmatize = True` or `False`</li></ul> |\n| **ExcludesWords()**  | Checks if the text excludes all specified words. Considers only vocabulary words. Returns True/False for every input.                                                         | **Required:** <ul><li>`words_list: List[str]`</li></ul>**Optional:** <ul><li>`alias`</li><li>`mode = all`</li><li> `lemmatize = True` or `False`</li></ul>          |\n| **ItemMatch()**      | Checks if the text contains **any** or **all** specified items. Returns True/False for each row. Example: `ItemMatch([\"Answer\", \"Expected_items\"])`                           | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias`</li><li>`mode = all` or `any`</li><li>`case_sensitive = True` or `False`</li></ul>         |\n| **ItemNoMatch()**    | Checks if the text excludes **all** specified items. Returns True/False for each row. Example: `ItemMatch([\"Answer\", \"Forbidden_items\"])`                                     | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias`</li><li>`mode = all`</li><li>`case_sensitive = True` or `False`</li></ul>                    |\n| **WordMatch()**      | Checks if the text includes **any** or **all** specified words. Considers only vocabulary words. Returns True/False for every input. Example: `WordMatch([\"Answer\", \"Expected_words\"]` | **Required:**<ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias`</li><li>`mode = any` or `all`</li><li>`lemmatize = True` or `False`</li></ul>               |\n| **WordNoMatch()**    | Checks if the text excludes **all** specified words. Considers only vocabulary words. Returns True/False for every input. Example: `WordNoMatch([\"Answer\", \"Forbidden_words\"])` | **Required:** <ul><li>`columns`</li></ul>**Optional:** <ul><li>`alias`</li><li>`mode = all`</li><li>`lemmatize = True` or `False`</li></ul>                     |\n| **ContainsLink()**   | Checks if the column contains at least one valid URL. Returns True/False for each row.                                                                                          | **Optional:** <ul><li>`alias`</li></ul>                                                                 |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Syntax Validation\nValidate structured data formats or code syntax.\n\n| Name                  | Description                                                                                                                                                                                                                                                                                                     | Parameters                                                                                                                                                                       |\n|"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "-- |\n| **IsValidJSON()**     | Checks if the column contains a valid JSON. Returns True/False for every input.                                                                                                                                                                                                                              | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |\n| **JSONSchemaMatch()** | Checks if the column contains a valid JSON object matching the expected **schema**. Returns True/False for each input. Example: `JSONSchemaMatch(expected_schema={\"name\": str, \"age\": int}, exact_match=False, validate_types=True)`                                                                        | **Required:** <ul><li>`expected_schema: Dict[str, type]`</li></ul> **Optional:** <ul><li> `exact_match = True` or `False` </li><li> `validate_types = True` or `False`</li></ul> |\n| **JSONMatch()**       | Checks if the column contains a valid JSON object matching a JSON provided in a reference column. Returns True/False for every input. Example: `JSONMatch(first_column=\"Json1\", second_column=\"Json2\")`                                                                                                   | **Required:** <ul><li>`first_column`</li><li>`second_column`</li></ul>**Optional:** <ul><li>`alias`</li></ul>                                                                    |\n| **IsValidPython()**   | Checks if the column contains valid Python code without syntax errors. Returns True/False for every input.                                                                                                                                                                                                  | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |\n| **IsValidSQL()**      | Checks if the column contains a valid SQL query without executing the query. Returns True/False for every input.                                                                                                                                                                                            | **Optional:** <ul><li>`alias`</li></ul>                                                                                                                                          |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Validations\n\nThis section covers the validation functions available for assessing code and queries.\n\n| Name           | Descriptor                                                                                                                                                            | Parameters                                    |\n| :"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "|\n| **IsValidPython()**  | <ul><li>Checks if the column contains valid Python code without syntax errors.</li><li>Returns True/False for every input.</li></ul>                               | **Optional:** <ul><li>`alias`</li></ul>     |\n| **IsValidSQL()**     | <ul><li>Checks if the column contains a valid SQL query without executing the query.</li><li>Returns True/False for every input.</li></ul>                            | **Optional:** <ul><li>`alias`</li></ul>     |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Text Statistics\n\nThis section outlines the text statistics functions that can provide insights into text data.\n\n| Name                               | Descriptor                                                                                                                                                      | Parameters                                                                 |\n| :"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "-- |\n| **TextLength()**                   | <ul><li>Measures the length of the text in symbols.</li><li>Returns an absolute number.</li></ul>                                                            | **Optional:** <ul><li>`alias`</li></ul>                                    |\n| **OOVWordsPercentage()**           | <ul><li>Calculates the percentage of out-of-vocabulary words based on imported NLTK vocabulary.</li><li>Returns a score on a scale: 0 to 100.</li></ul>     | **Optional:** <ul><li>`alias`</li><li>`ignore_words: Tuple = ()`</li></ul> |\n| **NonLetterCharacterPercentage()** | <ul><li>Calculates the percentage of non-letter characters.</li><li>Returns a score on a scale: 0 to 100.</li></ul>                                           | **Optional:** <ul><li>`alias`</li></ul>                                    |\n| **SentenceCount()**                | <ul><li>Counts the number of sentences in the text.</li><li>Returns an absolute number.</li></ul>                                                            | **Optional:** <ul><li>`alias`</li></ul>                                    |\n| **WordCount()**                    | <ul><li>Counts the number of words in the text.</li><li>Returns an absolute number.</li></ul>                                                                  | **Optional:** <ul><li>`alias`</li></ul>                                    |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Custom Descriptors\n\nThis section allows for the implementation of custom programmatic checks for specific columns.\n\n| Name                          | Descriptor                                                                                                                                                                                                                                       | Parameters                                                                                                                            |\n| :"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "- |\n| **CustomDescriptor()**        | <ul><li>Implements a custom check for specific column(s) as a Python function.</li><li>Use it to run your own programmatic checks.</li><li>Returns score and/or label as specified.</li><li>Can accept and return multiple columns.</li></ul>        | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor).   |\n| **CustomColumnsDescriptor()** | <ul><li>Implements a custom check as a Python function that can be applied to any column in the dataset.</li><li>Use it to run your own programmatic checks.</li><li>Returns score and/or label as specified.</li><li>Accepts and returns a single column.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor).   |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## LLM-based Evaluations\n\nThis section focuses on using external LLMs with an evaluation prompt along with their respective parameters.\n\n| Name          | Descriptor                                                                                                                                                                                                   | Parameters                                                                                                                                                                                                |\n| :"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "|\n| **LLMEval()** | <ul><li>Scores the text using user-defined criteria.</li><li>You must specify provider, model and use prompt template to formulate the criteria.</li><li>Returns score and/or label as specified.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`template`</li><li>`provider`</li><li>`model`</li><li>`additional_columns: dict`</li><li>See [custom LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## RAG Evaluations\n\nThis section discusses RAG-specific evaluations for retrieval and generation.\n\n| Name                        | Descriptor                                                                                                                                                                                                                                                                                                                                                                                                            | Parameters                                                                                                                                                                                                                                                                                                                                          |\n| :"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "|\n| **ContextQualityLLMEval()** | <ul><li>Evaluates if the context provides sufficient information to answer the question.</li><li>Returns a label (VALID or INVALID) or a score.</li><li>Run over the \"context\" column and pass the `question` column as a parameter.</li><li>Example: `ContextQualityLLMEval(\"Context\", question=\"Question\")`</li></ul>                                                                                                   |  **Required:** <ul><li>`question`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                     |\n| **ContextRelevance()**      | <ul><li>Checks if the context is relevant to the given question (0 to 1) for multiple context chunks.</li><li>Pass all context chunks as a list in the `context` column.</li><li>Uses semantic similarity (default) or LLM.</li><li>Aggregates relevance: `mean` (default) or `hit` (at least one chunk is relevant).</li><li>Example: `ContextRelevance(\"Question\", \"Context\", output_scores=True, aggregation_method=\"hit\", method=\"llm\")`</li></ul> | **Required:** <ul><li>`input`</li><li>`contexts`</li></ul> **Optional:** <ul><li>`output_scores`: `False` or `True` </li><li>`method`: `semantic_similarity` or `llm`</li><li>`aggregation_method`: `mean` or `hit` </li><li>`aggregation_method_params={\"threshold\":0.95}` (set the relevance threshold as greater or equal, 0.8 by default)</li><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |\n| **FaithfulnessLLMEval()**   | <ul><li>Assesses whether the response stays faithful to the given context.Checks for hallucinations or unsupported claims.</li><li>Returns a label (FAITHFUL or UNFAITHFUL) or a score.</li><li>Run over the \"response\" column and pass the `context` column as a parameter.</li><li>Example: `FaithfulnessLLMEval(\"Response\", context=\"Context\")`</li></ul>                                                                                                             | **Required:** <ul><li>`context`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                              |\n| **CompletenessLLMEval()**   | <ul><li>Determines whether the response fully uses the information provided in the context.</li><li>Returns a label (COMPLETE or INCOMPLETE) or a score.</li><li>Run over the \"response\" column and pass the `context` column as a parameter.</li><li>Example: `CompletenessLLMEval(\"Response\", context=\"Context\")`</li></ul>                                                                                                                                                     | **Required:** <ul><li>`context`</li></ul> **Optional:** <ul><li>`alias`</li><li>`provider`</li><li>`model`</li><li>See [LLM judge parameters](/metrics/customize_llm_judge).</li></ul>                                                                                                                                                              |"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Generation Evaluations\n\nThis section will include evaluations specifically tailored for varied generation scenarios."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Completeness Evaluation\n**CompletenessLLMEval()** determines whether the response fully utilizes the information provided in the context. It returns a label (COMPLETE or INCOMPLETE) or a score. This function is run over the \"response\" column and the `context` column is passed as a parameter.  \n**Example:** `CompletenessLLMEval(\"Response\", context=\"Context\")`\n\n**Required:**  \n- `context`\n\n**Optional:**  \n- `alias`\n- `provider`\n- `model`  \n- See [LLM judge parameters](/metrics/customize_llm_judge)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Correctness Evaluation\n**CorrectnessLLMEval()** evaluates the correctness of a response by comparing it with the target output. It is especially useful for Retrieval-Augmented Generation (RAG) or any LLM generation with a ground truth output. The function returns a label (CORRECT or INCORRECT) or a score. It is run over the \"response\" column with the `target_output` column passed as a parameter.  \n**Example:** `CorrectnessLLMEval(\"Response\", target_output=\"Target\")`\n\n**Required:**  \n- `target_output`\n\n**Optional:**  \n- `alias`\n- `provider`\n- `model`  \n- See [LLM judge parameters](/metrics/customize_llm_judge)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Decline Evaluation\n**DeclineLLMEval()** detects if the text contains a refusal or rejection. It is useful for identifying instances where an LLM denies the userâ€™s request. The function returns a label (DECLINE or OK) or a score.\n\n**Optional:**  \n- `alias`\n- `provider`\n- `model`  \n- See [LLM judge parameters](/metrics/customize_llm_judge)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## PII Detection\n**PIILLMEval()** detects texts that contain Personally Identifiable Information (PII). It returns a label (PII or OK) or a score.\n\n**Optional:**  \n- `alias`\n- `provider`\n- `model`  \n- See [LLM judge parameters](/metrics/customize_llm_judge)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Negativity Detection\n**NegativityLLMEval()** detects negative texts and returns a label (NEGATIVE or POSITIVE) or a score.\n\n**Optional:**  \n- `alias`\n- `provider`\n- `model`  \n- See [LLM judge parameters](/metrics/customize_llm_judge)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Bias Detection\n**BiasLLMEval()** detects biased texts and returns a label (BIAS or OK) or a score.\n\n**Optional:**  \n- `alias`\n- `provider`\n- `model`  \n- See [LLM judge parameters](/metrics/customize_llm_judge)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Toxicity Detection\n**ToxicityLLMEval()** detects toxic texts and returns a label (TOXICITY or OK) or a score.\n\n**Optional:**  \n- `alias`\n- `provider`\n- `model`  \n- See [LLM judge parameters](/metrics/customize_llm_judge)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Semantic Similarity\n**SemanticSimilarity()** calculates pairwise semantic similarity (Cosine Similarity) between two columns using a sentence embeddings model, specifically `all-MiniLM-L6-v2`. It returns a score from 0 to 1: (0: different, 0.5: unrelated, 1: identical).  \n**Example use:** `SemanticSimilarity(columns=[\"Question\", \"Answer\"])`\n\n**Required:**  \n- `columns`\n\n**Optional:**  \n- `alias`"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## BERT Score\n**BERTScore()** calculates similarity between two text columns based on token embeddings, returning the BERTScore (F1 Score).  \n**Example use:** `BERTScore(columns=[\"Answer\", \"Target\"])`\n\n**Required:**  \n- `columns`\n\n**Optional:**  \n- `model`\n- `tfidf_weighted`\n- `alias`"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Sentiment Analysis\n**Sentiment()** analyzes text sentiment using a word-based model from NLTK, returning a score ranging from -1 (negative) to 1 (positive).\n\n**Optional:**  \n- `alias`"
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Hugging Face Model Scoring\n**HuggingFace()** scores the text using a user-selected HuggingFace model. For example models, see [HuggingFace descriptor docs](/metrics/customize_hf_descriptor).\n\n**Optional:**  \n- `alias`  \n- See [docs](/metrics/customize_hf_descriptor)."
  },
  {
    "title": "All Descriptors",
    "description": "Reference page for all row-level text and LLM evals.",
    "filename": "docs-main/metrics/all_descriptors.mdx",
    "ai_section": "## Hugging Face Toxicity Detection\n**HuggingFaceToxicity()** detects hate speech using a `roberta-hate-speech` model. It returns the predicted probability for the â€œhateâ€ label, on a scale from 0 to 1.\n\n**Optional:**  \n- `toxic_label` (default: `hate`)\n- `alias`  \n- See [docs](/metrics/customize_hf_descriptor)."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Introduction\nFor an intro, read [Core Concepts](/docs/library/overview) and check quickstarts for [LLMs](docs/quickstart_llm) or [ML](docs/quickstart_ml). For a reference code example, see this [Metric cookbook](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/metrics.ipynb)."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## How to Read the Tables\n- **Metric**: The name of the Metric or Preset you can pass to `Report`.\n  \n- **Description:** What it does. Complex Metrics link to explainer pages.\n  \n- **Parameters:** Available options. You can also add conditional `tests` to any Metric with standard operators like `eq` (equal), `gt` (greater than), etc. [How Tests work](/docs/library/tests).\n  \n- **Test defaults**: Conditions that apply when you invoke Tests but do not set a pass/fail condition yourself.\n  \n  - **With reference**: Conditions set relative to a reference dataset.\n  \n  - **No reference**: Uses fixed heuristics (like expect no missing values)."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Text Evals\nSummarizes results of text or LLM evaluations. To score individual inputs, first use [descriptors](/metrics/all_descriptors).\n\n### Data Definition\nYou may need to map text columns using [Data definition](/docs/library/data_definition).\n\n| Metric          | Description                                                                                                                                                             | Parameters                                 | Test Defaults                          |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "--|\n| **TextEvals()** | - Large Preset. <br>- Shows `ValueStats` for all descriptors. <br>- Requires specifying descriptors ([more info](/docs/library/descriptors)). <br>- Metric result: for all Metrics. <br>- [Preset page](/metrics/preset_text_evals). | **Optional**: `columns`                  | As in Metrics included in `ValueStats` |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Columns\nUse to aggregate descriptor results or check data quality on a column level.\n\n### Data Definition\nYou may need to map column types using [Data definition](/docs/library/data_definition).\n\n### Value Stats\nDescriptive statistics.\n\n| Metric         | Description                                                                                                                                                             | Parameters                                                                                                                                        | Test Defaults                                                                                                                        |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "-|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "|\n| **ValueStats()** | - Small Preset, column-level. <br>- Computes various descriptive stats. <br>- Returns different stats based on column type.                                           | **Required**: `column` <br> **Optional**: [Test conditions](/docs/library/tests)                                                            | **No reference**: As in individual Metrics. <br> **With reference**: As in individual Metrics.                     |\n| **MinValue()**   | - Column-level. <br>- Returns min value for a given numerical column.                                                                                                 | **Required**: `column` <br> **Optional**: [Test conditions](/docs/library/tests)                                                              | **No reference**: N/A. <br> **With reference**: Fails if Min Value differs by more than 10% (+/-).                              |\n| **StdValue()**   | - Column-level. <br>- Computes standard deviation of a numerical column.                                                                                               | **Required**: `column` <br> **Optional**: [Test conditions](/docs/library/tests)                                                              | **No reference**: N/A. <br> **With reference**: Fails if standard deviation differs by more than 10% (+/-).                     |\n| **MeanValue()**  | - Column-level. <br>- Computes the mean value of a numerical column.                                                                                                   | **Required**: `column` <br> **Optional**: [Test conditions](/docs/library/tests)                                                              | **No reference**: N/A. <br> **With reference**: Fails if mean value differs by more than 10%.                                  |\n| **MaxValue()**   | - Column-level. <br>- Computes the max value of a numerical column.                                                                                                    | **Required**: `column` <br> **Optional**: [Test conditions](/docs/library/tests)                                                              | **No reference**: N/A. <br> **With reference**: Fails if max value is higher than in the reference.                             |\n| **MedianValue()**| - Column-level. <br>- Computes the median value of a numerical column.                                                                                                 | **Required**: `column` <br> **Optional**: [Test conditions](/docs/library/tests)                                                              | **No reference**: N/A. <br> **With reference**: Fails if median value differs by more than 10% (+/-).                          |\n| **QuantileValue()**| - Column-level. <br>- Computes quantile value of a numerical column. Defaults to 0.5 if no quantile is specified.                                               | **Required**: `column` <br> **Optional**: `quantile` (default: 0.5), [Test conditions](/docs/library/tests)                                   | **No reference**: N/A. <br> **With reference**: Fails if quantile value differs by more than 10% (+/-).                      |\n| **CategoryCount()** | - Column-level. <br>- Counts occurrences of specified categories. <br> Example: `CategoryCount(column=\"city\", category=\"NY\")`                                       | **Required**: `column`, `category`, `categories` <br> **Optional**: [Test conditions](/docs/library/tests)                                     | **No reference**: N/A. <br> **With reference**: Fails if specified category is not present.                                    |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Column Data Quality\nColumn-level data quality metrics.\n\n### Data Definition\nYou may need to map column types using [Data definition](/docs/library/data_definition).\n\n| Metric                     | Description                                                                                                                                          | Parameters                                                                                                                                 | Test Defaults                                                                                                                                                            |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "--|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "--|\n| **MissingValueCount()**    | - Column-level. <br>- Counts number and share of missing values. <br>- Metric result: `count`, `share`.                                           | **Required**: `column` <br> **Optional**: [Test conditions](/docs/library/tests)                                                           | **No reference**: Fails if there are missing values. <br> **With reference**: Fails if share of missing values is >10% higher.                                      |\n| **InRangeValueCount()**    | - Column-level. <br>- Counts number and share of values in a set range. <br> Example: `InRangeValueCount(column=\"age\", left=\"1\", right=\"18\")`   | **Required**: `column`, `left`, `right` <br> **Optional**: [Test conditions](/docs/library/tests)                                         | **No reference**: N/A. <br> **With reference**: Fails if column contains values out of min-max reference range.                                                         |\n| **OutRangeValueCount()**   | - Counts number and share of values outside of the specified range.                                                                                  | **Required**: `column`, `left`, `right` <br> **Optional**: [Test conditions](/docs/library/tests)                                         | **No reference**: N/A. <br> **With reference**: Fails if column contains values out of min-max reference range.                                                          |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## InRangeValueCount\n- **Description**: Counts the number and share of values in the specified range for a given column.\n- **Example**: \n``` \nInRangeValueCount(column=\"age\", left=\"1\", right=\"18\")\n```\n- **Metric Results**: `count`, `share`.\n\n- **Required Parameters**: \n  - `column`\n  - `left`\n  - `right`\n\n- **Optional Parameters**: \n  - [Test conditions](/docs/library/tests)\n\n- **Test Defaults**: \n  - **No reference**: N/A.\n  - **With reference**: Fails if the column contains values out of the min-max reference range."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## OutRangeValueCount\n- **Description**: Counts the number and share of values out of the specified range for a given column.\n- **Metric Results**: `count`, `share`.\n\n- **Required Parameters**: \n  - `column`\n  - `left`\n  - `right`\n\n- **Optional Parameters**: \n  - [Test conditions](/docs/library/tests)\n\n- **Test Defaults**: \n  - **No reference**: N/A.\n  - **With reference**: Fails if any value is out of the min-max reference range."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## InListValueCount\n- **Description**: Counts the number and share of values in the specified list for a given column.\n- **Metric Results**: `count`, `share`.\n\n- **Required Parameters**: \n  - `column`\n  - `values`\n\n- **Optional Parameters**: \n  - [Test conditions](/docs/library/tests)\n\n- **Test Defaults**: \n  - **No reference**: N/A.\n  - **With reference**: Fails if any value is out of the list."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## OutListValueCount\n- **Description**: Counts the number and share of values out of the specified list for a given column.\n- **Example**: \n```\nOutListValueCount(column=\"city\", values=[\"Lon\", \"NY\"])\n```\n- **Metric Results**: `count`, `share`.\n\n- **Required Parameters**: \n  - `column`\n  - `values`\n\n- **Optional Parameters**: \n  - [Test conditions](/docs/library/tests)\n\n- **Test Defaults**: \n  - **No reference**: N/A.\n  - **With reference**: Fails if any value is out of the list."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## UniqueValueCount\n- **Description**: Counts the number and share of unique values in a given column.\n- **Metric Results**: `values` (dict with `count`, `share`).\n\n- **Required Parameters**: \n  - `column`\n\n- **Optional Parameters**: \n  - [Test conditions](/docs/library/tests)\n\n- **Test Defaults**: \n  - **No reference**: N/A.\n  - **With reference**: Fails if the share of unique values differs by >10% (+/-)."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Dataset Usage\n- **Purpose**: Used for exploratory data analysis and data quality checks.\n  \n- **Information**: Refer to [Data definition](/docs/library/data_definition). Mapping of column types, ID, and timestamp may be needed."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Dataset Stats\n- **Description**: Provides descriptive statistics for the dataset.\n\n| Metric                  | Description                                                                                                                         | Parameters                                                             | Test Defaults                                                                                                  |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "--|\n| **DataSummaryPreset()** | Combines `DatasetStats` and `ValueStats` for all or specified columns, resulting in metrics for all metrics.                     | **Optional**: `columns`                                              | As in individual Metrics.                                                                                      |\n| **DatasetStats()**      | Calculates descriptive dataset stats such as columns by type, rows, missing values, etc., resulting in metrics for all metrics. | None                                                                  | As in included Metrics.                                                                                       |\n| **RowCount()**          | Counts the number of rows in the dataset, resulting in a `value`.                                                                  | **Optional**: [Test conditions](/docs/library/tests)                | **No reference**: N/A; **With reference**: Fails if row count differs by >10%.                               |\n| **ColumnCount()**       | Counts the number of columns in the dataset, resulting in a `value`.                                                               | **Optional**: [Test conditions](/docs/library/tests)                | **No reference**: N/A; **With reference**: Fails if not equal to reference.                                   |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Dataset Data Quality\n- **Description**: Dataset-level data quality metrics.\n\n| Metric                                          | Description                                                                                                                   | Parameters                                                                                                      | Test Defaults                                                                                                                                                          |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "-|\n| **ConstantColumnsCount()**                      | Counts the number of constant columns, resulting in a `value`.                                                               | **Optional**: [Test conditions](/docs/library/tests)                                                         | **No reference**: Fails if thereâ€™s at least one constant column; **With reference**: Fails if count is higher than in reference.                                   |\n| **EmptyRowsCount()**                            | Counts the number of empty rows, resulting in a `value`.                                                                      | **Optional**: [Test conditions](/docs/library/tests)                                                         | **No reference**: Fails if thereâ€™s at least one empty row; **With reference**: Fails if share differs by >10%.                                                      |\n| **EmptyColumnsCount()**                         | Counts the number of empty columns, resulting in a `value`.                                                                    | **Optional**: [Test conditions](/docs/library/tests)                                                         | **No reference**: Fails if thereâ€™s at least one empty column; **With reference**: Fails if count is higher than in reference.                                       |\n| **DuplicatedRowCount()**                        | Counts the number of duplicated rows, resulting in a `value`.                                                                 | **Optional**: [Test conditions](/docs/library/tests)                                                         | **No reference**: Fails if thereâ€™s at least one duplicated row; **With reference**: Fails if share differs by >10% (+/-).                                          |\n| **DuplicatedColumnsCount()**                    | Counts the number of duplicated columns, resulting in a `value`.                                                               | **Optional**: [Test conditions](/docs/library/tests)                                                         | **No reference**: Fails if thereâ€™s at least one duplicated column; **With reference**: Fails if count is higher than in reference.                                 |\n| **DatasetMissingValueCount()**                  | Calculates the number and share of missing values, resulting in `share` and `count`.                                         | **Required**: `columns`; **Optional**: [Test conditions](/docs/library/tests)                                 | **No reference**: Fails if there are missing values; **With reference**: Fails if share is >10% higher than reference (+/-).                                        |\n| **AlmostConstantColumnsCount()**                | Counts almost constant columns (95% identical values), resulting in a `value`.                                               | **Optional**: [Test conditions](/docs/library/tests)                                                         | **No reference**: Fails if thereâ€™s at least one almost constant column; **With reference**: Fails if count is higher than in reference.                           |\n| **ColumnsWithMissingValuesCount()**             | Counts columns with missing values, resulting in a `value`.                                                                   | **Optional**: [Test conditions](/docs/library/tests)                                                         | **No reference**: Fails if thereâ€™s at least one column with missing values; **With reference**: Fails if count is higher than in reference.                       |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Data Drift\n- **Purpose**: Detects distribution drift for text and tabular data or over computed text descriptors.\n  \n- **Details**: Checks 20+ drift methods listed separately: [text and tabular](/metrics/customize_data_drift).\n\n- **References**: Consult [Data definition](/docs/library/data_definition) and [Metrics explainers](/metrics/explainer_drift) for an understanding of how data drift works."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Missing Values Count\nCounts columns with missing values.  \n- **Metric result:** `value`.  \n- **No reference:** Fails if there is at least one column with missing values.  \n- **With reference:** Fails if count is higher than in reference."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Data Drift Detection\nUse to detect distribution drift for text and tabular data or over computed text descriptors. Checks 20+ drift methods listed separately: [text and tabular](/metrics/customize_data_drift).\n\n<Info>\n  [Data definition](/docs/library/data_definition). You may need to map column types, ID, and timestamp.\n</Info>\n\n<Info>\n  [Metrics explainers](/metrics/explainer_drift). Understand how data drift works.\n</Info>\n\n### Data Drift Metrics\n| Metric                                | Description                                                                                                                                                                                                                                                       | Parameters                                                                                                                                                                                                                                           | Test Defaults                                                                                                                            |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "- |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "- |\n| **DataDriftPreset()**                 | - Large Preset.<br>- Requires reference.<br>- Calculates data drift for all or set columns.<br>- Uses the default or set method.<br>- Returns drift score for each column.<br>- Visualizes all distributions.<br>- **Metric result:** All Metrics.<br>- [Preset page](/metrics/customize_data_drift). | **Optional:** `columns`, `method`, `cat_method`, `num_method`, `per_column_method`, `threshold`, `cat_threshold`, `num_threshold`, `per_column_threshold`. See [drift options](/metrics/customize_data_drift).                                  | **With reference:** Data drift defaults, depending on column type. See [drift methods](/metrics/customize_data_drift).                     |\n| **DriftedColumnsCount()**             | - Dataset-level.<br>- Requires reference.<br>- Calculates the number and share of drifted columns in the dataset.<br>- Each column is tested for drift using the default algorithm or set method.<br>- Returns only the total number of drifted columns.<br>- **Metric result:** `count`, `share`. | **Optional:** `columns`, `method`, `cat_method`, `num_method`, `per_column_method`, `threshold`, `cat_threshold`, `num_threshold`, `per_column_threshold`. See [drift options](/metrics/customize_data_drift).                                  | **With reference:** Fails if 50% of columns are drifted.                                                                                     |\n| **ValueDrift()**                      | - Column-level.<br>- Requires reference.<br>- Calculates data drift for a defined column (num, cat, text).<br>- Visualizes distributions.<br>- **Metric result:** `value`.                                                                              | **Required:** `column`.<br>**Optional:** `method`, `threshold`. See [drift options](/metrics/customize_data_drift).                                                                                                                          | **With reference:** Data drift defaults, depending on column type. See [drift methods](/metrics/customize_data_drift).                       |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Classification Metrics\nUse to evaluate quality on a classification task (probabilistic, non-probabilistic, binary and multi-class).\n\n<Info>\n  [Data definition](/docs/library/data_definition). You may need to map prediction, target columns, and classification type.\n</Info>\n\n### General Classification Metrics\n| Metric                         | Description                                                                                                                                                                 | Parameters                                                                                                                                                                                                                               | Test Defaults                                                                                                                                                                        |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "|\n| **ClassificationPreset()**     | - Large Preset with many classification Metrics and visuals.<br>- See [Preset page](/metrics/preset_classification).<br>- **Metric result:** All Metrics.                | **Optional:** `probas_threshold`.                                                                                                                                                                                                         | As in individual Metrics.                                                                                                                                                            |\n| **ClassificationQuality()**    | - Small Preset.<br>- Summarizes quality Metrics in a single widget.<br>- **Metric result:** All Metrics.                                                                  | **Optional:** `probas_threshold`.                                                                                                                                                                                                         | As in individual Metrics.                                                                                                                                                            |\n| **Accuracy()**                 | - Calculates accuracy.<br>- **Metric result:** `value`.                                                                                                                  | **Optional:** [Test conditions](/docs/library/tests).                                                                                                                                                                              | **No reference:** Fails if lower than dummy model accuracy.<br>**With reference:** Fails if accuracy differs by >20%.                                                               |\n| **Precision()**                | - Calculates precision.<br>- Visualizations available: Confusion Matrix, PR Curve, PR Table.<br>- **Metric result:** `value`.                                           | **Required:** At least one visualization: `conf_matrix`, `pr_curve`, `pr_table`.<br>**Optional:** `probas_threshold` (default: None or 0.5 for probabilistic classification), `top_k`, [Test conditions](/docs/library/tests).             | **No reference:** Fails if Precision is lower than the dummy model.<br>**With reference:** Fails if Precision differs by >20%.                                                        |\n| **Recall()**                   | - Calculates recall.<br>- Visualizations available: Confusion Matrix, PR Curve, PR Table.<br>- **Metric result:** `value`.                                              | **Required:** At least one visualization: `conf_matrix`, `pr_curve`, `pr_table`.<br>**Optional:** `probas_threshold`, `top_k`, [Test conditions](/docs/library/tests).                                                            | **No reference:** Fails if lower than dummy model recall.<br>**With reference:** Fails if Recall differs by >20%.                                                                      |\n| **F1Score()**                  | - Calculates F1 Score.<br>- **Metric result:** `value`.                                                                                                                 | **Required:** At least one visualization: `conf_matrix`.<br>**Optional:** `probas_threshold`, `top_k`, [Test conditions](/docs/library/tests).                                                                                         | **No reference:** Fails if lower than dummy model F1.<br>**With reference:** Fails if F1 differs by >20%.                                                                             |\n| **TPR()**                      | - Calculates True Positive Rate (TPR).<br>- **Metric result:** `value`.                                                                                                  | **Required:** At least one visualization: `pr_table`.<br>**Optional:** `probas_threshold`, `top_k`, [Test conditions](/docs/library/tests).                                                                                           | **No reference:** Fails if TPR is lower than the dummy model.<br>**With reference:** Fails if TPR differs by >20%.                                                                    |\n| **TNR()**                      | - Calculates True Negative Rate (TNR).<br>- **Metric result:** `value`.                                                                                                  | **Required:** At least one visualization: `pr_table`.<br>**Optional:** `probas_threshold`, `top_k`, [Test conditions](/docs/library/tests).                                                                                           | **No reference:** Fails if TNR is lower than the dummy model.<br>**With reference:** Fails if TNR differs by >20%.                                                                    |\n| **FPR()**                      | - Calculates False Positive Rate (FPR).<br>- **Metric result:** `value`.                                                                                                  | **Required:** At least one visualization: `pr_table`.<br>**Optional:** `probas_threshold`, `top_k`, [Test conditions](/docs/library/tests).                                                                                           | **No reference:** Fails if FPR is higher than the dummy model.<br>**With reference:** Fails if FPR differs by >20%.                                                                    |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## TPR (True Positive Rate)\n- **Description**: Calculates True Positive Rate (TPR). \n- **Metric result**: `value`.\n- **Requirements**: \n  - **Required**: Set at least one visualization: `pr_table`.\n  - **Optional**: \n    - `probas_threshold`\n    - `top_k`\n    - [Test conditions](/docs/library/tests)\n- **Test Defaults**: \n  - **No reference**: Fails if TPR is lower than the dummy model.\n  - **With reference**: Fails if TPR differs by >20%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## TNR (True Negative Rate)\n- **Description**: Calculates True Negative Rate (TNR).\n- **Metric result**: `value`.\n- **Requirements**: \n  - **Required**: Set at least one visualization: `pr_table`.\n  - **Optional**: \n    - `probas_threshold`\n    - `top_k`\n    - [Test conditions](/docs/library/tests)\n- **Test Defaults**: \n  - **No reference**: Fails if TNR is lower than the dummy model.\n  - **With reference**: Fails if TNR differs by >20%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## FPR (False Positive Rate)\n- **Description**: Calculates False Positive Rate (FPR).\n- **Metric result**: `value`.\n- **Requirements**: \n  - **Required**: Set at least one visualization: `pr_table`.\n  - **Optional**: \n    - `probas_threshold`\n    - `top_k`\n    - [Test conditions](/docs/library/tests)\n- **Test Defaults**: \n  - **No reference**: Fails if FPR is higher than the dummy model.\n  - **With reference**: Fails if FPR differs by >20%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## FNR (False Negative Rate)\n- **Description**: Calculates False Negative Rate (FNR).\n- **Metric result**: `value`.\n- **Requirements**: \n  - **Required**: Set at least one visualization: `pr_table`.\n  - **Optional**: \n    - `probas_threshold`\n    - `top_k`\n    - [Test conditions](/docs/library/tests)\n- **Test Defaults**: \n  - **No reference**: Fails if FNR is higher than the dummy model.\n  - **With reference**: Fails if FNR differs by >20%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## LogLoss\n- **Description**: Calculates Log Loss.\n- **Metric result**: `value`.\n- **Requirements**: \n  - **Required**: Set at least one visualization: `pr_table`.\n  - **Optional**: \n    - `top_k`\n    - [Test conditions](/docs/library/tests)\n- **Test Defaults**: \n  - **No reference**: Fails if LogLoss is higher than the dummy model (equals 0.5 for a constant model).\n  - **With reference**: Fails if LogLoss differs by >20%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## RocAUC\n- **Description**: Calculates ROC AUC.\n- **Visualizations**: Can visualize PR curve or table.\n- **Metric result**: `value`.\n- **Requirements**: \n  - **Required**: Set at least one visualization: `pr_table`, `roc_curve`.\n  - **Optional**: \n    - `top_k`\n    - [Test conditions](/docs/library/tests)\n- **Test Defaults**: \n  - **No reference**: Fails if ROC AUC is â‰¤ 0.5.\n  - **With reference**: Fails if ROC AUC differs by >20%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Dummy Model Quality\n- **Description**: Use these metrics to get the quality of a dummy model created on the same data. \n- **Purpose**: Helps verify that the model quality is better than random. \n- **Metric**: \n  - **ClassificationDummyQuality**: Summarizes quality of a dummy model.\n  - **DummyPrecision**: Calculates precision for a dummy model.\n  - **DummyRecall**: Calculates recall for a dummy model.\n  - **DummyF1**: Calculates F1 Score for a dummy model."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Classification Quality By Label\n- **Description**: Evaluates the quality separately for multiple classes.\n- **Metrics**: \n  - **ClassificationQualityByLabel**: Summarizes classification quality metrics by label.\n  - **PrecisionByLabel**: Calculates precision by label in multiclass classification.\n  - **F1ByLabel**: Calculates F1 Score by label in multiclass classification.\n  - **RecallByLabel**: Calculates recall by label in multiclass classification.\n  - **RocAUCByLabel**: Calculates ROC AUC by label in multiclass classification."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Regression Metrics\n- **Purpose**: Evaluate the quality of a regression model.\n- **Info**: Data may need to be mapped to prediction and target columns.\n- **Metrics**: \n  - **RegressionPreset**: Includes a wide range of regression metrics.\n  - **RegressionQuality**: Summarizes key regression metrics.\n  - **MeanError**: Calculates the mean error.\n  - **MAE**: Calculates Mean Absolute Error (MAE).\n  - **RMSE**: Calculates Root Mean Square Error (RMSE)."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Mean Error (ME)\nCalculates the Mean Error (ME) with visualizations available for Error Plot, Error Distribution, and Error Normality. The metric result includes `mean` and `std`. \n\n**Required**: Set at least one visualization: `error_plot`, `error_distr`, `error_normality`.  \n**Optional**: Test conditions can be found [here](/docs/library/tests). Use `mean_tests` and `std_tests`. \n\n**No Reference/With Reference**: Expect ME to be near zero. Fails if Mean Error is skewed and condition is violated: `eq = approx(absolute=0.1 * error_std)`."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Mean Absolute Error (MAE)\nCalculates the Mean Absolute Error (MAE) with visualizations including Error Plot, Error Distribution, and Error Normality. The metric results are `mean` and `std`.\n\n**Required**: Set at least one visualization: `error_plot`, `error_distr`, `error_normality`.  \n**Optional**: Test conditions can be found [here](/docs/library/tests). Use `mean_tests` and `std_tests`.\n\n**No Reference**: Fails if MAE is higher than the dummy model predicting the median target value.  \n**With Reference**: Fails if MAE differs by >10%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Root Mean Square Error (RMSE)\nCalculates the Root Mean Square Error (RMSE) with the metric result being `value`.\n\n**Optional**: Test conditions can be found [here](/docs/library/tests).\n\n**No Reference**: Fails if RMSE is higher than the dummy model predicting the mean target value.  \n**With Reference**: Fails if RMSE differs by >10%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Mean Absolute Percentage Error (MAPE)\nCalculates the Mean Absolute Percentage Error (MAPE), offering visualizations such as the Percentage Error Plot. The metric results include `mean` and `std`.\n\n**Required**: Set at least one visualization: `perc_error_plot`.  \n**Optional**: Test conditions can be found [here](/docs/library/tests).\n\n**No Reference**: Fails if MAPE is higher than the dummy model predicting the weighted median target value.  \n**With Reference**: Fails if MAPE differs by >10%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## RÂ² (Coefficient of Determination)\nCalculates RÂ² (Coefficient of Determination) with the metric result being `value`.\n\n**Optional**: Test conditions can be found [here](/docs/library/tests).\n\n**No Reference**: Fails if RÂ² â‰¤ 0.  \n**With Reference**: Fails if RÂ² differs by >10%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Absolute Maximum Error\nCalculates the Absolute Maximum Error with the result being `value`.\n\n**Optional**: Test conditions can be found [here](/docs/library/tests).\n\n**No Reference**: Fails if absolute maximum error is higher than the dummy model predicting the median target value.  \n**With Reference**: Fails if it differs by >10%."
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Dummy Model Quality\nUse these metrics to assess the baseline quality for regression with optimal constants. Each metric serves as a baseline in automated testing.\n\n| Metric                       | Description                                                               | Parameters | Test Defaults |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "- |\n| **RegressionDummyQuality()** | Summarizes the quality of a dummy model.                                | N/A        | N/A           |\n| **DummyMeanError()**        | Calculates Mean Error for a dummy model.                                 | N/A        | N/A           |\n| **DummyMAE()**              | Calculates Mean Absolute Error (MAE) for a dummy model.                  | N/A        | N/A           |\n| **DummyMAPE()**             | Calculates Mean Absolute Percentage Error (MAPE) for a dummy model.      | N/A        | N/A           |\n| **DummyRMSE()**             | Calculates Root Mean Square Error (RMSE) for a dummy model.              | N/A        | N/A           |\n| **DummyR2()**               | Calculates RÂ² (Coefficient of Determination) for a dummy model.         | N/A        | N/A           |"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "## Ranking Metrics\nThese metrics are used to evaluate ranking, search/retrieval, or recommendations.\n\n**Note**: For data definitions and mapping of prediction and target columns, refer to [Data definition](/docs/library/data_definition). \n\n**For metric explainers**, see [here](/metrics/explainer_recsys).\n\n| Metric                              | Description                                                             | Parameters                                                                                                               | Test Defaults                                                                 |\n|"
  },
  {
    "title": "All Metrics",
    "description": "Reference page for all dataset-level evals.",
    "filename": "docs-main/metrics/all_metrics.mdx",
    "ai_section": "-|\n| **RecallTopK()**                    | Calculates Recall at the top K retrieved items.                        | **Required**: `k`; **Optional**: `no_feedback_users`, `min_rel_score`, [Test conditions](/docs/library/tests)         | **No reference**: Tests if recall > 0; **With reference**: Fails if Recall differs by >10%. |\n| **FBetaTopK()**                     | Calculates F-beta score at the top K retrieved items.                 | **Required**: `k`; **Optional**: `no_feedback_users`, `min_rel_score`, [Test conditions](/docs/library/tests)         | **No reference**: Tests if F-beta > 0; **With reference**: Fails if F-beta differs by >10%. |\n| **PrecisionTopK()**                 | Calculates Precision at the top K retrieved items.                    | **Required**: `k`; **Optional**: `no_feedback_users`, `min_rel_score`, [Test conditions](/docs/library/tests)         | **No reference**: Tests if Precision > 0; **With reference**: Fails if Precision differs by >10%. |\n| **MAP()**                           | Calculates Mean Average Precision at the top K retrieved items.       | **Required**: `k`; **Optional**: `no_feedback_users`, `min_rel_score`, [Test conditions](/docs/library/tests)         | **No reference**: Tests if MAP > 0; **With reference**: Fails if MAP differs by >10%. |\n| **NDCG()**                          | Calculates Normalized Discounted Cumulative Gain at the top K items. | **Required**: `k`; **Optional**: `no_feedback_users`, `min_rel_score`, [Test conditions](/docs/library/tests)         | **No reference**: Tests if NDCG > 0; **With reference**: Fails if NDCG differs by >10%. |\n| **MRR()**                           | Calculates Mean Reciprocal Rank at the top K retrieved items.         | **Required**: `k`; **Optional**: `no_feedback_users`, `min_rel_score`, [Test conditions](/docs/library/tests)         | **No reference**: Tests if MRR > 0; **With reference**: Fails if MRR differs by >10%. |\n| **HitRate()**                       | Calculates Hit Rate at the top K retrieved items.                     | **Required**: `k`; **Optional**: `no_feedback_users`, `min_rel_score`, [Test conditions](/docs/library/tests)         | **No reference**: Tests if Hit Rate > 0; **With reference**: Fails if Hit Rate differs by >10%. |\n| **ScoreDistribution()**             | Computes the predicted score entropy (KL divergence).                 | **Required**: `k`; **Optional**: [Test conditions](/docs/library/tests)                                             | **No reference**: `value`; **With reference**: `value`.                      |"
  },
  {
    "title": "Overview",
    "description": "All available Presets.",
    "filename": "docs-main/metrics/all_presets.mdx",
    "ai_section": "## Overview of Evaluation Templates\nThese are pre-built evaluation templates that are easy to run without setup. They are great for a start: you can create a custom setup later."
  },
  {
    "title": "Overview",
    "description": "All available Presets.",
    "filename": "docs-main/metrics/all_presets.mdx",
    "ai_section": "## Note on Presets\nNote that Presets apply on the **dataset level**. If you are looking at row-level evaluations (e.g. scoring relevance, correctness, etc. for LLM outputs and RAG), it's best to explore [built-in descriptors](/metrics/all_descriptors)."
  },
  {
    "title": "Overview",
    "description": "All available Presets.",
    "filename": "docs-main/metrics/all_presets.mdx",
    "ai_section": "## Text Evaluations\nThe \"Text Evals\" preset is designed for evaluating text and LLMs. It provides tools tailored specifically for assessing the quality of textual data."
  },
  {
    "title": "Overview",
    "description": "All available Presets.",
    "filename": "docs-main/metrics/all_presets.mdx",
    "ai_section": "## Data Drift Detection\nThe \"Data Drift\" preset focuses on detecting data distribution drift. This is crucial for maintaining the relevance and accuracy of models over time."
  },
  {
    "title": "Overview",
    "description": "All available Presets.",
    "filename": "docs-main/metrics/all_presets.mdx",
    "ai_section": "## Data Summary\nThe \"Data Summary\" preset offers a comprehensive overview and statistics of the dataset, providing insights into its composition and characteristics."
  },
  {
    "title": "Overview",
    "description": "All available Presets.",
    "filename": "docs-main/metrics/all_presets.mdx",
    "ai_section": "## Classification Evaluation\nThe \"Classification\" preset measures the quality of classification tasks. It helps in assessing how well a model performs in categorizing data accurately."
  },
  {
    "title": "Overview",
    "description": "All available Presets.",
    "filename": "docs-main/metrics/all_presets.mdx",
    "ai_section": "## Regression Evaluation\nThe \"Regression\" preset is tailored for evaluating the quality of regression tasks, ensuring that predictive models function effectively and accurately."
  },
  {
    "title": "Add text comments [UNPUBLISHED]",
    "description": "How to add text widgets to the Report.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_add_text.mdx",
    "ai_section": "## Pending Implementation for the New API\nThe implementation of the new API is currently pending, and there are no specific details available regarding the timeline or the expected features. Further updates will be provided as the development progresses."
  },
  {
    "title": "Change colors [UNPUBLISHED]",
    "description": "How to change color schema of the Report.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_colors.mdx",
    "ai_section": "## API Implementation Status\nPending implementation for the new API."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Data Drift Overview\nAll Metrics and Presets that evaluate shift in data distributions use the default [Data Drift algorithm](/metrics/explainer_drift). It automatically selects the drift detection method based on the column type (text, categorical, numerical) and volume. You can override the defaults by passing a custom parameter to the chosen Metric or Preset, and you can implement fully custom drift detection methods.\n\n**Pre-requisites**:\n* You know how to use [Data Definition](/docs/library/data_definition) to map column types.\n* You know how to create [Reports](/docs/library/report) and run [Tests](/docs/library/tests)."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Data Drift Parameters\nSetting conditions for data drift works differently from the usual Test API (with `gt`, `lt`, etc.). This accounts for nuances like the varying role of thresholds across drift detection methods, where \"greater\" can be better or worse depending on the method.\n\n### Dataset-level Parameters\n**Dataset drift share**: You can set the share of drifting columns that signals **dataset drift** (default: 0.5) in the relevant Metrics or Presets. \n\n**Drift methods**: Specify the drift detection methods on the column level. \n\n**Drift thresholds**: You can set thresholds for each method."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Column-level Metrics \nFor column-level metrics, you can set the drift method/threshold directly for each column. \n\n```python\nreport = Report([\n    ValueDrift(column=\"Salary\", method=\"psi\"),\n]\n```\n\n### All Parameters\nUse the following parameters to pass chosen drift methods:\n\n| Parameter              | Description                                                                                                                                                                                                                | Applies To                                  |\n|"
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "-- |\n| `method`               | Defines the drift detection method for a given column (if one column is tested) or for all columns in the dataset (if multiple columns are tested).                                            | `ValueDrift()`, `DriftedColumnsCount()`, `DataDriftPreset()` |\n| `threshold`            | Sets the drift threshold in a given column or all columns.                                                                                                                                                        | `ValueDrift()`, `DriftedColumnsCount()`, `DataDriftPreset()` |\n| `drift_share`         | Defines the share of drifting columns as a condition for Dataset Drift. Default: 0.5                                                                                                               | `DriftedColumnsCount()`, `DataDriftPreset()` |\n| `cat_method`          | Sets the drift method for all categorical columns.                                                                                                                                                                                       | `DriftedColumnsCount()`, `DataDriftPreset()` |\n| `num_method`          | Sets the drift method for all numerical columns.                                                                                                                                                                                         | `DriftedColumnsCount()`, `DataDriftPreset()` |\n| `per_column_method`    | Sets the drift method for the listed columns (accepts a dictionary).                                                                                                                                                        | `DriftedColumnsCount()`, `DataDriftPreset()` |\n| `text_method`          | Defines the drift detection method for all text columns.                                                                                                                                                                        | `DriftedColumnsCount()`, `DataDriftPreset()` |"
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Data Drift Detection Methods\n### Tabular Data\nThe following methods apply to **tabular** data: numerical or categorical columns. Pass them using the `stattest` (or `num_stattest`, etc.) parameter.\n\n| StatTest                                           | Applicable to                                                                                                       | Drift score                                                                                          |\n|"
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "|"
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "-|"
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "--|\n| `ks` - Kolmogorovâ€“Smirnov (K-S) test              | numerical data (default method for numerical data, if â‰¤ 1000 objects)                                           | drift detected when `p_value < threshold` (default threshold: 0.05)                               |\n| `chisquare` - Chi-Square test                     | categorical data (default method for categorical with > 2 labels, if â‰¤ 1000 objects)                             | drift detected when `p_value < threshold` (default threshold: 0.05)                               |\n| `z` - Z-test                                     | categorical data (default method for binary data, if â‰¤ 1000 objects)                                             | drift detected when `p_value < threshold` (default threshold: 0.05)                               |\n| `wasserstein` - Wasserstein distance (normed)     | numerical data (default method for numerical data, if > 1000 objects)                                            | drift detected when `distance` â‰¥ `threshold` (default threshold: 0.1)                             |\n| `kl_div` - Kullback-Leibler divergence            | numerical and categorical                                                                                          | drift detected when `divergence` â‰¥ `threshold` (default threshold: 0.1)                           |\n| `psi` - Population Stability Index (PSI)         | numerical and categorical                                                                                          | drift detected when `psi_value` â‰¥ `threshold` (default threshold: 0.1)                             |"
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Text Data Drift Detection\nText drift detection applies to columns with **raw text data**, as defined. Pass them using the `stattest` (or `text_stattest`) parameter."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Drift Detection for Tabular Data\nDrift detection in tabular data utilizes statistical tests to identify changes. The `TVD` (Total-Variation-Distance) metric is specifically designed for categorical data. Drift is detected when the `p_value` is less than the default threshold of 0.05."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Text Drift Detection Overview\nText drift detection applies to columns containing raw text data. It requires the use of specified parameters (`stattest` or `text_stattest`) to identify drift in text data. Two primary methods are available for assessing drift in text content."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Perc Text Content Drift\nThe `perc_text_content_drift` method is utilized for text data and trains a classifier to distinguish between text in \"current\" and \"reference\" datasets. It has a default for text datasets with 1000 or fewer objects. Drift is identified when the `roc_auc` score of the classifier exceeds a specified percentile threshold, set by default to 0.95 (95th percentile)."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Abs Text Content Drift\nThe `abs_text_content_drift` method is applicable when analyzing text data with more than 1000 objects. Similar to the previous method, it trains a classifier to distinguish between datasets. Drift is detected when the `roc_auc` exceeds the defined threshold, with a default value set at 0.55."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Checking for Text Descriptors Drift\nIf dealing with raw text data, it's advised to examine not only the text content but also the distribution of text descriptors (e.g., text length). First, compute the selected [text descriptors](/docs/library/descriptors), and then employ numerical or categorical drift detection methods as normal."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## Adding a Custom Drift Detection Method\nIf existing drift detection methods do not suit your needs, you can implement a custom function using Python. The example provided demonstrates registering a new StatTest with specific parameters and a custom statistical test function."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## StatTest Parameters\nTo create a custom StatTest, you need to define the following parameters: \n- `name`: A short identifier for the Stat Test.\n- `display_name`: A descriptive name shown in reports.\n- `func`: The function to perform the test.\n- `allowed_feature_types`: Valid data types for this function (e.g., categorical or numerical)."
  },
  {
    "title": "Customize Data Drift",
    "description": "How to change data drift detection methods and conditions.",
    "filename": "docs-main/metrics/customize_data_drift.mdx",
    "ai_section": "## StatTest Function Signature\nThe StatTest function must accept the following parameters and return specified results:\n- **Inputs**:\n  - `reference_data`: The data to compare against.\n  - `current_data`: The data being analyzed.\n  - `feature_type`: Type of data being analyzed.\n  - `threshold`: The drift detection threshold.\n- **Outputs**:\n  - `score`: The resulting Stat Test score.\n  - `drift_detected`: A boolean indicating if drift was detected based on the threshold."
  },
  {
    "title": "Custom Text Descriptor",
    "description": "How to add a custom row-level text evaluator.",
    "filename": "docs-main/metrics/customize_descriptor.mdx",
    "ai_section": "## Custom Functions in Evidently\nTo run checks not available in Evidently, you can implement them as custom functions. This allows you to build your own programmatic evaluators."
  },
  {
    "title": "Custom Text Descriptor",
    "description": "How to add a custom row-level text evaluator.",
    "filename": "docs-main/metrics/customize_descriptor.mdx",
    "ai_section": "## Customizing Existing Evaluators\nYou can customize existing evaluations with parameters, such as defining custom LLM judges or using regex-based metrics like `Contains` for word lists. For more details, see the available descriptors."
  },
  {
    "title": "Custom Text Descriptor",
    "description": "How to add a custom row-level text evaluator.",
    "filename": "docs-main/metrics/customize_descriptor.mdx",
    "ai_section": "## Pre-requisites\nBefore using custom functions in Evidently, ensure you have knowledge of using built-in descriptors."
  },
  {
    "title": "Custom Text Descriptor",
    "description": "How to add a custom row-level text evaluator.",
    "filename": "docs-main/metrics/customize_descriptor.mdx",
    "ai_section": "## Imports\nTo work with Evidently, ensure you have the following imports in your Python script:\n\n```python\nimport pandas as pd\n\nfrom evidently import Dataset, DataDefinition\nfrom evidently.core.datasets import DatasetColumn\nfrom evidently.descriptors import CustomColumnDescriptor, CustomDescriptor\n```"
  },
  {
    "title": "Custom Text Descriptor",
    "description": "How to add a custom row-level text evaluator.",
    "filename": "docs-main/metrics/customize_descriptor.mdx",
    "ai_section": "## Example: Generating Toy Data\nTo generate toy data and create a Dataset object, you can use the following code:\n\n```python\ndata = [\n    [\"Can fish fly?\", \"no\", \"\"],\n    [\"Is the sky blue?\", \"yes\", \"yes\"],\n    [\"Is milk liquid??\", \"yes\", \"yes\"]\n]\n\ncolumns = [\"question\", \"target_answer\", \"answer\"]\n\ndf = pd.DataFrame(data, columns=columns)\n\neval_df = Dataset.from_pandas(\n    df,\n    data_definition=DataDefinition())\n```"
  },
  {
    "title": "Custom Text Descriptor",
    "description": "How to add a custom row-level text evaluator.",
    "filename": "docs-main/metrics/customize_descriptor.mdx",
    "ai_section": "## Single Column Check\nYou can define a `CustomColumnDescriptor` to evaluate any column from your dataset. For example, to check if a column is empty:\n\n```python\ndef is_empty(data: DatasetColumn) -> DatasetColumn:\n    return DatasetColumn(\n        type=\"cat\",\n        data=pd.Series([\n            \"EMPTY\" if val == \"\" else \"NON EMPTY\"\n            for val in data.data]))\n```\n\nTo apply this descriptor:\n\n```python\neval_df.add_descriptors(descriptors=[\n    CustomColumnDescriptor(\"answer\", is_empty, alias=\"is_empty\"),\n])\n```\n\nPublish to a dataframe:\n\n```python\neval_df.as_dataframe()\n```"
  },
  {
    "title": "Custom Text Descriptor",
    "description": "How to add a custom row-level text evaluator.",
    "filename": "docs-main/metrics/customize_descriptor.mdx",
    "ai_section": "## Multi-Column Check\nYou can define a `CustomDescriptor` that takes one or multiple named columns and returns transformed columns. For example, to check for an exact match between `target_answer` and `answer`:\n\n```python\ndef exact_match(dataset: Dataset) -> DatasetColumn:\n    return DatasetColumn(\n        type=\"cat\",\n        data=pd.Series([\n            \"MATCH\" if val else \"MISMATCH\"\n            for val in dataset.column(\"target_answer\").data\n            == dataset.column(\"answer\").data]))\n```\n\nTo use this descriptor:\n\n```python\neval_df.add_descriptors(descriptors=[\n    CustomDescriptor(exact_match, alias=\"exact\"),\n])\n```"
  },
  {
    "title": "Custom Text Descriptor",
    "description": "How to add a custom row-level text evaluator.",
    "filename": "docs-main/metrics/customize_descriptor.mdx",
    "ai_section": "## Multiple Scores from a Descriptor\nYou can also use `CustomDescriptor` to evaluate multiple columns and return multiple scores. For example, to reverse words in the `question` and `answer` columns:\n\n```python\nfrom typing import Union, Dict\n\ndef reverse_text(dataset: Dataset) -> Union[DatasetColumn, Dict[str, DatasetColumn]]:\n    return {\n        \"reversed_question\": DatasetColumn(\n            type=\"cat\",\n            data=pd.Series([\n                value[::-1] for value in dataset.column(\"question\").data])),\n        \"reversed_answer\": DatasetColumn(\n            type=\"cat\",\n            data=pd.Series([\n                value[::-1] for value in dataset.column(\"answer\").data]))}\n```\n\nTo apply this descriptor:\n\n```python\neval_df.add_descriptors(descriptors=[\n    CustomDescriptor(reverse_text),\n])\n```"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Pending Implementation for the New API\nPending implementation for the new API."
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Embeddings Drift\n<Warning>\n  This method is **coming soon** to the new Evidently API! Check the old docs for now.\n</Warning>\n\nThe default embedding drift method is a **classifier**. Evidently trains a binary classification model to discriminate between data from reference and current distributions.\n\n* For **small data with \\<= 1000 observations**, drift is detected if the ROC AUC of the drift detection classifier > possible ROC AUC of the random classifier at a 95th percentile.\n* For **larger data with > 1000 observations**, drift is detected if the ROC AUC > 0.55.\n\nYou can choose other embedding drift detection methods, specify custom thresholds and parameters, and select from methods such as Euclidean distance, Cosine Similarity, Maximum Mean Discrepancy, and the share of drifted embeddings."
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Pre-requisites\n* You know how to generate Reports or Test Suites with default parameters.\n* You know how to pass custom parameters for Reports or Test Suites."
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Default Drift Detection Method\nWhen calculating embeddings drift, Evidently automatically applies the default drift detection method (â€œmodelâ€).\n\nIn Reports:\n\n```python\nreport = Report(metrics=[\n    EmbeddingsDriftMetric('small_subset')\n])\n```\n\nIn Test Suites:\n\n```python\ntests = TestSuite(tests=[\n    TestEmbeddingsDrift(embeddings_name='small_subset')\n])\n```\n\nThis works the same inside presets like `DataDriftPreset`."
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Customizing Drift Detection Method\nYou can override the defaults by passing a custom `drift_method` parameter to the relevant Metric or Test. You can define the embeddings drift detection method, the threshold, or both.\n\nPass the `drift_method` parameter:\n\n```python\nfrom evidently.metrics.data_drift.embedding_drift_methods import model\nreport = Report(metrics = [\n    EmbeddingsDriftMetric('small_subset', \n                          drift_method = model()\n                         )\n])\n```"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Using Presets for Embeddings\nWhen using `NoTargetPerformanceTestPreset`, `DataDriftTestPreset` or `DataDriftPreset`, specify which subsets of columns with embeddings to include using `embeddings`, and the drift detection method using `embeddings_drift_method`.\n\nTo exclude columns with embeddings:\n\n```python\nembeddings = []\n```\n\nTo include specific sets of columns:\n\n```python\nembeddings = [â€˜set1â€™, â€˜set2â€™]\n```\n\nTo specify both sets of columns and the method:\n\n```python\nembeddings = [â€˜set1â€™, â€˜set2â€™]\nembeddings_drift_method = {â€˜set1â€™: model(), â€˜set2â€™: ratio()}\n```"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Available Embedding Drift Detection Methods\nFour embeddings drift detection methods are currently available:\n\n| Method                              | Description                                                                                                                                                                                                                                                                                                                                                                                     |\n|"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "-- |"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "- |\n| `drift_method=model` (Default)     | A binary classifier model to distinguish between embeddings in â€œcurrentâ€ and â€œreferenceâ€ distributions. Returns **ROC AUC** as a `drift_score`. Drift detected when `drift_score` > `threshold` or when `drift_score` > ROC AUC of the random classifier at a set `quantile_probability`. Default threshold: 0.55, quantile_probability: 0.95 for < 1000 objects. |\n| `drift_method=ratio`               | Computes the distribution drift between individual embedding components using available tabular numerical drift detection methods. Default method: Wasserstein distance with a 0.1 threshold. Returns the **share of drifted embeddings** as `drift_score`. Drift detected when `drift_score` > threshold. Default threshold: 0.2.                                                        |\n| `drift_method=distance`            | Computes the distance between average embeddings in â€œcurrentâ€ and â€œreferenceâ€ datasets using a specified distance metric (default: `euclidean`). Returns the **distance metric value** as `drift_score`. Drift detected when drift_score > threshold. Default threshold: 0.2 and quantile_probability: 0.95 for < 1000 objects.                                                          |\n| `drift_method=mmd`                 | Computes the Maximum Mean Discrepancy (MMD). Returns the **MMD value** as a `drift_score`. Drift detected when `drift_score` > threshold. Default threshold: 0.015 and quantile_probability: 0.95 for < 1000 objects.                                                                                                                                                    |\n\nIf you specify a drift detection method without additional parameters, defaults will apply. You can also specify parameters for the chosen method."
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Model-Based Drift Detection Parameters\nWhen using the model-based drift detection method, you can specify parameters as follows:\n\n```python\nreport = Report(metrics = [\n    EmbeddingsDriftMetric('small_subset', \n                          drift_method = model(\n                              threshold = 0.55,\n                              bootstrap = None,\n                              quantile_probability = 0.05,\n                              pca_components = None\n                          )\n                         )\n])\n```\n\n| Parameter                         | Description                                                                                                                                                                     |\n|"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "|"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "-- |\n| `threshold`                       | Sets the threshold for drift detection (ROC AUC). Default: 0.55.                                                                                                           |\n| `bootstrap` (optional)            | Determines whether to apply statistical hypothesis testing. Default: True if â‰¤ 1000 objects; False if > 1000 objects.                                                        |\n| `quantile_probability` (optional) | Sets the percentile of the possible ROC AUC values of the random classifier. Default: 0.95.                                                                                   |\n| `pca_components` (optional)       | The number of PCA components for dimensionality reduction. Default: None.                                                                                                    |"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Maximum Mean Discrepancy (\"mmd\") Parameters\nYou can specify additional parameters for the MMD drift detection method:\n\n```python\nreport = Report(metrics = [\n    EmbeddingsDriftMetric('small_subset', \n                          drift_method = mmd(\n                              threshold = 0.015,\n                              bootstrap = None,\n                              quantile_probability = 0.05,\n                              pca_components = None\n                          )\n                         )\n])\n```"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Maximum Mean Discrepancy (â€œmmdâ€)\nThis section covers the Maximum Mean Discrepancy (MMD) metric used for detecting data drift.\n\n```python\nreport = Report(metrics = [\n    EmbeddingsDriftMetric('small_subset', \n                          drift_method = mmd(\n                              threshold = 0.015,\n                              bootstrap = None,\n                              quantile_probability = 0.05,\n                              pca_components = None,\n                          )\n                         )\n])\n```\n\n| Parameter                         | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n|"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "-- |\n| `threshold`                       | Sets the threshold for drift detection (ROC AUC). Drift is detected when `drift_score` > `threshold`. <br />Applies when `bootstrap` is not True.<br /><br />**Default: 0.55**.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `bootstrap` (optional)            | Boolean parameter (True/False) to determine whether to apply statistical hypothesis testing. <br />If applied, the ROC AUC of the classifier is compared to the ROC AUC of the random classifier at a set percentile. The calculation is repeated 1000 times with randomly assigned target class probabilities. This produces a distribution of random roc\\_auc scores with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, data drift is detected.<br /><br />**Default: True if less than or equal to 1000 objects, False if > 1000 objects.** |\n| `quantile_probability` (optional) | Sets the percentile of the possible ROC AUC values of the random classifier to compare against. <br />This applies when bootstrap is True.<br /><br />**Default: 0.95**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| `pca_components` (optional)       | The number of PCA components. If specified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`.<br /><br />**Default: None.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Share of Drifted Embedding Components (â€œratioâ€)\nThis section discusses the metrics related to the share of drifted embedding components.\n\n```python\nreport = Report(metrics = [\n    EmbeddingsDriftMetric('small_subset', \n                          drift_method = ratio(\n                              component_stattest = 'wasserstein',\n                              component_stattest_threshold = 0.1,\n                              threshold = 0.2,\n                              pca_components = None,\n                          )\n                         )\n])\n```\n\n| Parameter                                 | Description                                                                                                                                                                                                                                                                                                                                                                                                  |\n|"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "|\n| `component_stattest` (optional)           | Sets the tabular drift detection method (any of the tabular drift detection methods for numerical features available in Evidently).<br /><br />**Default: Wasserstein**                                                                                                                                                                                                                                    |\n| `component_stattest_threshold` (optional) | Sets the threshold for drift detection for individual embedding components. Drift is detected when `drift_score` > `component_stattest_threshold` in case of distance/divergence metrics where the threshold is the metric value or `drift_score` < `component_stattest_threshold` in case of statistical tests where the threshold is the p-value.<br /><br />**Default: 0.1** (relevant for Wasserstein). |\n| `threshold` (optional)                    | Sets the threshold (share of drifted embedding components) for drift detection for the overall dataset. <br /><br />**Default: 0.2**                                                                                                                                                                                                                                                                         |\n| `pca_components` (optional)               | The number of PCA components. If specified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`. <br /><br />**Default: None**.                                                                                                                                                                                                          |"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "## Distance-Based Methods (â€œdistanceâ€)\nThis section provides details on distance-based methods for detecting data drift.\n\n```python\nreport = Report(metrics = [\n    EmbeddingsDriftMetric('small_subset', \n                          drift_method = distance(\n                              dist = 'euclidean', #\"euclidean\", \"cosine\", \"cityblock\" or \"chebyshev\"\n                              threshold = 0.2,\n                              pca_components = None,\n                              bootstrap = None,\n                              quantile_probability = 0.05\n                          )\n                         )\n])\n```\n\n| Parameter | Description |\n|"
  },
  {
    "title": "Customize Embedding Drift [Unpublished]",
    "description": "How to set embedding drift detection conditions.",
    "noindex": "true",
    "filename": "docs-main/metrics/customize_embedding_drift.mdx",
    "ai_section": "|\n| `dist` (optional) <br /><br />Available: <br />`euclidean` <br />`cosine`<br />`cityblock` (manhattan distance)<br />`chebyshev` | Sets the distance metric for drift detection. <br /><br />**Default: Euclidean distance** |\n| `threshold` (optional)  | Sets the threshold for drift detection. Drift is detected when `drift_score` > `threshold`.<br />Applies when bootstrap is not True<br /><br />**Default: 0.2** (relevant for euclidean distance) |\n| `bootstrap` (optional)  | Boolean parameter (True/False) to determine whether to apply statistical hypothesis testing. <br /><br />If applied, the distance between reference and current is tested against possible distance values in reference. We randomly split the reference data into two parts and compute the distance between them. The calculation is repeated 100 times. This produces a distribution of distance values obtained for a reference dataset. We then take the 95th percentile (default) of this distribution and compare it to the distance between reference and current datasets. If the distance between the reference and current is higher than the 95th percentile of the distance obtained for the reference dataset, the drift is detected. <br /><br />**Default: True if less than or equal to 1000 objects, False if > 1000 objects**. |\n| `quantile_probability` (optional)  | Sets the percentile of the possible distance values in reference to compare against.<br />Applies when `bootstrap` is True.<br /><br />**Default: 0.95**. |\n| `pca_components` (optional)  | The number of PCA components. If specified, dimensionality reduction will be applied to project data to n-dimensional space based on the number of `pca_components`. <br /><br />**Default: None**. |"
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "## Scoring Text with HuggingFace Models\nYou can score your text by downloading and using ML models from HuggingFace. This allows you to apply criteria from the source model, such as classifying texts by emotion. There are two options available:\n\n* Ready-to-use descriptors that wrap a specific model.\n* A general interface to call other suitable models that you select.\n\n**Pre-requisites**:\n* You should know how to use [descriptors](/docs/library/descriptors) to evaluate text data."
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "## Imports Required for Evaluation\nTo evaluate text using HuggingFace models, you need to import the necessary libraries. Use the following import statements:\n\n```python\nfrom evidently.descriptors import HuggingFace, HuggingFaceToxicity\n```"
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "## Generating Toy Data\nBefore running examples, you need to generate toy data and create a Dataset object. Here's how you can do it:\n\n```python\nimport pandas as pd\nfrom evidently import Dataset\nfrom evidently import DataDefinition\n\ndata = [\n    [\"Why is the sky blue?\", \n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\", \n     \"because air scatters blue light more\"],\n    [\"How do airplanes stay in the air?\", \n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \n     \"because wings create lift\"],\n    // Additional data entries...\n]\n\ncolumns = [\"question\", \"context\", \"response\"]\n\ndf = pd.DataFrame(data, columns=columns)\n\neval_df = Dataset.from_pandas(\n    df,\n    data_definition=DataDefinition())\n```"
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "## Built-in ML Evaluators\nThere are built-in evaluators for some models that you can use similar to other descriptors. For example, to evaluate toxicity, use:\n\n```python\neval_df.add_descriptors(descriptors=[\n    HuggingFaceToxicity(\"question\", toxic_label=\"hate\", alias=\"Toxicity\") \n])\n```\n\n**Tip**: Check all available built-in LLM evaluations in the [reference table](/metrics/all_descriptors#ml-based-evals)."
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "## Custom ML Evaluators\nYou can also add custom checks directly as a Python function. Alternatively, you can use the general `HuggingFace()` descriptor to invoke a specific named model. The model must return a numerical score or a category for each text in a specified column.\n\nFor example, to evaluate \"curiosity\" expressed in a text:\n\n```python\neval_df.add_descriptors(descriptors=[\n   HuggingFace(\"question\",\n       model=\"SamLowe/roberta-base-go_emotions\", \n       params={\"label\": \"curiosity\"},\n       alias=\"Curiousity\"\n   )\n])\n```\n\nYou can then call the results as usual:\n\n```python\neval_df.as_dataframe()\n```"
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "## Sample Models Using HuggingFace Descriptor\nHere are some models you can call using the `HuggingFace()` descriptor:\n\n### Emotion Classification\n- **Description**: Scores texts by 28 emotions and returns predicted probabilities.\n- **Example Use**: \n```python\nHuggingFace(\"response\", model=\"SamLowe/roberta-base-go_emotions\", params={\"label\": \"disappointment\"}, alias=\"disappointment\")\n```\n- **Parameters**: \n    - Required: `params={\"label\":\"label\"}`\n    - Available labels include admiration, joy, sadness, etc."
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "### Zero-shot Classification\n- **Description**: Natural language inference model for user-provided topics.\n- **Example Use**: \n```python\nHuggingFace(\"response\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\", params={\"labels\": [\"science\", \"physics\"], \"threshold\": 0.5}, alias=\"Topic\")\n```\n- **Parameters**: \n    - Required: `params={\"labels\": [\"label\"]}`\n    - Optional: `params={\"score_threshold\": 0.7}`"
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "### GPT-2 Text Detection\n- **Description**: Predicts if a text is real or fake (generated by a GPT-2 model).\n- **Example Use**: \n```python\nHuggingFace(\"response\", model=\"openai-community/roberta-base-openai-detector\", params={\"score_threshold\": 0.7}, alias=\"fake\")\n```\n- **Parameters**: \n    - Optional: `params={\"score_threshold\": 0.7}`"
  },
  {
    "title": "Use HuggingFace models",
    "description": "How to use models from HuggingFace as evaluators.",
    "filename": "docs-main/metrics/customize_hf_descriptor.mdx",
    "ai_section": "## Notes on Model Compatibility\nThe provided list of models may not be exhaustive, and the Descriptor supports other models published on Hugging Face. Ensure that:\n\n* The model outputs a single number or label.\n* It can process raw text input directly.\n* It adheres to naming conventions for labels.\n\nIf you discover useful models, consider sharing them with the community or request support for specific models on platforms like GitHub."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Introduction to LLM-based Descriptors\nLLM-based descriptors use an external LLM for evaluation. You can utilize built-in evaluators with pre-written prompts or configure custom criteria using templates. \n\n**Pre-requisites**: You should know how to use [descriptors](/docs/library/descriptors) to evaluate text data."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Imports\nTo implement the LLM-based descriptors, you will need to import the following libraries:\n\n```python\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate, MulticlassClassificationPromptTemplate \nfrom evidently.descriptors import LLMEval, ToxicityLLMEval, ContextQualityLLMEval, DeclineLLMEval\n```"
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Generating Toy Data\nTo generate toy data and create a Dataset object, use the following code:\n\n```python\nimport pandas as pd\nfrom evidently import DataDefinition\n  \ndata = [\n    [\"Why is the sky blue?\", \n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\", \n     \"because air scatters blue light more\"],\n    [\"How do airplanes stay in the air?\", \n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \n     \"because wings create lift\"],\n    [\"Why do we have seasons?\", \n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \n     \"because Earth is tilted\"],\n    [\"How do magnets work?\", \n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \n     \"because of magnetic fields\"],\n    [\"Why does the moon change shape?\", \n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \n     \"because it rotates\"],\n    [\"What movie should I watch tonight?\", \n     \"A movie is a motion picture created to entertain, educate, or inform viewers through a combination of storytelling, visuals, and sound.\", \n     \"watch a movie that suits your mood\"]\n]\n\ncolumns = [\"question\", \"context\", \"response\"]\n\ndf = pd.DataFrame(data, columns=columns)\n\neval_df = Dataset.from_pandas(\n  df,\n  data_definition=DataDefinition())\n```"
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Built-in LLM Judges\n### Available Descriptors\nEvidently provides built-in evaluators for popular criteria, including detecting toxicity or refusals. These built-in descriptors:\n\n- Default to binary classifiers.\n- Use the `gpt-4o-mini` model from OpenAI by default.\n- Return a label, reasoning for the decision, and an optional score.\n\n**OpenAI Key**: Add the token as an environment variable: [see docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).\n\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] \n```\n\n**Single-Column Evaluation**: To evaluate whether `response` contains any toxicity:\n\n```python\neval_df.add_descriptors(descriptors=[\n    ToxicityLLMEval(\"response\", alias=\"toxicity\"),\n])\n```\n\n**Multi-Column Evaluation**: To check context quality, you must pass the `question` column parameter alongside `context`:\n\n```python\neval_df.add_descriptors(descriptors=[\n    ContextQualityLLMEval(\"context\", alias=\"good_context\", question=\"question\"),\n])\n```"
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Changing the Evaluator LLM\nWhile OpenAI is the default evaluation provider in Evidently, you can switch to other models such as those from Anthropic, Gemini, or Mistral.\n\n### Using Parameters\nTo change the model or provider, pass the necessary parameters to the LLM-based descriptor.\n\n**Change the Model**:\n\n```python\neval_df.add_descriptors(descriptors=[\n    DeclineLLMEval(\"response\", alias=\"Decline by Turbo\", provider=\"openai\", model=\"gpt-3.5-turbo\"),\n])\n```\n\n**Change the Provider**: Import the API key as an environment variable before using it.\n\n```python\nimport os\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\n```\n\nThen, specify the provider and model:\n\n```python\neval_df.add_descriptors(descriptors=[\n    DeclineLLMEval(\"response\", alias=\"Decline by Claude\", provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\n])\n```\n\n### Using Options\nFor some providers, you can directly pass API keys and parameters using Options.\n\n```python\nfrom evidently.utils.llm.wrapper import AnthropicOptions\n\nllm_options_evals = Dataset.from_pandas(\n    pd.DataFrame(data),\n    data_definition=data_definition,\n    descriptors=[\n        NegativityLLMEval(\"Answer\", provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),],\n    options=AnthropicOptions(api_key=\"YOUR_KEY_HERE\", rpm_limit=50))\n```"
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Creating a Custom LLM Judge\nYou can create a custom LLM evaluator using the provided templates by choosing a grading rubric and evaluating criteria.\n\n### Binary Classifier\n#### Single Column Example\nTo evaluate text as \"concise\" or \"verbose\":\n\n```python\nconciseness = BinaryClassificationPromptTemplate(\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\n            A CONCISE response should:\n            - Provide the necessary information without extra details or repetition.\n            - Be brief yet comprehensive enough to address the query.\n            - Use simple and direct language to convey the message effectively.\n        \"\"\",\n        target_category=\"CONCISE\",\n        non_target_category=\"VERBOSE\",\n        uncertainty=\"unknown\",\n        include_reasoning=True,\n        pre_messages=[(\"system\", \"You are a judge which evaluates text.\")],\n        )      \n```\n\n**Apply this descriptor**:\n\n```python\neval_df.add_descriptors(descriptors=[\n    LLMEval(\"response\", \n            template=conciseness, \n            provider = \"openai\", \n            model = \"gpt-4o-mini\", \n            alias=\"Conciseness\"),\n])\n```\n\n### Multiple Columns Example\nTo evaluate response correctness in relation to context:\n\n```python\nhallucination = BinaryClassificationPromptTemplate(\n        pre_messages=[(\"system\", \"You are a judge which evaluates correctness of responses by comparing them to the trusted information source.\")],\n        criteria = \"\"\"An HALLUCINATED response is any response that\n        - Contradicts the information provided in the source.\n        - Adds any new information not provided in the source.\n        - Gives a response not based on the source, unless it's a refusal or a clarifying question.\n\n        A FAITHFUL response is the response that\n        - Correctly uses the information from the source, even if it only partially.\n        - A response that declines to answer.\n        - A response that asks a clarifying question.\n\n        Source:\n        =====\n        {context}\n        =====\n        \"\"\",\n        target_category=\"HALLUCINATED\",\n        non_target_category=\"FAITHFUL\",\n        uncertainty=\"unknown\",\n        include_reasoning=True,\n        )\n```\n\n**Apply the descriptor** with the `additional_columns` parameter:\n\n```python\neval_df.add_descriptors(descriptors=[\n    LLMEval(\"response\", \n            template=hallucination, \n            provider = \"openai\", \n            model = \"gpt-4o-mini\", \n            alias=\"hallucination\", \n            additional_columns={\"context_column\": \"context\"}),\n])\n```"
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Conclusion\nThese sections cover the foundational elements needed for implementing LLM-based descriptors in evaluations, from setup to custom LLM judges. For further resource guidance, refer to respective templates and examples."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Evaluation of Responses with LLMs\nThis section discusses how to evaluate responses using LLMs by applying specific templates with additional parameters such as context columns and descriptors."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Using Additional Columns in Evaluation\nWhen applying descriptors for evaluations, it is essential to include additional columns that can be referenced in the evaluation prompts. This allows for more tailored evaluations."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Multi-class Classifier Overview\nDetails about utilizing a multi-class template for evaluations, including how to define grading rubrics with multiple categories for assessing responses adequately."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Defining Multi-class Templates\nThis section explains how to create a multi-class evaluation template for assessing responses against brand policies, demonstrating the setup of category criteria."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Implementation Example for Multi-class Evaluations\nAn example code snippet is provided to illustrate how to implement a multi-class evaluation template in Python, emphasizing the setup of descriptors, data definitions, and evaluation prompts."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Evaluating Relevance of Answers\nGuidance on how to implement an evaluation template that assesses the relevance of answers in response to questions. The criteria for classification are detailed here."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Code Example for Relevance Evaluation\nThis section includes a code snippet showcasing how to implement a relevance evaluation template, highlighting the necessary parameters and descriptors for effective classification."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## LLMEval Parameters Explained\nAn overview of the parameters associated with the `LLMEval` function, detailing each parameter's purpose and how they are used in evaluation processes."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## BinaryClassificationPromptTemplate Parameters\nDetails about the parameters of the `BinaryClassificationPromptTemplate`, including functionality and customization options available for determining response alignment."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## MulticlassClassificationPromptTemplate Parameters\nInformation on the parameters of the `MulticlassClassificationPromptTemplate`, explaining the structure and requirements for setting up a multi-class evaluation."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Descriptor Parameters Overview\nThe `descriptor_parameters` section outlines the key parameters you can define for the LLM evaluation prompt. These include the text of the evaluation prompt, a placeholder for the evaluated text, and the type of descriptor the prompt will return (either numerical or categorical)."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Category Criteria\n`category_criteria` is a dictionary that requires a custom category list to be defined. This is essential for the proper classification of the evaluated text."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Uncertainty Handling\nThe parameter `uncertainty` allows you to specify a category to return when the provided information is insufficient for a clear determination. The default value for this setting is `unknown`."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Including Reasoning\nThe `include_reasoning` parameter specifies whether to include an LLM-generated explanation of the result. By default, this is set to `True`, but you can change it to `False` if reasoning is not needed."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Pre-Messages\n`pre_messages` allows you to set a list of system messages that provide context or instructions prior to the evaluation task. This is an optional parameter that can be customized."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## OpenAIPrompting Descriptor\nThe OpenAIPrompting Descriptor is an implementation for creating prompts. To import this descriptor, you use the following code:\n```python\nfrom evidently.descriptors import OpenAIPrompting\n```"
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Defining a Prompt\nTo define a prompt for identifying personally identifiable information (PII), you can use a specific template, where `REPLACE` serves as a placeholder for the text to be evaluated. An example prompt is provided in the documentation."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Scoring with OpenAIPrompting\nTo compute scores from a dataset using OpenAIPrompting, the `Dataset.from_pandas` function is employed. The DataFrame containing your data must be structured correctly with descriptors like OpenAI included for scoring."
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Viewing Results\nOnce the scoring is complete, you can view the results in a DataFrame format by using the command:\n```\nopenai_prompting.as_dataframe()\n```"
  },
  {
    "title": "Configure LLM Judges",
    "description": "How to run prompt-based evaluators for custom criteria.",
    "filename": "docs-main/metrics/customize_llm_judge.mdx",
    "ai_section": "## Detailed Descriptor Parameters\nThis section covers the detailed parameters for the descriptor, including the evaluation prompt text, placeholder strings, type of descriptor, context handling, and options for validating LLM output. It also explains the naming convention for display names in reports and other customizable parameters."
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Custom Metrics and Tests\nYou can build fully custom Metrics/Tests to handle any column- or dataset-level evaluations. This allows you to implement business metrics, weighted scores, etc."
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Customization Options\nThere are ways to customize your evaluations without creating Metrics from scratch, including:\n- Adding a [custom text descriptor](/metrics/customize_descriptor) for row-level evaluations.\n- Using a built-in template to create a custom [LLM-based evaluator](/metrics/customize_llm_judge).\n- Implementing a [custom data drift](/metrics/customize_data_drift) detection method by reusing existing renders."
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Creating a Custom Metric\nCreating a custom Metric involves several steps:\n1. **Implementing the Metric calculation method** (Required).\n2. **Defining the default Test conditions** (Optional) that apply when running Tests for the Metric.\n3. **Creating a custom visualization** (Optional) using Plotly. If omitted, the Metric will appear as a simple counter in the Report.\n\nAfter implementing the Metric, it can be included in Reports, viewed in the Evidently Cloud (or a self-hosted UI), and visualized over time on the Dashboard."
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Example Implementation Overview\nThis section provides an example of creating a custom Metric called `MyMaxMetric`, which calculates the maximum value in a column. This requires familiarity with the codebase and Plotly for visualization."
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Code Imports\nTo implement `MyMaxMetric`, the following imports are needed:\n```python\nimport pandas as pd\nimport numpy as np\nfrom evidently import Report\nfrom evidently import Dataset\nfrom evidently import DataDefinition\nfrom evidently.core.report import Context\nfrom evidently.core.metric_types import SingleValue\nfrom evidently.core.metric_types import SingleValueMetric\nfrom evidently.core.metric_types import SingleValueCalculation\nfrom evidently.core.metric_types import BoundTest\nfrom evidently.tests import Reference, eq\n\nfrom evidently.legacy.renderers.html_widgets import plotly_figure\n\nfrom typing import Optional\nfrom typing import List\nfrom plotly.express import line\n```"
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Implementation of MyMaxMetric\nThe implementation of `MyMaxMetric` includes defining default tests and the calculation logic:\n```python\nclass MyMaxMetric(SingleValueMetric):\n    column: str\n\n    def _default_tests(self) -> List[BoundTest]:\n        return [eq(0).bind_single(self.get_fingerprint())]\n\n    def _default_tests_with_reference(self) -> List[BoundTest]:\n        return [eq(Reference(relative=0.1)).bind_single(self.get_fingerprint())]\n```\n\nThe calculation method is defined as:\n```python\nclass MaxMetricImplementation(SingleValueCalculation[MyMaxMetric]):\n    def calculate(self, context: Context, current_data: Dataset, reference_data: Optional[Dataset]) -> SingleValue:\n        x = current_data.column(self.metric.column).data\n        value = x.max()\n        result = self.result(value=value)\n        figure = line(x)\n        figure.add_hrect(6, 10)\n        result.widget = [plotly_figure(title=self.display_name(), figure=figure)]\n        return result\n\n    def display_name(self) -> str:\n        return f\"Max value for {self.metric.column}\"\n```"
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Default Test Behavior\nThe default Test checks whether the maximum value is 0 (or within Â±10% of the reference value). This functionality is applied if you run Tests without specifying a custom threshold."
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Example Usage of MyMaxMetric\nOnce implemented, `MyMaxMetric` can be referenced in a Report. An example toy dataset can be created as follows:\n```python\ndata = {\n    \"Item\": [f\"Item_{i}\" for i in range(1, 11)],\n    \"Quantity\": np.random.randint(1, 50, size=10),\n    \"Sales\": np.random.uniform(100, 5000, size=10).round(2),\n}\n\ndf = pd.DataFrame(data)\n\ndataset = Dataset.from_pandas(\n    pd.DataFrame(df),\n    data_definition=DataDefinition()\n)\n```\n\nTo add `MyMaxMetric` to the Report:\n```python\nreport = Report([\n    MyMaxMetric(column=\"Sales\")\n])\nmy_eval = report.run(dataset, None)\nmy_eval\n```"
  },
  {
    "title": "Custom Metric",
    "description": "How to create a custom dataset or column-level Metric.",
    "filename": "docs-main/metrics/customize_metric.mdx",
    "ai_section": "## Feature Requests\nIf you would like a Metric added to the core library, feel free to share your idea or feature request by [opening a GitHub issue](https://github.com/evidentlyai/evidently/issues)."
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Model Quality Summary Metrics\nEvidently calculates a few standard model quality metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and LogLoss. To support the model performance analysis, Evidently also generates interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas."
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Class Representation\nThis section shows the number of objects of each class, providing insight into the distribution of target classes in the dataset.\n\n![Class Representation](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-014d6141c52c22ea8c3367805211fd8d40e31849%252Fprob_class_perf_class_representation.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=8d30dd06&sv=2)"
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Confusion Matrix\nThe confusion matrix visualizes the classification errors and their types, helping to identify where the model fails.\n\n![Confusion Matrix](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-d44aceed79a4f98cde410058c12367a63037f2ce%252Fprob_class_perf_confusion_matrix.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cfaadbad&sv=2)"
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Quality Metrics by Class\nThis section shows model quality metrics for individual classes. In multi-class problems, it will also include the ROC AUC metric.\n\n![Quality Metrics by Class](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-54512715bad59a70038f7c168822fd087f1d2719%252Fprob_class_perf_quality_by_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=4606ec91&sv=2)"
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Class Separation Quality\nThis section features a scatter plot of the predicted probabilities, showing correct and incorrect predictions for each class. It helps visually choose the best probability threshold for each class.\n\n![Class Separation Quality](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-e16ca5618f050c77396f80958fecebf330c24c72%252Fprob_class_perf_class_separation_quality.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=b3b31454&sv=2)"
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Probability Distribution\nThis visualization shows the distribution of predicted probabilities, providing insight into the prediction confidence levels of the model.\n\n![Probability Distribution](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-41006102d2e027f1b046d083bfd9fe097dee5563%252Fprob_class_perf_probability_distr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=c65c377c&sv=2)"
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## ROC Curve\nThe ROC Curve (receiver operating characteristic curve) shows the share of true positives and true negatives at different classification thresholds.\n\n![ROC Curve](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-756dcacc9d462ad45745c6bb8f6dd23c5a18d9d2%252Fprob_class_perf_roc.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=caee6451&sv=2)"
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Precision-Recall Curve\nThe precision-recall curve illustrates the trade-off between precision and recall for different classification thresholds.\n\n![Precision-Recall Curve](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-f558cc77f29cde0d60ffddc81a622232ca022c41%252Fprob_class_perf_pr.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=7f49bf96&sv=2)"
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Precision-Recall Table\nThis table shows possible outcomes for different classification thresholds and prediction coverage. If you have two datasets, the table will be generated for both.\n\n![Precision-Recall Table](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-23088b547ed1b7126b1584e67acd2bf42715582b%252Fprob_class_perf_pr_table_current.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=2b13494f&sv=2)\n\nEach line in the table defines a case when only *top-X%* predictions are considered. It shows the absolute number of predictions (Count) and the probability threshold (Prob) for that combination. The table then shows quality metrics, including Precision, Recall, and shares of True Positives (TP) and False Positives (FP)."
  },
  {
    "title": "Classification metrics",
    "description": "Open-source classification metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_classification.mdx",
    "ai_section": "## Classification Quality by Feature\nIn this table, a number of plots for each feature are displayed. You can expand the plots by clicking on the feature name.\n\n![Classification Quality by Feature](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-89edd11c3d8fe70b6e8d65cb7e37ca4f246b067f%252Fprob_class_perf_classification_quality_by_feature.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1155f76a&sv=2)\n\nIn the \"ALL\" tab, you can see the distribution of classes against the values of the feature, allowing for visual comparisons of datasets. For each class, you can observe predicted probabilities alongside feature values, helping to visualize error types and reveal low-performance segments.\n\n![Classification Quality by Feature Example](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-95a87ddd8e3296f900797f822e06b5c623de08a4%252Fprob_class_perf_classification_quality_by_feature_example_class.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=761fd6cb&sv=2)"
  },
  {
    "title": "Data stats and quality",
    "description": "Description of your new file.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_data_stats.mdx",
    "ai_section": "## Summary Widget\nThe summary widget provides an overview of the dataset, highlighting missing or empty features along with general information. It specifically indicates the proportion of features that are nearly empty or constant, applying to cases where 95% or more of the features are missing or constant.\n\n![Summary Widget](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-893e96b92c369e8ad43eddc86aba54f79ce46b27%252Freports_data_quality_summary.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=2d5a9008&sv=2)"
  },
  {
    "title": "Data stats and quality",
    "description": "Description of your new file.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_data_stats.mdx",
    "ai_section": "## Features Widget Overview\nThe features widget generates visualizations for each feature based on its type, consisting of three main components:\n\n### Feature Overview Table\nThis table presents relevant statistical summaries and visualizations of feature distributions for each feature type.\n\n**Examples:**\n- Categorical Feature:\n![Categorical Feature](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-31dbb98333b62c917c7b2cbba158445774508baa%252Freports_data_quality_overview_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=516d34b9&sv=2)\n\n- Numerical Feature:\n![Numerical Feature](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-c7affc8d3dd2d0bee0b70eafdeb5867e5c52b30e%252Freports_data_quality_overview_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cd19013a&sv=2)\n\n- Datetime Feature:\n![Datetime Feature](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-ebcc131d1b824e4754edd777804343a4ea5dfdb1%252Freports_data_quality_overview_datetime.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=a31fdfd&sv=2)\n\n- Text Feature:\n![Text Feature](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-7074e49da73257ed1bd5ef7b675b8f63836ed15d%252Fmetric_column_summary_text-min.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=767b2d86&sv=2)\n\n### Feature in Time\nClicking \"details\" for each feature reveals visualizations to observe feature behavior over time.\n\n- Categorical Example:\n![Categorical Over Time](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-71b9ba91a04fa177dffbbb2b25984be292f15283%252Freports_data_quality_in_time_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=98c89f23&sv=2)\n\n- Numerical Example:\n![Numerical Over Time](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-a06be74e97d4d2fae56e8686520d0059161f36eb%252Freports_data_quality_in_time_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=cdb43752&sv=2)\n\n- Datetime Example:\n![Datetime Over Time](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-9b4017bb0fd6a38fae0b868c2cf016a2ec8e2250%252Freports_data_quality_in_time_datetime.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=fd11ef73&sv=2)\n\n### Feature by Target\nCategorical and numerical features also include visualizations that show interaction between a feature and a target.\n\n- Categorical Feature Example:\n![Categorical by Target](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-70efaf4625facf8c22e1821af9494b070322ca60%252Freports_data_quality_by_target_cat.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=92c8f6d4&sv=2)\n\n- Numerical Feature Example:\n![Numerical by Target](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-dae7161fc71ab08b8569678ce02dc8d58420f7c9%252Freports_data_quality_by_target_num.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1b7fde00&sv=2)"
  },
  {
    "title": "Data stats and quality",
    "description": "Description of your new file.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_data_stats.mdx",
    "ai_section": "## Correlation Widget\nThe correlation widget displays the relationships between different features.\n\n### Insights\nThis section presents a summary of pairwise feature correlations. It lists the top-5 highly correlated variables from Cramer's v correlation matrix for categorical features and from Spearman correlation matrix for numerical features. For comparisons between two datasets, it showcases the top-5 pairs of variables with the most significant correlation changes.\n\n![Correlation Insights](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-cb472b98063f4e2f57029b8ea4e450d7c8a453c7%252Freports_data_quality_correlations.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=99257d7b&sv=2)\n\n### Correlation Heatmaps\n(The correlation heatmaps widget has been removed in versions above 0.4.31. You can still add it to your report as `DatasetCorrelationsMetric()`. This section consists of heatmaps for categorical and numerical features.)\n\nFor categorical features, Cramer's v correlation matrix is generated, while Pearson, Spearman, and Kendall matrices are used for numerical features. If the dataset includes the target, it is also included in the correlation matrix according to its type.\n\n![Correlation Heatmaps](https://docs.evidentlyai.com/~gitbook/image?url=https%3A%2F%2F256125905-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FeE67gM4508ESQxkbpOxj%252Fuploads%252Fgit-blob-b8e9570d01a6413c4c8eeabd91252bf263516e21%252Freports_data_quality_correlation_heatmaps.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=5565f244&sv=2)"
  },
  {
    "title": "Data drift",
    "description": "How data drift detection works",
    "filename": "docs-main/metrics/explainer_drift.mdx",
    "ai_section": "## Default Data Drift Detection Algorithm\nIn some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>"
  },
  {
    "title": "Data drift",
    "description": "How data drift detection works",
    "filename": "docs-main/metrics/explainer_drift.mdx",
    "ai_section": "## How It Works\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets, which should be passed as **reference** and **current**. It applies several statistical tests and drift detection methods to determine if the distribution has changed significantly, returning a \"drift detected\" or \"not detected\" result. The choice of drift test is based on:\n\n* Column type: categorical, numerical, text data \n* The number of observations in the reference dataset\n* The number of unique values in the column (n_unique)\n\nYou can also set a rule to detect dataset-level drift based on the number of columns that are drifted."
  },
  {
    "title": "Data drift",
    "description": "How data drift detection works",
    "filename": "docs-main/metrics/explainer_drift.mdx",
    "ai_section": "## Data Requirements\nTo effectively evaluate data or prediction drift, you need to meet the following requirements:\n\n**Two datasets**: Always pass two datasets: current (evaluated for drift) and reference (benchmark).\n\n**Non-empty columns**: Ensure that the columns tested for drift are not empty; otherwise, Evidently will raise an error.\n\n**Empty values**: Empty or infinite values (+-np.inf) in columns will be filtered out when calculating distribution drift. \n\n<Note>\n  By default, drift tests do **not** respond to changes in the number of empty values. We recommend running separate tests on the share of nulls in the dataset.\n</Note>"
  },
  {
    "title": "Data drift",
    "description": "How data drift detection works",
    "filename": "docs-main/metrics/explainer_drift.mdx",
    "ai_section": "## Dataset Drift\nWith presets like `DatasetDriftPreset()` and metrics such as `DriftedColumnsCount()`, you can set rules on individual column drift results to detect dataset-level drift. For example, declare dataset drift if 50% of all features drifted. Each column in the dataset is tested for drift individually using the default method for the column type, and you can specify a custom threshold."
  },
  {
    "title": "Data drift",
    "description": "How data drift detection works",
    "filename": "docs-main/metrics/explainer_drift.mdx",
    "ai_section": "## Tabular Data Drift\nFor tabular data, the following defaults apply for numerical and categorical columns:\n\n**Small Data (<= 1000 observations)**:\n* For numerical columns (n_unique > 5): Use the two-sample Kolmogorov-Smirnov test.\n* For categorical columns or numerical columns with n_unique <= 5: Use the chi-squared test.\n\n**Larger Data (> 1000 observations)**:\n* For numerical columns (n_unique > 5): Use Wasserstein Distance.\n* For categorical columns or numerical columns with n_unique <= 5: Use Jensenâ€“Shannon divergence.\n\n<Info>\n  All tests use a 0.95 confidence level by default; drift score is P-value (<= 0.05 means drift).\n</Info>\n\nYou can modify the drift detection logic by selecting available methods or specifying thresholds."
  },
  {
    "title": "Data drift",
    "description": "How data drift detection works",
    "filename": "docs-main/metrics/explainer_drift.mdx",
    "ai_section": "## Text Data Drift\nText content drift is assessed using a **domain classifier** that distinguishes between data from reference and current distributions. If the model confidently identifies which samples belong to the â€œnewerâ€ data, the datasets are considered significantly different. \n\n<Info>\n  You can read more about the domain classifier approach in the [paper](https://arxiv.org/pdf/1810.11953.pdf).\n</Info>\n\nThe drift score here is the ROC AUC of the resulting classifier, with defaults set for larger and smaller datasets. If drift is detected, Evidently calculates the top features of the domain classifier, providing characteristic words that indicate whether a sample belongs to the reference or current datasets.\n\n<Tip>\n  **Text descriptors drift**: You can check for distribution drift in text descriptors (like text length) by first computing selected [text descriptors](/docs/library/descriptors).\n</Tip>"
  },
  {
    "title": "Data drift",
    "description": "How data drift detection works",
    "filename": "docs-main/metrics/explainer_drift.mdx",
    "ai_section": "## Resources\nFor a better understanding of different tests in various use cases, you can explore our blogs:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets.](https://evidentlyai.com/blog/data-drift-detection-large-datasets)\n* [Shift happens: how to detect drift in ML embeddings.](https://www.evidentlyai.com/blog/embedding-drift-detection)\n\nAdditional links:\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)"
  },
  {
    "title": "LLM evals",
    "description": "Description of your new file.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_llm_evals.mdx",
    "ai_section": "## Section Title\nSection text with all relevant details."
  },
  {
    "title": "LLM evals",
    "description": "Description of your new file.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_llm_evals.mdx",
    "ai_section": "## Another Section\nAnother section text."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Ranking Metrics Overview\nThe following metrics can be used for ranking, retrieval, and recommender systems. Each metric is designed to evaluate different aspects of a model's performance in providing relevant item recommendations."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Recall\n**Evidently Metric**: `RecallTopK`. \n\nRecall at K reflects the ability of the recommender or ranking system to retrieve all relevant items within the top K results.\n\n**Implemented method**:\n* **Compute recall at K by user**: Measure the share of all relevant items that appear in the top K results.\n\n$$\\text{Recall at } K = \\frac{\\text{Number of relevant items in } K}{\\text{Total number of relevant items}}$$\n\n* **Compute overall recall**: Average the results across all users (queries).\n\n**Range**: 0 to 1.\n\n**Interpretation**: A higher recall at K indicates a higher proportion of relevant items retrieved."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Precision\n**Evidently Metric**: `PrecisionTopK`. \n\nPrecision at K reflects the ability of the system to suggest items relevant to usersâ€™ preferences.\n\n**Implemented method**:\n* **Compute precision at K by user**: Measure the share of relevant results within the top K.\n\n$$\\text{Precision at } K = \\frac{\\text{Number of relevant items in } K}{\\text{Total number of items in } K}$$\n\n* **Compute overall precision**: Average the results across all users (queries).\n\n**Range**: 0 to 1.\n\n**Interpretation**: A higher precision at K indicates a larger proportion of the top results are relevant."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## F Beta\n**Evidently Metric**: `FBetaTopK`.\n\nThe F Beta score at K combines precision and recall into a single value for balanced performance measurement.\n\n$$F_{\\beta} = \\frac{(1 + \\beta^2) \\times \\text{Precision at K} \\times \\text{Recall at K}}{(\\beta^2 \\times \\text{Precision at K}) + \\text{Recall at K}}$$\n\n`Beta` parameter adjusts weighting between recall and precision.\n\n**Range**: 0 to 1.\n\n**Interpretation**: Higher F Beta values indicate better overall performance."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Mean Average Precision (MAP)\n**Evidently Metric**: `MAP`.\n\nMAP at K assesses the ability to suggest relevant items while ranking them higher.\n\n**Implemented method**:\n* **Compute Average Precision (AP) at K by user**: Average precision values at relevant item positions within K.\n\n$$\\text{AP@K} = \\frac{1}{N} \\sum_{k=1}^{K} Precision(k) \\times rel(k)$$\n\n* **Compute Mean Average Precision (MAP) at K**: Average the results across all users (queries).\n\n$$\\text{MAP@K} = \\frac{1}{U} \\sum_{u=1}^{U} \\text{AP@K}_u$$\n\n**Range**: 0 to 1.\n\n**Interpretation**: Higher MAP values indicate better placement of relevant items."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Mean Average Recall (MAR)\n**Evidently Metric**: `MAR`.\n\nMAR at K evaluates the ability of the system to retrieve all relevant items within the top-K results.\n\n**Implemented method**:\n* **Compute average recall at K by user**: Average the recall at each relevant position within the top K.\n\n$$\\text{AR@K} = \\frac{1}{N} \\sum_{k=1}^{K} Recall(k) \\times rel(k)$$\n\n* **Compute mean average recall at K**: Average results across all users (queries).\n\n$$\\text{MAR@K} = \\frac{1}{U} \\sum_{u=1}^{U} \\text{AR@K}_u$$\n\n**Range**: 0 to 1.\n\n**Interpretation**: Higher MAR values indicate better retrieval of relevant items."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Normalized Discounted Cumulative Gain (NDCG)\n**Evidently Metric**: `NDCG`.\n\nNDCG at K assesses ranking quality compared to an ideal order of relevant items.\n\n**Implemented method**:\n* **Provide item relevance score**: Assign relevance scores for each top-K item.\n* **Compute DCG at K**: Measure ranking quality for top-K items.\n\n$$\\text{DCG@K} = \\sum_{i=1}^{K} \\frac{rel_i}{\\log_2(i + 1)}$$\n\n* **Compute NDCG**: Normalize by ideal DCG.\n\n$$\\text{NDCG@K} = \\frac{DCG@K}{IDCG@K}$$\n\n**Range**: 0 to 1.\n\n**Interpretation**: Higher NDCG values indicate better ranking ability of the system."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Hit Rate\n**Evidently Metric**: `HitRate`.\n\nHit Rate at K calculates the share of users with at least one relevant item in the K recommendations.\n\n**Implemented method**:\n* **Compute â€œhitâ€ for each user**: Identify if any top-K item is relevant.\n* **Compute average hit rate**: Average this metric across all users.\n\n**Range**: 0 to 1.\n\n**Interpretation**: A higher Hit Rate indicates more users have relevant recommendations."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Mean Reciprocal Rank (MRR)\n**Evidently Metric**: `MRR`.\n\nMRR measures ranking quality based on the position of the first relevant item.\n\n**Implemented method**:\n* Identify position of the **first relevant item** for each user.\n* Calculate the **reciprocal rank** for each user.\n\n$$\\text{MRR} = \\frac{1}{U} \\sum_{u=1}^{U}\\frac{1}{rank_i}$$\n\n**Range**: 0 to 1.\n\n**Interpretation**: A higher MRR indicates relevant items are closer to the top of recommendations."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## Score Distribution (Entropy)\n**Evidently Metric**: `ScoreDistribution`.\n\nThis metric computes predicted score entropy for score-type recommendations.\n\n**Implementation**:\n* Apply softmax transformation for top-K scores for all users.\n* Compute KL divergence (relative entropy).\n\nThe visualization shows score distribution at K and across available scores."
  },
  {
    "title": "Ranking and RecSys metrics",
    "description": "Open-source metrics for ranking and recommendations.",
    "filename": "docs-main/metrics/explainer_recsys.mdx",
    "ai_section": "## RecSys\n<Warning>\n   These metrics are **coming soon** to the new Evidently API! Check the old docs for now.\n</Warning>"
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Model Quality Summary Metrics\nEvidently calculates several standard model quality metrics, including Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). Each quality metric is displayed alongside one standard deviation to estimate the stability of the performance. Additionally, Evidently generates interactive visualizations to support model performance analysis, helping to analyze mistakes and develop improvement ideas."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Predicted vs Actual\nThis section focuses on the comparison between predicted and actual values, visualized in a scatter plot. This representation allows users to quickly identify how closely predictions align with actual outcomes."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Predicted vs Actual in Time\nEvidently provides visualizations that depict the predicted and actual values over time or by index when a datetime is not available. This helps in understanding the trends and variations of predictions compared to actual results in a time series format."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Error (Predicted - Actual)\nThis section showcases the model error values over time or by index if a datetime is not provided. It assists in identifying the specific instances where the model's predictions deviated from the actual values."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Absolute Percentage Error\nAbsolute percentage error values are visualized over time or by index in this section. This metric provides insight into how reliable the predictions are relative to the actual values, expressed in percentage terms."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Error Distribution\nThis visualization presents the distribution of model error values, allowing users to understand the spread and frequency of errors, which can indicate patterns in the model's performance."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Error Normality\nEvidently utilizes a quantile-quantile plot (Q-Q plot) to estimate the normality of the error values. This visual representation helps to understand if the errors follow a normal distribution, which can be crucial for further statistical analysis."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Mean Error per Group\nThis section summarizes the model quality metrics (ME, MAE, MAPE) for two segments: the highest 5% of predictions with the largest positive errors (overestimations) and the highest 5% of predictions with the largest negative errors (underestimations)."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Predicted vs Actual per Group\nHere, prediction plots visualize the areas where the model underestimates and overestimates the target function within the defined groups. This differentiation helps target specific problems or improvement opportunities in the model."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Error Bias: Mean/Most Common Feature Value per Group\nThis table illustrates the differences in feature values among the three groups: \"OVER\" for overestimations, \"UNDER\" for underestimations, and \"MAJORITY\" for the remaining predictions. It displays mean values for numerical features and most common values for categorical features. A significant difference between groups indicates sensitivity of model error to certain feature values."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Error Bias per Feature\nFor each feature, a histogram shows the distribution of its values within segments exhibiting extreme errors as well as the majority group. This visualization aids in identifying a correlation between high error rates and particular feature values."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Predicted vs Actual per Feature\nThis section depicts scatterplots comparing predicted vs actual values for each feature. It facilitates the identification of underperforming segments that may be sensitive to specific feature values, thereby highlighting potential areas for improvement."
  },
  {
    "title": "Regression metrics",
    "description": "Open-source regression quality metrics.",
    "noindex": "true",
    "filename": "docs-main/metrics/explainer_regression.mdx",
    "ai_section": "## Metrics Output\nThis final section contains details regarding the output of various metrics calculated by Evidently, summarizing the findings and providing insights into the model performance as analyzed through the various sections discussed."
  },
  {
    "title": "Evaluations",
    "description": "Available metrics, tests and how to customize them.",
    "mode": "wide",
    "filename": "docs-main/metrics/introduction.mdx",
    "ai_section": "## Evaluations in Evidently Library\nEvaluations are a core feature of the Evidently library. It offers both a catalog of 100+ evaluations and a framework to easily configure your own. Before diving deeper, it's recommended to familiarize yourself with the core workflow, which includes trying examples for [LLMs](docs/quickstart_llm) or [ML](docs/quickstart_ml)."
  },
  {
    "title": "Evaluations",
    "description": "Available metrics, tests and how to customize them.",
    "mode": "wide",
    "filename": "docs-main/metrics/introduction.mdx",
    "ai_section": "## All Descriptors\nThe \"All Descriptors\" section provides access to text and LLM evaluations. Users can explore various descriptors that assist in assessing performance and outcomes within the Evidently library."
  },
  {
    "title": "Evaluations",
    "description": "Available metrics, tests and how to customize them.",
    "mode": "wide",
    "filename": "docs-main/metrics/introduction.mdx",
    "ai_section": "## All Metrics\nThe \"All Metrics\" section encompasses all data and ML evaluations. This resource is essential for users who need to measure and analyze the effectiveness of their machine learning models."
  },
  {
    "title": "Evaluations",
    "description": "Available metrics, tests and how to customize them.",
    "mode": "wide",
    "filename": "docs-main/metrics/introduction.mdx",
    "ai_section": "## All Presets\nIn the \"All Presets\" section, pre-built evaluation templates are available. These templates are designed to help users quickly set up evaluations without starting from scratch, saving time and effort."
  },
  {
    "title": "Evaluations",
    "description": "Available metrics, tests and how to customize them.",
    "mode": "wide",
    "filename": "docs-main/metrics/introduction.mdx",
    "ai_section": "## Popular Links\nThe \"Popular Links\" section directs users to key resources within the Evidently library. These links are particularly helpful for users looking for specific functionalities."
  },
  {
    "title": "Evaluations",
    "description": "Available metrics, tests and how to customize them.",
    "mode": "wide",
    "filename": "docs-main/metrics/introduction.mdx",
    "ai_section": "## LLM Judges\nThe \"LLM Judges\" resource explains how to create a custom LLM judge. This feature is beneficial for users seeking to tailor their evaluations to meet specific criteria or requirements."
  },
  {
    "title": "Evaluations",
    "description": "Available metrics, tests and how to customize them.",
    "mode": "wide",
    "filename": "docs-main/metrics/introduction.mdx",
    "ai_section": "## Data Drift\nThe \"Data Drift\" section covers how to customize data drift detection. Understanding and modifying this aspect is crucial for maintaining the relevance and accuracy of machine learning models over time."
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Pre-requisites\nBefore using the `ClassificationPreset`, ensure you are familiar with the following:\n* How to use [Data Definition](/docs/library/data_definition) to prepare your data.\n* How to create [Reports](/docs/library/report)."
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Report Overview\nThe `ClassificationPreset` enables evaluation and visualization of classification task performance, applicable to binary and multi-class classifications. You can run this report on a single dataset or compare against a reference dataset, such as previous performance or different model prompts."
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Running a Report on Current Dataset\nTo run a Preset on a single current dataset, you can use the following code snippet:\n\n```python\nreport = Report([\n    ClassificationPreset(),\n])\nmy_eval = report.run(current, None)\n```"
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Running Tests with Evaluation\nTo include pass/fail classification quality tests that are auto-generated from the `ref` dataset, use the following:\n\n```python\nreport = Report([\n    ClassificationPreset(),\n],\ninclude_tests=True)\nmy_eval = report.run(current, ref)\n```"
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Available Metrics and Visualizations\nThe report provides a variety of metrics and visualizations, including:\n* **Metrics**: Accuracy, Precision, Recall, F1-score, ROC AUC, LogLoss, etc.\n* **Visualizations**: Class Representation, Confusion Matrix, Class Separation Quality, Probability Distribution, ROC Curve, PR Curve, etc.\n\nIf feature columns are included, the report will illustrate Classification Quality by column, highlighting the relationship between features and the target."
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Test Suite Functionality\nThe Test Suite enables automatic checks to assess model performance metrics. These tests are generated:\n* **Based on a reference dataset**: Conditions like expected prediction accuracy are derived from it.\n* **Based on heuristics**: If there is no reference dataset, Evidently creates a dummy classification model as a baseline."
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Use Cases for Classification Presets\nClassification Presets are applicable in various scenarios:\n* **Model/System Comparison**: Evaluate performance across different datasets, useful for A/B testing and experimenting with prompt configurations.\n* **Production Monitoring**: Run evaluations upon receiving true labels in production to visualize performance and inform model updates.\n* **Debugging**: Investigate performance drop with visual reports."
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Data Requirements\nTo effectively use the Classification Preset, the following data requirements should be met:\n* **Target and Prediction Columns**: Crucial for calculating performance.\n* **One or Two Datasets**: Use two datasets for side-by-side comparisons or auto-generated tests.\n* (Optional) **Input Features**: Include for exploring column-target relationships.\n* (Optional) **Timestamp**: Pass it if available for some plots."
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Report Customization Options\nYou can customize the report in various ways:\n* **Change Test Conditions**: Modify auto-generated conditions to your specific requirements.\n* **Modify Report Composition**: Add additional metrics such as column Correlations, Missing Values, or Data Drift."
  },
  {
    "title": "Classification",
    "description": "Overview of the Classification Quality Preset",
    "filename": "docs-main/metrics/preset_classification.mdx",
    "ai_section": "## Additional Resources\nFor further information:\n* **Data Schema Mapping**: Refer to the [data definition](/docs/library/data_definition).\n* **Creating a Custom Report**: Consult the documentation on how to create a [custom Report](/docs/library/report) and modify [Tests](/docs/library/tests)."
  },
  {
    "title": "Data Drift",
    "description": "Overview of the Data Drift Preset.",
    "filename": "docs-main/metrics/preset_data_drift.mdx",
    "ai_section": "## Pre-requisites\nBefore running a data drift evaluation, ensure you are familiar with the following:\n* How to use [Data Definition](https://your-url/docs/library/data_definition) to prepare your data.\n* How to create [Reports](https://your-url/docs/library/report)."
  },
  {
    "title": "Data Drift",
    "description": "Overview of the Data Drift Preset.",
    "filename": "docs-main/metrics/preset_data_drift.mdx",
    "ai_section": "## Report Usage\nTo run a Preset comparing `current` data to `ref` data, use the following code snippet:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n])\n\nmy_eval = report.run(current, ref)\n```\n\nIf you want to add Tests with explicit pass/fail for each column, implement this code:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```"
  },
  {
    "title": "Data Drift",
    "description": "Overview of the Data Drift Preset.",
    "filename": "docs-main/metrics/preset_data_drift.mdx",
    "ai_section": "## Overview of DataDriftPreset\nThe `DataDriftPreset` evaluates shifts in data distribution between two datasets to detect significant changes.\n\nKey features include:\n* **Column drift:** Checks for shifts in each column using an automatic drift detection method based on column type and observations.\n* **Target / Prediction Drift:** Evaluates prediction or target values alongside other columns.\n* **Overall dataset drift:** Indicates the share of drifting columns; a dataset is considered to have drift if at least 50% of columns drift.\n\nYou can sort rows by feature name or type and view individual columns for detailed distribution insights."
  },
  {
    "title": "Data Drift",
    "description": "Overview of the Data Drift Preset.",
    "filename": "docs-main/metrics/preset_data_drift.mdx",
    "ai_section": "## Use Cases for Data Drift Evaluation\nData drift can be evaluated in various situations including:\n* **Monitoring ML model performance without ground truth:** Use feature and prediction drift as proxy metrics when true labels are unavailable. \n* **Debugging ML model quality decay:** Evaluate data drift to understand changes in feature patterns and identify quality drops.\n* **Understanding model drift in offline environments:** Analyze historical data drift to define optimal detection and retraining strategies.\n* **Deciding on model retraining:** Check for data drift before introducing new data to avoid unnecessary retraining."
  },
  {
    "title": "Data Drift",
    "description": "Overview of the Data Drift Preset.",
    "filename": "docs-main/metrics/preset_data_drift.mdx",
    "ai_section": "## Data Requirements for Drift Evaluation\nTo effectively evaluate drift, adhere to the following data requirements:\n* **Input columns:** Must be non-empty.\n* **Two datasets:** Always compare current data against a reference dataset.\n* **Set column types (optional):** Specify column types (numerical, categorical, text) explicitly for better accuracy. Otherwise, the system will auto-detect these."
  },
  {
    "title": "Data Drift",
    "description": "Overview of the Data Drift Preset.",
    "filename": "docs-main/metrics/preset_data_drift.mdx",
    "ai_section": "## Customizing Reports\nYou have multiple options to customize your reports:\n* **Select columns:** Focus drift detection on specific columns using the `columns` parameter.\n* **Change drift parameters:** Modify detection methods, thresholds, or implement custom methods.\n* **Modify report composition:** Add additional metrics to the report for comprehensive evaluation, including evaluating a separate target/prediction column or adding data quality checks.\n\nFor further details, visit the [Drift Customization](https://your-url/metrics/customize_data_drift) section."
  },
  {
    "title": "Data Summary",
    "description": "Overview of the Data Summary Preset.",
    "filename": "docs-main/metrics/preset_data_summary.mdx",
    "ai_section": "## Pre-requisites\nTo effectively use the features discussed, ensure you are familiar with the following:\n* Using [Data Definition](/docs/library/data_definition) to prepare your data.\n* Creating [Reports](/docs/library/report)."
  },
  {
    "title": "Data Summary",
    "description": "Overview of the Data Summary Preset.",
    "filename": "docs-main/metrics/preset_data_summary.mdx",
    "ai_section": "## Report Execution\nTo run a Preset on your data for a single `current` dataset, use the following code snippet:\n\n```python\nreport = Report([\n    DataSummaryPreset(),\n])\n\nmy_eval = report.run(current, None)\n```"
  },
  {
    "title": "Data Summary",
    "description": "Overview of the Data Summary Preset.",
    "filename": "docs-main/metrics/preset_data_summary.mdx",
    "ai_section": "## Test Suite\nTo incorporate pass/fail data quality Tests generated from a `ref` dataset, implement this code:\n\n```python\nreport = Report([\n    DataSummaryPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```"
  },
  {
    "title": "Data Summary",
    "description": "Overview of the Data Summary Preset.",
    "filename": "docs-main/metrics/preset_data_summary.mdx",
    "ai_section": "## Overview of DataSummaryPreset\nThe `DataSummaryPreset` allows you to visualize key descriptive statistics for the dataset and each column. Providing two datasets will enable a side-by-side comparison.\n\n**Dataset stats.** Show statistics like the number of rows/columns, and counts of empty columns/rows.\n\n**Column stats.** Display relevant statistics and visualize distributions for each column typeâ€”including numerical, categorical, text, and datetime.\n\nIf Tests are enabled, an additional Test Suite view will be available."
  },
  {
    "title": "Data Summary",
    "description": "Overview of the Data Summary Preset.",
    "filename": "docs-main/metrics/preset_data_summary.mdx",
    "ai_section": "## How Tests Work\nTests are auto-generated under two scenarios:\n\n* **Based on reference dataset.** If a reference dataset is provided, conditions like minimum and maximum feature ranges will be derived from it.\n* **Based on heuristics.** If no reference is provided, some Tests will run using heuristics, such as expecting no missing values.\n\nFor more information about Tests, refer to [Tests](/docs/library/tests) and the [reference table](/metrics/all_metrics)."
  },
  {
    "title": "Data Summary",
    "description": "Overview of the Data Summary Preset.",
    "filename": "docs-main/metrics/preset_data_summary.mdx",
    "ai_section": "## Use Cases for DataSummaryPreset\nYou can apply this Preset in several scenarios:\n\n* **Exploratory data analysis.** Utilize the visual Report for dataset exploration during model training, after new data arrives, or while debugging.\n* **Dataset comparison.** Compare any datasets to identify differencesâ€”such as training vs. test datasets or current production data against training.\n* **Data quality tests in production.** Enable Tests to check quality and stability of input data before generating predictions or processing transformations.\n* **Data profiling in production.** Use this Preset to monitor and analyze the shape of production data for future visualization."
  },
  {
    "title": "Data Summary",
    "description": "Overview of the Data Summary Preset.",
    "filename": "docs-main/metrics/preset_data_summary.mdx",
    "ai_section": "## Data Requirements\nTo effectively use the DataSummaryPreset, ensure the following data criteria are met:\n\n* **Input columns.** Any input columns can be provided but must be non-empty.\n* **One or two datasets.** Pass one dataset for a general summary or two for side-by-side comparisons and test auto-generation.\n* **(Optional) Set column types.** While it's recommended to explicitly specify column types, the presetter will auto-detect types like numerical and categorical. Text data must always be mapped.\n\nFor guidelines on data schema mapping, refer to [data definition](/docs/library/data_definition)."
  },
  {
    "title": "Data Summary",
    "description": "Overview of the Data Summary Preset.",
    "filename": "docs-main/metrics/preset_data_summary.mdx",
    "ai_section": "## Report Customization\nYou have various options to customize your Report:\n\n**Select columns.** Use the `columns` parameter for stats only on specific columns.\n\n**Modify Report composition.** Add other Metrics for a comprehensive assessment, such as:\n* **Correlations.** Include correlations heatmap.\n* **Missing values.** Add a missing values heatmap and monitor data quality.\n* **Data drift.** Evaluate distribution shifts between datasets.\n\n**Customize Test conditions.** Alter auto-generated Test conditions by setting your own, whether different from the reference or entirely custom for each Test.\n\nFor more details on creating a [Report](/docs/library/report) and adding [Tests](/docs/library/tests), refer to the documentation."
  },
  {
    "title": "Recommendations",
    "description": "Overview of the Recommender Systems Preset",
    "filename": "docs-main/metrics/preset_recsys.mdx",
    "ai_section": "## Warning About Preset Availability\nThis Preset is **coming soon** to the new Evidently API! Check the old docs for now."
  },
  {
    "title": "Recommendations",
    "description": "Overview of the Recommender Systems Preset",
    "filename": "docs-main/metrics/preset_recsys.mdx",
    "ai_section": "## Running a Report\nTo run a Preset on your data for a single current dataset for top-k recommendations, use the following code:\n\n```python\nreport = Report([\n    RecSysPreset(k=5),\n])\n\nmy_eval = report.run(current, None)\n```"
  },
  {
    "title": "Recommendations",
    "description": "Overview of the Recommender Systems Preset",
    "filename": "docs-main/metrics/preset_recsys.mdx",
    "ai_section": "## Test Suite Overview\nTo add pass/fail ranking quality Tests, which are auto-generated from the `ref` dataset, use:\n\n```python\nreport = Report([\n    RecSysPreset(k=5),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```"
  },
  {
    "title": "Recommendations",
    "description": "Overview of the Recommender Systems Preset",
    "filename": "docs-main/metrics/preset_recsys.mdx",
    "ai_section": "## RecsysPreset Overview\n`RecsysPreset` evaluates the quality of the recommender system by generating multiple metrics to assess the quality of ranking and diversity of recommendations. You must provide the `k` parameter to evaluate the Top-K recommendations. It includes 10+ metrics like NDCG at K, MAP at K, HitRate, diversity, serendipity, etc. Metric selection depends on the data provided.\n\n<Info>\n  **Metric explainers.** Check the [Ranking and RecSys Metrics](/metrics/explainer_recsys) to see how each Metric works.\n</Info>"
  },
  {
    "title": "Recommendations",
    "description": "Overview of the Recommender Systems Preset",
    "filename": "docs-main/metrics/preset_recsys.mdx",
    "ai_section": "## Automatic Test Suite\nIf you enable Tests, the system will automatically run checks to assess if the model performance metrics are within bounds. Tests are auto-generated based on the reference dataset. If provided, conditions like expected ranking accuracy will be derived from it.\n\n<Info>\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table](/metrics/all_metrics).\n</Info>"
  },
  {
    "title": "Recommendations",
    "description": "Overview of the Recommender Systems Preset",
    "filename": "docs-main/metrics/preset_recsys.mdx",
    "ai_section": "## Use Cases for Presets\nThese Presets are useful in various scenarios:\n\n* **Experimental evaluations** as you iterate on building your recommender system.\n* **Side-by-side comparison** for two different models or periods.\n* **Production monitoring** checks after acquiring ground truth labels.\n* **Debugging** to investigate any drop in performance using the visual Report."
  },
  {
    "title": "Recommendations",
    "description": "Overview of the Recommender Systems Preset",
    "filename": "docs-main/metrics/preset_recsys.mdx",
    "ai_section": "## Data Requirements\nTo effectively use the Presets, you need to fulfill the following data requirements:\n\n* **Prediction:** Recommended items with rank or score.\n* **Target:** True relevance score or interaction result.\n* **(Optional) Input/user features:** For some diversity metrics.\n* **(Optional) Training data:** For some diversity metrics.\n* **(Optional) Reference dataset:** For side-by-side comparison or auto-generated test conditions.\n\n<Info>\n  **Data schema mapping.** Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>"
  },
  {
    "title": "Recommendations",
    "description": "Overview of the Recommender Systems Preset",
    "filename": "docs-main/metrics/preset_recsys.mdx",
    "ai_section": "## Report Customization Options\nYou can customize the Report in various ways:\n\n* **Change Test conditions:** Modify the auto-generated conditions, either relative to the reference or any custom conditions.\n* **Modify Report composition:** Add metrics like Data Drift for user or item feature distributions, or evaluate prediction drift.\n\n<Info>\n  **Custom Report.** Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\n</Info>"
  },
  {
    "title": "Regression",
    "description": "Overview of the Regression Quality Preset",
    "filename": "docs-main/metrics/preset_regression.mdx",
    "ai_section": "## Pre-requisites\nBefore you begin, make sure you are familiar with how to use [Data Definition](/docs/library/data_definition) to prepare the data and how to create [Reports](/docs/library/report)."
  },
  {
    "title": "Regression",
    "description": "Overview of the Regression Quality Preset",
    "filename": "docs-main/metrics/preset_regression.mdx",
    "ai_section": "## Running a Basic Report\nTo run a Preset on your data for a single current dataset, use the following code:\n\n```python\nreport = Report([\n    RegressionPreset(),\n])\n\nmy_eval = report.run(current, None)\n```"
  },
  {
    "title": "Regression",
    "description": "Overview of the Regression Quality Preset",
    "filename": "docs-main/metrics/preset_regression.mdx",
    "ai_section": "## Adding Tests to Your Report\nTo add pass/fail regression quality Tests that are auto-generated from the `ref` dataset, use this code:\n\n```python\nreport = Report([\n    RegressionPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```"
  },
  {
    "title": "Regression",
    "description": "Overview of the Regression Quality Preset",
    "filename": "docs-main/metrics/preset_regression.mdx",
    "ai_section": "## Overview of RegressionPreset\nThe `RegressionPreset` allows you to evaluate and visualize performance on regression tasks. You can run a Report for a single dataset or compare it against a reference dataset (e.g., past performance or a different model/prompt).\n\n### Report Features\nThe Report includes various metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and visualizations such as Actual vs Predicted Plot and Error Distribution.\n\nIf Tests are enabled, they automatically run checks to assess if the model performance metrics are within bounds."
  },
  {
    "title": "Regression",
    "description": "Overview of the Regression Quality Preset",
    "filename": "docs-main/metrics/preset_regression.mdx",
    "ai_section": "## How Tests Work\nTests are auto-generated based on the reference dataset provided or, in the absence of a reference, a dummy regression model is created to run checks against it. For more details on how Tests function and the defaults for each Test, refer to [Tests](/docs/library/tests) and the [reference table](/metrics/all_metrics)."
  },
  {
    "title": "Regression",
    "description": "Overview of the Regression Quality Preset",
    "filename": "docs-main/metrics/preset_regression.mdx",
    "ai_section": "## Use Cases for Regression Presets\nThese Presets are particularly useful in various scenarios:\n\n- **Model/System Comparison**: Compare predictive system performance across different datasets, useful for A/B testing and experimenting with model configurations.\n  \n- **Production Monitoring**: Run evaluations whenever actual values are received in production to visualize performance and decide on model updates or retraining.\n\n- **Debugging**: When performance drops, use the visual Report to check error distributions and explore model errors."
  },
  {
    "title": "Regression",
    "description": "Overview of the Regression Quality Preset",
    "filename": "docs-main/metrics/preset_regression.mdx",
    "ai_section": "## Data Requirements for Reporting\nTo successfully generate a Report, the following data requirements must be met:\n\n- **Target and Prediction Columns**: These are required to calculate performance metrics.\n  \n- **One or Two Datasets**: Pass two datasets for side-by-side comparisons or to auto-generate tests.\n\n- **Optional Inputs**: You can include input features for exploring underperforming segments and a timestamp to appear on some plots.\n\nFor data schema mapping, refer to the [data definition](/docs/library/data_definition)."
  },
  {
    "title": "Regression",
    "description": "Overview of the Regression Quality Preset",
    "filename": "docs-main/metrics/preset_regression.mdx",
    "ai_section": "## Customizing Your Report\nYou can customize the Report in several ways:\n\n- **Change Test Conditions**: Modify the auto-generated conditions or set your own conditions relative to the reference or custom thresholds.\n\n- **Modify Report Composition**: Add additional metrics such as column Correlations, Missing Values, or Data Drift, which can be useful for evaluating statistical distribution shifts in the model target (concept drift).\n\nFor guidance on creating a [Report](/docs/library/report) and adding [Tests](/docs/library/tests), consult the relevant documentation."
  },
  {
    "title": "Text Evals",
    "description": "Overview of the Text Evals Preset.",
    "filename": "docs-main/metrics/preset_text_evals.mdx",
    "ai_section": "## Running the Report\nTo run a Preset on your data for a single `current` dataset, start by computing `descriptors` and adding them to your Dataset. Hereâ€™s how you can do it:\n\n```python\nreport = Report(metrics=[\n    TextEvals(),\n])\n\nmy_eval = report.run(current, None)\n```"
  },
  {
    "title": "Text Evals",
    "description": "Overview of the Text Evals Preset.",
    "filename": "docs-main/metrics/preset_text_evals.mdx",
    "ai_section": "## Test Suite\nTo add pass/fail data quality Tests that are auto-generated from a `ref` dataset, you can enable this feature in your Report:\n\n```python\nreport = Report([\n    TextEvals(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```"
  },
  {
    "title": "Text Evals",
    "description": "Overview of the Text Evals Preset.",
    "filename": "docs-main/metrics/preset_text_evals.mdx",
    "ai_section": "## Overview of TextEvals\nThe `TextEvals` Preset summarizes results of all **descriptors** (output-level text evaluations) computed on your dataset. It provides a visual exploration of distributions and captures relevant statistics, which may vary based on descriptor type. If two datasets are provided, a side-by-side comparison will be generated.\n\n<Info>\n  **How text and LLM evaluations work.** Read about [Descriptors](/docs/library/descriptors), or try a [Quickstart](/quickstart_llm).\n</Info>"
  },
  {
    "title": "Text Evals",
    "description": "Overview of the Text Evals Preset.",
    "filename": "docs-main/metrics/preset_text_evals.mdx",
    "ai_section": "## Test Suite Functionality\nIf the Test Suite is enabled, an additional view presents:\n\n* **Reference Dataset**: If provided, expected descriptor values are derived from it.\n* **Heuristic-Based Tests**: If no reference is available, some Tests will run based on heuristics, such as expecting no missing values.\n\n<Info>\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\n</Info>"
  },
  {
    "title": "Text Evals",
    "description": "Overview of the Text Evals Preset.",
    "filename": "docs-main/metrics/preset_text_evals.mdx",
    "ai_section": "## Use Cases for TextEvals\nThe Preset can be used in various scenarios, including:\n\n* **LLM experiments**: Create visual Reports to explore evaluation results during prompt or model version experiments, enabling comparisons between different runs.\n* **LLM observability**: Evaluate production data to capture statistics over time."
  },
  {
    "title": "Text Evals",
    "description": "Overview of the Text Evals Preset.",
    "filename": "docs-main/metrics/preset_text_evals.mdx",
    "ai_section": "## Data Requirements\nTo utilize this Report, youâ€™ll need:\n\n* **Input dataset with descriptors**: Ensure your Dataset includes computed descriptors (see [how](/docs/library/descriptors)).\n* **One or two datasets**: You can pass either a single dataset or two for comparison or to auto-generate test conditions.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>"
  },
  {
    "title": "Text Evals",
    "description": "Overview of the Text Evals Preset.",
    "filename": "docs-main/metrics/preset_text_evals.mdx",
    "ai_section": "## Report Customization Options\nYou can customize your Report in several ways:\n\n**Select descriptors**: Specify seen descriptors in the Dataset using the `columns` parameter.\n\n**Customize Test conditions**: Set your own conditions, like failing if texts exceed or fall below a specified Length Range. Refer to a [Quickstart example](/quickstart_llm).\n\n**Modify Report composition**: Incorporate additional Metrics for a more comprehensive evaluation. Options include:\n\n* **Correlations**: Use a correlations heatmap to analyze connections between descriptor values or with metadata in the Dataset.\n* **Data drift**: Measure data drift for comparing descriptor distributions between two datasets.\n\n<Info>\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\n</Info>"
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## Overview of Evidently\nEvidently helps you evaluate LLM outputs automatically. It allows you to compare prompts, models, and run regression or adversarial tests with clear, repeatable checks, leading to faster iterations, more confident decisions, and fewer surprises in production."
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## Getting Help\nNeed help at any point? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b)."
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## 1. Set up your environment\nFor a fully local flow, skip steps 1.1 and 1.3.\n\n### 1.1. Set up Evidently Cloud\n<CloudSignup />\n\n### 1.2. Installation and imports\nInstall the Evidently Python library:\n\n```python\n!pip install evidently\n```\n\nComponents to run the evals:\n\n```python\nimport pandas as pd\nfrom evidently import Dataset\nfrom evidently import DataDefinition\nfrom evidently import Report\nfrom evidently.presets import TextEvals\nfrom evidently.tests import lte, gte, eq\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\n```\n\nComponents to connect with Evidently Cloud:\n\n```python\nfrom evidently.ui.workspace import CloudWorkspace\n```\n\n### 1.3. Create a Project\n<CreateProject />"
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## 2. Prepare the dataset\nLet's create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\n\n```python\ndata = [\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\n    [\"Tell me a joke.\", \"Why don't programmers like nature? Too many bugs!\"],\n    [\"When does water boil?\", \"Water's boiling point is 100 degrees Celsius.\"],\n    [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\n    [\"Whatâ€™s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\n    [\"Can you help me with my math homework?\", \"I'm sorry, but I can't assist with homework.\"],\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\n    [\"Whatâ€™s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\n    [\"Can you tell me the latest stock market trends?\", \"I'm sorry, but I can't provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\n]\ncolumns = [\"question\", \"answer\"]\n\neval_df = pd.DataFrame(data, columns=columns)\n```"
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## Customizing Your Dataset\n**Preparing your own data**: You can provide data with any structure, such as:\n- Inputs and outputs from your LLM\n- Inputs, outputs, and reference outputs (for comparison)\n- Inputs, context, and outputs (for RAG evaluation)\n\n**Collecting live data**: You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)."
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## 3. Run evaluations\nWe'll evaluate the answers for:\n\n- **Sentiment:** from -1 (negative) to 1 (positive)\n- **Text length:** character count\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with a built-in prompt.\n\nSet the OpenAI key as an environment variable:\n\n```python\n## import os\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\n```\n\nIf you don't have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\n\nTo run evals, pass the dataset and specify the list of descriptors to add:\n\n```python\neval_dataset = Dataset.from_pandas(\n    eval_df,\n    data_definition=DataDefinition(),\n    descriptors=[\n        Sentiment(\"answer\", alias=\"Sentiment\"),\n        TextLength(\"answer\", alias=\"Length\"),\n        DeclineLLMEval(\"answer\", alias=\"Denials\")\n    ]\n)\n```\n\n**Congratulations!** You've just run your first eval. Preview the results locally in pandas:\n\n```python\neval_dataset.as_dataframe()\n```"
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## Available Evaluations\n**What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors)."
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## 4. Create a Report\n**Create and run a Report**. It will summarize the evaluation results.\n\n```python\nreport = Report([TextEvals()])\nmy_eval = report.run(eval_dataset, None)\n```\n\n**Local preview**: In a Python environment like Jupyter notebook or Colab, run:\n\n```python\nmy_eval\n```\n\nYou can also export results as JSON, Python dictionary, or save as an external HTML file.\n\n```python\n# my_eval.json()\n# my_eval.dict()\n# my_report.save_html(â€œfile.htmlâ€)\n```\n\n**Upload the Report to Evidently Cloud** together with scored data:\n\n```python\nws.add_run(project.id, my_eval, include_data=True)\n```\n\n**Explore**: Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to Reports for score summaries and data browsing."
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## 5. Get a Dashboard\nAs you run more evals, track them over time. Go to \"Dashboard\" in the left menu, enter \"Edit\" mode, and add a new \"Columns\" tab.\n\nYou'll see a set of panels with descriptor values. Each will have a single data point initially. As you log more evaluations, track trends and set up alerts."
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## 6. (Optional) Add tests\nYou can add conditions to your evaluations. For example, you may expect that:\n\n- **Sentiment** is non-negative (greater or equal to 0)\n- **Text length** is at most 150 symbols (less or equal to 150)\n- **Denials**: there are none\n\nYou can implement this logic easily.\n\n```python\n# Run the evaluation with tests \neval_dataset = Dataset.from_pandas(\n    eval_df,\n    data_definition=DataDefinition(),\n    descriptors=[\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[gte(0, alias=\"Is_non_negative\")]),\n        TextLength(\"answer\", alias=\"Length\", tests=[lte(150, alias=\"Has_expected_length\")]),\n        DeclineLLMEval(\"answer\", alias=\"Denials\", tests=[eq(\"OK\", column=\"Denials\", alias=\"Is_not_a_refusal\")]),\n        TestSummary(success_all=True, alias=\"All_tests_passed\")\n    ]\n)\n```"
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## 7. (Optional) Add a custom LLM judge\nYou can implement custom criteria using built-in LLM judge templates.\n\nFor instance, classify user questions as \"appropriate\" or \"inappropriate\":\n\n```python\n# Define the evaluation criteria\nappropriate_scope = BinaryClassificationPromptTemplate(\n    criteria=\"\"\"An appropriate question is any educational query related to\n    academic subjects, general school-level world knowledge, or skills.\n    An inappropriate question is anything offensive, irrelevant, or out of\n    scope.\"\"\",\n    target_category=\"APPROPRIATE\",\n    non_target_category=\"INAPPROPRIATE\",\n    include_reasoning=True,\n)\n\n# Apply evaluation\nllm_evals = Dataset.from_pandas(\n    eval_df,\n    data_definition=DataDefinition(),\n    descriptors=[\n        LLMEval(\"question\", template=appropriate_scope,\n                provider=\"openai\", model=\"gpt-4o-mini\",\n                alias=\"Question topic\")\n    ]\n)\n```"
  },
  {
    "title": "LLM Evaluation",
    "description": "Evaluate text outputs in under 5 minutes",
    "filename": "docs-main/quickstart_llm.mdx",
    "ai_section": "## What's next?\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge). We also have lots of other examples; [explore tutorials](/metrics/introduction)."
  },
  {
    "title": "Data and ML checks",
    "description": "Run a simple evaluation for tabular data",
    "filename": "docs-main/quickstart_ml.mdx",
    "ai_section": "## Overview of Evidently\nEvidently helps you run tests and evaluations for your production ML systems. This includes evaluating prediction quality, input data quality, and detecting data and prediction drift."
  },
  {
    "title": "Data and ML checks",
    "description": "Run a simple evaluation for tabular data",
    "filename": "docs-main/quickstart_ml.mdx",
    "ai_section": "## Setting Up Your Environment\nTo run Evidently locally, follow a few setup steps. If you prefer a fully local flow, skip steps 1.1 and 1.3.\n\n### 1.1. Set Up Evidently Cloud\nTo begin, you can set up an account with Evidently Cloud.\n\n<CloudSignup />\n\n### 1.2. Installation and Imports\nInstall the Evidently Python library:\n\n```python\n!pip install evidently\n```\n\nImport essential components:\n\n```python\nimport pandas as pd\nfrom sklearn import datasets\n    \nfrom evidently import Dataset\nfrom evidently import DataDefinition\nfrom evidently import Report\nfrom evidently.presets import DataDriftPreset, DataSummaryPreset \n```\n\nFor connecting with Evidently Cloud:\n\n```python\nfrom evidently.ui.workspace import CloudWorkspace\n```\n\n### 1.3. Create a Project\nFollow the necessary steps to create a project in Evidently Cloud.\n\n<CreateProject />"
  },
  {
    "title": "Data and ML checks",
    "description": "Run a simple evaluation for tabular data",
    "filename": "docs-main/quickstart_ml.mdx",
    "ai_section": "## Preparing a Toy Dataset\nYou can import a toy dataset with tabular data for your evaluations:\n\n```python\nadult_data = datasets.fetch_openml(name=\"adult\", version=2, as_frame=\"auto\")\nadult = adult_data.frame\n```\n\nIf OpenML is not available, download the dataset directly:\n\n```python\nurl = \"https://github.com/evidentlyai/evidently/blob/main/test_data/adults.parquet?raw=true\"\nadult = pd.read_parquet(url, engine='pyarrow')\n```\n\nNext, split the data into two sets and introduce artificial drift for demonstration. Map the column types accordingly:\n\n```python\nschema = DataDefinition(\n    numerical_columns=[\"education-num\", \"age\", \"capital-gain\", \"hours-per-week\", \"capital-loss\", \"fnlwgt\"],\n    categorical_columns=[\"education\", \"occupation\", \"native-country\", \"workclass\", \"marital-status\", \"relationship\", \"race\", \"sex\", \"class\"],\n)\n```\n\nCreate Evidently Datasets to work with:\n\n```python\neval_data_1 = Dataset.from_pandas(\n    pd.DataFrame(adult_prod),\n    data_definition=schema\n)\n```\n\n```python\neval_data_2 = Dataset.from_pandas(\n    pd.DataFrame(adult_ref),\n    data_definition=schema\n)\n```"
  },
  {
    "title": "Data and ML checks",
    "description": "Run a simple evaluation for tabular data",
    "filename": "docs-main/quickstart_ml.mdx",
    "ai_section": "## Generating a Report\nTo generate a Data Drift report, you can create a Report object and run it:\n\n```python\nreport = Report([\n    DataDriftPreset() \n])\n\nmy_eval = report.run(eval_data_1, eval_data_2)\n```\n\nYou can customize drift parameters based on your requirements."
  },
  {
    "title": "Data and ML checks",
    "description": "Run a simple evaluation for tabular data",
    "filename": "docs-main/quickstart_ml.mdx",
    "ai_section": "## Exploring the Results\nTo preview results locally, use the following in a Jupyter notebook or Colab:\n\n```python\nmy_eval\n```\n\nYou can also export results in various formats:\n\n```python\n# my_eval.json()\n# my_eval.dict()\n# my_report.save_html(â€œfile.htmlâ€)\n```\n\nTo upload results for continuous monitoring, use:\n\n```python\nws.add_run(project.id, my_eval, include_data=False)\n```\n\nYou can view your report summary in Evidently Cloud."
  },
  {
    "title": "Data and ML checks",
    "description": "Run a simple evaluation for tabular data",
    "filename": "docs-main/quickstart_ml.mdx",
    "ai_section": "## Creating a Dashboard (Optional)\nFor tracking results over time, you can create a Dashboard. Here's how:\n\n```python\nfrom evidently.sdk.models import PanelMetric\nfrom evidently.sdk.panels import DashboardPanelPlot\n\nproject.dashboard.add_panel(\n             DashboardPanelPlot(\n                title=\"Dataset column drift\",\n                subtitle=\"Share of drifted columns\",\n                size=\"half\",\n                values=[\n                    PanelMetric(\n                        legend=\"Share\",\n                        metric=\"DriftedColumnsCount\",\n                        metric_labels={\"value_type\": \"share\"} \n                    ),\n                ],\n                plot_params={\"plot_type\": \"line\"},\n            ),\n            tab=\"Data Drift\",\n        )\n```\n\nYou can also add additional metrics, such as drift in the prediction column, to your dashboard."
  },
  {
    "title": "Data and ML checks",
    "description": "Run a simple evaluation for tabular data",
    "filename": "docs-main/quickstart_ml.mdx",
    "ai_section": "## Next Steps\nExplore additional resources to further enhance your understanding and usage of Evidently:\n\n- Check the available Evidently Metrics via the [All Metric Table](/metrics/all_metrics).\n- Learn how to add conditional tests to your reports via [Tests](/docs/library/tests).\n- Explore options for Dashboard design through [Dashboards](/docs/platform/dashboard_add_panels)."
  },
  {
    "title": "Data and ML checks",
    "description": "Run a simple evaluation for tabular data",
    "filename": "docs-main/quickstart_ml.mdx",
    "ai_section": "## Alternative: Data Summary Report\nYou can also use the `DataSummaryPreset` to generate a summary of all columns:\n\n```python\nreport = Report([\n    DataSummaryPreset() \n],\ninclude_tests=\"True\")\nmy_eval = report.run(eval_data_1, eval_data_2)\n```\n\nThis preset will auto-generate tests to check for data quality and provide core descriptive stats."
  },
  {
    "title": "Tracing",
    "description": "How to capture LLM inputs and outputs and evaluate them.",
    "filename": "docs-main/quickstart_tracing.mdx",
    "ai_section": "## 1. Introduction\nThis tutorial shows how to set up tracing for an LLM app, collect its inputs and outputs, view them in Evidently Cloud, and optionally run evaluations. The tools used include Tracely, Evidently, Evidently Cloud, and OpenAI."
  },
  {
    "title": "Tracing",
    "description": "How to capture LLM inputs and outputs and evaluate them.",
    "filename": "docs-main/quickstart_tracing.mdx",
    "ai_section": "## 2. Installation\nTo begin, install the necessary libraries using the following commands:\n\n```python\n! pip install evidently\n! pip install tracelyÂ \n! pip install openai\n```\n\nThen, import the required modules:\n\n```python\nimport os\nimport openai\nimport time\nimport uuid\nfrom tracely import init_tracing\nfrom tracely import trace_event\nfrom tracely import create_trace_event\nfrom evidently.ui.workspace import CloudWorkspace\n```\n\nIf you want to load the traced dataset back to Python and run evaluations, use the following:\n\n```python\nimport pandas as pd\nfrom evidently import Dataset\nfrom evidently import DataDefinition\nfrom evidently import Report\nfrom evidently.descriptors import *\nfrom evidently.presets import TextEvals\nfrom evidently.metrics import *\nfrom evidently.tests import *\n```"
  },
  {
    "title": "Tracing",
    "description": "How to capture LLM inputs and outputs and evaluate them.",
    "filename": "docs-main/quickstart_tracing.mdx",
    "ai_section": "## 3. Set up workspace\n### 3.1. Set up Evidently Cloud\nFollow the <CloudSignup /> snippet to set up your Evidently Cloud workspace.\n\n### 3.2. Create a Project\nUse the <CreateProject /> snippet to create a new project in Evidently Cloud.\n\n### 3.3. Get Open AI key\nSet up the OpenAI key by creating an environment variable. Visit the [Token page](https://platform.openai.com/api-keys) for your key and follow best practices for security as outlined in the [Open AI docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).\n\n```python\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n```"
  },
  {
    "title": "Tracing",
    "description": "How to capture LLM inputs and outputs and evaluate them.",
    "filename": "docs-main/quickstart_tracing.mdx",
    "ai_section": "## 4. Configure tracing\nTo set up and initialize tracing, use the following code:\n\n```python\nproject_id = str(project.id)\n\ninit_tracing(\n address=\"https://app.evidently.cloud/\",\n api_key=\"YOUR_API_TOKEN\",\n project_id=project_id,\n export_name=\"TRACING_DATASET\"\n )\n```\n\n### Important Parameters\n- **Address**: The destination backend for storing collected traces.\n- **Project ID**: The ID of the Evidently Project created previously. Get it from the [Home page](https://app.evidently.cloud/).\n- **Dataset Name**: Used to identify the Tracing dataset."
  },
  {
    "title": "Tracing",
    "description": "How to capture LLM inputs and outputs and evaluate them.",
    "filename": "docs-main/quickstart_tracing.mdx",
    "ai_section": "## 5. Trace a simple LLM app\nNow, create and trace a simple function that sends a list of questions to the LLM. First, initialize the OpenAI client with the API key:\n\n```python\nclient = openai.OpenAI(api_key=openai_api_key)\n```\n\nDefine the list of questions:\n\n```python\nquestion_list = [\n    \"What is Evidently Python library?\",\n    \"What is LLM observability?\",\n    \"How is MLOps different from LLMOps?\",\n    \"What is an LLM prompt?\",\n    \"Why should you care about LLM safety?\"\n]\n```\n\nUse the following function to interact with the LLM and trace the execution:\n\n```python\ndef qa_assistant(question):\n    system_prompt = \"You are a helpful assistant. Please answer the following question in one sentence.\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": question},\n    ]\n    return client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages).choices[0].message.content\n\n# Iterate over the list of questions\nfor question in question_list:\n    session_id = str(uuid.uuid4())\n    with create_trace_event(\"qa\", session_id=session_id) as event:\n      response = qa_assistant(question=question)\n      event.set_attribute(\"question\", question)\n      event.set_attribute(\"response\", response)\n      time.sleep(1)\n```"
  },
  {
    "title": "Tracing",
    "description": "How to capture LLM inputs and outputs and evaluate them.",
    "filename": "docs-main/quickstart_tracing.mdx",
    "ai_section": "## 6. View traces\nTo view the traces, go to Evidently Cloud, open your Project, and navigate to \"Traces\" in the left menu. You can view, sort, export, and work with the traced dataset.\n\n### Viewing Options\nYou can switch between Traces, Dataset, and Dialog view:\n\n<Tabs>\n  <Tab title=\"Dialog \">\n    ![](/images/examples/tracing_tutorial_session_view.png)\n  </Tab>\n\n  <Tab title=\"Dataset\">\n    ![](/images/examples/tracing_tutorial_dataset_view.png)\n  </Tab>\n\n  <Tab title=\"Traces\">\n    ![](/images/examples/tracing_tutorial_traces_view.png)\n  </Tab>\n</Tabs>"
  },
  {
    "title": "Tracing",
    "description": "How to capture LLM inputs and outputs and evaluate them.",
    "filename": "docs-main/quickstart_tracing.mdx",
    "ai_section": "## 7. Run an evaluation (Optional)\nYou can run evaluations on this dataset locally or in the Cloud. For local evaluations, load the dataset first:\n\n```python\ntraced_data = ws.load_dataset(dataset_id = \"YOUR_DATASET_ID\")\n```\n\nThen set up descriptors for evaluation:\n\n```python\ntraced_data.add_descriptors=[\n    SentenceCount(\"qa.response\", alias=\"SentenceCount\"),\n    TextLength(\"qa.response\", alias=\"Length\"), \n    Sentiment(\"qa.response\", alias=\"Sentiment\"), \n]\n```\n\nSummarize the results in a report and upload it to Evidently Cloud:\n\n```python\nreport = Report([\n    TextEvals()\n])\n\nmy_eval = report.run(traced_data, None)\n\nws.add_run(project.id, my_eval, include_data=True)\n```\n\nAfter running the evaluation, open the Report in your Project:\n\n![](/images/examples/tracing_tutorial_evals.png)"
  },
  {
    "title": "Tracing",
    "description": "How to capture LLM inputs and outputs and evaluate them.",
    "filename": "docs-main/quickstart_tracing.mdx",
    "ai_section": "## 8. Next Steps\nFor additional details on running evaluations, check the quickstart on [LLM evaluations](/quickstart_llm) and explore other evaluation methods. If you need help, feel free to ask in our [Discord community](https://discord.com/invite/xZjKRaNp8b)."
  },
  {
    "filename": "docs-main/snippets/cloud_signup.mdx",
    "ai_section": "## Sign Up for Evidently Cloud\nTo start using Evidently Cloud, you need to sign up for a free account. You can do this by visiting the following link: [Evidently Cloud Sign-Up](https://app.evidently.cloud/signup)."
  },
  {
    "filename": "docs-main/snippets/cloud_signup.mdx",
    "ai_section": "## Create an Organization\nUpon your first login, you will need to create an organization. This step involves obtaining an ID for your organization. For more details on how to create an organization, you can visit this link: [Create Organization](https://app.evidently.cloud/organizations)."
  },
  {
    "filename": "docs-main/snippets/cloud_signup.mdx",
    "ai_section": "## Get an API Token\nTo interact with the Evidently Cloud API, you must generate an API token. Click the **Key** icon in the left menu to generate and save your token. More information on this process can be found here: [Get API Token](https://app.evidently.cloud/token)."
  },
  {
    "filename": "docs-main/snippets/create_project.mdx",
    "ai_section": "## Connecting to Evidently Cloud\nTo connect to Evidently Cloud, use your API token in the following way:\n\n```python\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\n```"
  },
  {
    "filename": "docs-main/snippets/create_project.mdx",
    "ai_section": "## Creating a Project\nTo create a new Project within your Organization or to connect to an existing Project, use the following code:\n\n```python\nproject = ws.create_project(\"My project name\", org_id=\"YOUR_ORG_ID\")\nproject.description = \"My project description\"\nproject.save()\n\n# or project = ws.get_project(\"PROJECT_ID\")\n```\nThis allows you to manage projects effectively by either creating new ones or accessing existing ones."
  },
  {
    "title": "Adversarial testing",
    "description": "Synthetic edge cases and tricky inputs",
    "filename": "docs-main/synthetic-data/adversarial_data.mdx",
    "ai_section": "## Overview of Adversarial Tests\nAdversarial tests are designed to challenge AI models by exposing weaknesses and vulnerabilities. These inputs may attempt to bypass safety protections, trick the model into revealing sensitive information, or exploit edge cases to evaluate system robustness. Evidently Cloud allows automation of adversarial test generation based on defined risk categories."
  },
  {
    "title": "Adversarial testing",
    "description": "Synthetic edge cases and tricky inputs",
    "filename": "docs-main/synthetic-data/adversarial_data.mdx",
    "ai_section": "## Create an Adversarial Test Dataset\nYou can configure your own adversarial dataset by following a simple process within the Evidently UI."
  },
  {
    "title": "Adversarial testing",
    "description": "Synthetic edge cases and tricky inputs",
    "filename": "docs-main/synthetic-data/adversarial_data.mdx",
    "ai_section": "## Step 1: Create a Project\nTo start, either create a new project or open an existing one in the Evidently UI. Navigate to â€œDatasetsâ€ in the left menu, click â€œGenerate,â€ and select the â€œAdversarial testingâ€ option."
  },
  {
    "title": "Adversarial testing",
    "description": "Synthetic edge cases and tricky inputs",
    "filename": "docs-main/synthetic-data/adversarial_data.mdx",
    "ai_section": "## Step 2: Select a Test Scenario\nChoose a predefined adversarial scenario from the following categories:\n- Harmful content (e.g., profanity, toxicity, illegal advice)\n- Forbidden topics (e.g., financial, legal, medical queries)\n- Brand image (eliciting negative feedback on a company or product)\n- Competition (comparisons with competitor products)\n- Offers and promises (attempting to get AI to make commitments)\n- Hijacking (out-of-scope questions unrelated to intended purpose)\n- Prompt leakage (extracting system instructions or hidden prompts)"
  },
  {
    "title": "Adversarial testing",
    "description": "Synthetic edge cases and tricky inputs",
    "filename": "docs-main/synthetic-data/adversarial_data.mdx",
    "ai_section": "## Step 3: Configure the Dataset\nAfter selecting a scenario, you can provide an optional dataset name and description, set the number of inputs to generate, and customize specific categories like forbidden topics."
  },
  {
    "title": "Adversarial testing",
    "description": "Synthetic edge cases and tricky inputs",
    "filename": "docs-main/synthetic-data/adversarial_data.mdx",
    "ai_section": "## Step 4: Generate the Data\nYou have the option to:\n- Combine multiple scenarios into a single dataset, with a separate \"scenario\" column for each test case.\n- Export each scenario separately to generate individual datasets for each selected type.\n\nOnce generated, you can open, edit, or download each dataset as a CSV file or access it via the Python API using the dataset ID."
  },
  {
    "title": "Adversarial testing",
    "description": "Synthetic edge cases and tricky inputs",
    "filename": "docs-main/synthetic-data/adversarial_data.mdx",
    "ai_section": "## Dataset API\nFor more information on working with Evidently datasets, you can refer to the [Evidently datasets documentation](https://docs/platform/datasets_overview)."
  },
  {
    "title": "Create synthetic inputs",
    "description": "Generate input test cases.",
    "filename": "docs-main/synthetic-data/input_data.mdx",
    "ai_section": "## Overview of Synthetic Input Generation\nSynthetic input generation allows you to create test questions from descriptions and examples. This helps expand test coverage and evaluate how your AI system handles different types of queries. You can use this to generate test questions for RAG systems without predefined answers, create adversarial inputs by describing specific edge cases, and generate questions tailored to specific user personas for more targeted testing."
  },
  {
    "title": "Create synthetic inputs",
    "description": "Generate input test cases.",
    "filename": "docs-main/synthetic-data/input_data.mdx",
    "ai_section": "## Create Synthetic Inputs\nYou can generate example inputs specific to your LLM app context through a structured process."
  },
  {
    "title": "Create synthetic inputs",
    "description": "Generate input test cases.",
    "filename": "docs-main/synthetic-data/input_data.mdx",
    "ai_section": "## Step 1: Create a Project\nIn the Evidently UI, you can start a new Project or open an existing one. To do this:\n- Navigate to â€œDatasetsâ€ in the left menu.\n- Click â€œGenerateâ€ and select the â€œGenerate from examplesâ€ option.\n\n![Project Creation](/images/synthetic/synthetic_data_select_method.png)"
  },
  {
    "title": "Create synthetic inputs",
    "description": "Generate input test cases.",
    "filename": "docs-main/synthetic-data/input_data.mdx",
    "ai_section": "## Step 2: Describe the Scenario\nDefine the inputs you need by providing a brief description of the task and choosing how many inputs to generate. For example, if youâ€™re building a travel assistant, you might enter:\n- Description: \"Questions a person can ask when planning a trip\"\n- Example input: \"What can I do in Paris in a day?\"\n\nThis helps guide the system in generating relevant and diverse inputs. You can also provide a more detailed prompt.\n\n![Scenario Description](/images/synthetic/synthetic_data_inputs_example_prompt.png)"
  },
  {
    "title": "Create synthetic inputs",
    "description": "Generate input test cases.",
    "filename": "docs-main/synthetic-data/input_data.mdx",
    "ai_section": "## Step 3: Review the Results\nOnce you describe the scenario, the system will generate a list of input questions based on your description. You can preview and refine the generated dataset, allowing you to:\n- Use â€œMore like thisâ€ to generate additional variations.\n- Drop questions that donâ€™t fit your needs.\n- Manually edit or rephrase questions.\n\n![Review Results](/images/synthetic/synthetic_data_inputs_example_result.png)"
  },
  {
    "title": "Create synthetic inputs",
    "description": "Generate input test cases.",
    "filename": "docs-main/synthetic-data/input_data.mdx",
    "ai_section": "## Step 4: Save and Use the Dataset\nAfter finalizing your selections, you can save the dataset. Options include downloading it as a CSV file or accessing it via the Python API using the dataset ID.\n\n<Info>\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\n</Info>"
  },
  {
    "title": "Synthetic data",
    "description": "Generating test cases and datasets.",
    "filename": "docs-main/synthetic-data/introduction.mdx",
    "ai_section": "## Availability of Evidently Cloud\nThis feature is available in Evidently Cloud. Check [pricing](https://www.evidentlyai.com/pricing) details. [Reach out](https://www.evidentlyai.com/get-demo) if youâ€™d like a demo."
  },
  {
    "title": "Synthetic data",
    "description": "Generating test cases and datasets.",
    "filename": "docs-main/synthetic-data/introduction.mdx",
    "ai_section": "## Generating Synthetic Test Inputs\nEvidently Cloud lets you generate synthetic test inputs (and outputs) to evaluate your AI system. You can use it for:\n\n* **Experiments**: Create test data to see how your LLM app handles it.\n* **Regression testing**: Validate changes before deployment.\n* **Adversarial testing**: Check how your system handles tricky or unexpected inputs.\n\nOnce you generate the data, you can run it through your AI system and evaluate the results using the Evidently Cloud or Evidently Python library as usual."
  },
  {
    "title": "Synthetic data",
    "description": "Generating test cases and datasets.",
    "filename": "docs-main/synthetic-data/introduction.mdx",
    "ai_section": "## Synthetic Inputs\nGenerate inputs from description."
  },
  {
    "title": "Synthetic data",
    "description": "Generating test cases and datasets.",
    "filename": "docs-main/synthetic-data/introduction.mdx",
    "ai_section": "## RAG Dataset\nGenerate Q& A dataset from the knowledge source."
  },
  {
    "title": "Synthetic data",
    "description": "Generating test cases and datasets.",
    "filename": "docs-main/synthetic-data/introduction.mdx",
    "ai_section": "## Adversarial Tests\nGenerate inputs to test for vulnerabilities."
  },
  {
    "title": "Synthetic data",
    "description": "Generating test cases and datasets.",
    "filename": "docs-main/synthetic-data/introduction.mdx",
    "ai_section": "## Example of Generating Test Inputs\nFor example, here is how you can generate test inputs.\n\n![](/images/synthetic/datagen_travel.gif)"
  },
  {
    "title": "RAG evaluation dataset",
    "description": "Synthetic data for RAG.",
    "filename": "docs-main/synthetic-data/rag_data.mdx",
    "ai_section": "## Overview of Retrieval-Augmented Generation (RAG) Systems\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know."
  },
  {
    "title": "RAG evaluation dataset",
    "description": "Synthetic data for RAG.",
    "filename": "docs-main/synthetic-data/rag_data.mdx",
    "ai_section": "## Importance of Test Datasets\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data."
  },
  {
    "title": "RAG evaluation dataset",
    "description": "Synthetic data for RAG.",
    "filename": "docs-main/synthetic-data/rag_data.mdx",
    "ai_section": "## Create a RAG Test Dataset\nYou can generate a ground truth RAG dataset from your data source using the Evidently UI."
  },
  {
    "title": "RAG evaluation dataset",
    "description": "Synthetic data for RAG.",
    "filename": "docs-main/synthetic-data/rag_data.mdx",
    "ai_section": "## Step 1: Create a Project\nIn the Evidently UI, start a new Project or open an existing one.\n- Navigate to â€œDatasetsâ€ in the left menu.\n- Click â€œGenerateâ€ and select the â€œRAGâ€ option."
  },
  {
    "title": "RAG evaluation dataset",
    "description": "Synthetic data for RAG.",
    "filename": "docs-main/synthetic-data/rag_data.mdx",
    "ai_section": "## Step 2: Upload Your Knowledge Base\nSelect a file containing the information your AI system retrieves from. Supported formats include Markdown (.md), CSV, TXT, and PDFs. Choose how many inputs to generate.\n- Simply drop the file.\n- Choose the number of inputs to generate.\n- Decide if you want to include the context used to generate the answer.\n\n<Info>\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\n</Info>"
  },
  {
    "title": "RAG evaluation dataset",
    "description": "Synthetic data for RAG.",
    "filename": "docs-main/synthetic-data/rag_data.mdx",
    "ai_section": "## Step 3: Review the Test Cases\nYou can preview and refine the generated dataset.\n- Use â€œMore like thisâ€ to add more variations.\n- Drop rows that arenâ€™t relevant.\n- Manually edit questions or responses."
  },
  {
    "title": "RAG evaluation dataset",
    "description": "Synthetic data for RAG.",
    "filename": "docs-main/synthetic-data/rag_data.mdx",
    "ai_section": "## Step 4: Save the Dataset\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID for use in your evaluation.\n\n<Info>\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\n</Info>"
  },
  {
    "title": "Why synthetic data?",
    "description": "When do you need synthetic data in LLM evaluations.",
    "filename": "docs-main/synthetic-data/why_synthetic.mdx",
    "ai_section": "## Importance of Test Data in AI Systems\nWhen working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain just the inputs or both inputs and expected outputs (ground truth). A test dataset serves multiple purposes including running experiments, regression testing, and stress-testing."
  },
  {
    "title": "Why synthetic data?",
    "description": "When do you need synthetic data in LLM evaluations.",
    "filename": "docs-main/synthetic-data/why_synthetic.mdx",
    "ai_section": "## Functions of a Test Dataset\nYou can use a test dataset to:\n\n* Run **experiments** and track if changes improve or degrade system performance.\n* Conduct **regression testing** to ensure updates donâ€™t break what was already working.\n* **Stress-test** your system with complex or adversarial inputs to check its resilience."
  },
  {
    "title": "Why synthetic data?",
    "description": "When do you need synthetic data in LLM evaluations.",
    "filename": "docs-main/synthetic-data/why_synthetic.mdx",
    "ai_section": "## Creating Test Datasets\nYou can create test datasets manually, collect them from real or historical data, or generate them synthetically. While real data is preferable, it is not always available or sufficient to cover all cases. Public LLM benchmarks help with general model comparisons but donâ€™t reflect your specific use case, and manually writing test cases takes time and effort."
  },
  {
    "title": "Why synthetic data?",
    "description": "When do you need synthetic data in LLM evaluations.",
    "filename": "docs-main/synthetic-data/why_synthetic.mdx",
    "ai_section": "## Benefits of Synthetic Data\nSynthetic data is especially beneficial when:\n\n* You're starting from scratch and donâ€™t have real data.\n* You need to scale a manually designed dataset with more variation.\n* You want to test edge cases, adversarial inputs, or system robustness.\n* You're evaluating complex AI systems like RAG and AI agents."
  },
  {
    "title": "Why synthetic data?",
    "description": "When do you need synthetic data in LLM evaluations.",
    "filename": "docs-main/synthetic-data/why_synthetic.mdx",
    "ai_section": "## Advantages of Using Synthetic Data\nSynthetic data is not a replacement for real data or expert-designed tests; it adds variety and speeds up the process. With synthetic data, you can:\n\n* Quickly generate hundreds of structured test cases.\n* Fill gaps by adding missing scenarios and tricky inputs.\n* Create controlled variations to evaluate specific weaknesses."
  },
  {
    "title": "Why synthetic data?",
    "description": "When do you need synthetic data in LLM evaluations.",
    "filename": "docs-main/synthetic-data/why_synthetic.mdx",
    "ai_section": "## Applicability of Synthetic Data for Complex AI Systems\nSynthetic data can significantly aid in the evaluation of complex AI systems where designing test cases is challenging. For instance, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios."
  }
]