{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b894dee4-b145-4509-9ac8-a924257718f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e70a861-e953-4025-8b35-a9d3f4dd7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ae22297-c155-4569-a97f-038e60dc20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "212cf312-ce0b-480b-bdfc-a6b5d479822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f824db8-adff-4e2f-8d63-67db75b15419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5f4c570-bc16-4f98-aa8e-f5b3cccd4667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a996d-7d89-42db-9e04-c0192e9eeb21",
   "metadata": {},
   "source": [
    "# Day 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d412131d-1be3-4793-b5c0-08df681511aa",
   "metadata": {},
   "source": [
    "## Step 1. Sliding Window Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cef310da-7794-4f58-9d32-5dd1f5e8c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"\n",
    "    Split a sequence (string) into overlapping chunks.\n",
    "    Example: size=2000, step=1000\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49f36035-1f79-431b-a127-955adbda2701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 21 chunks\n",
      "In this tutorial, you will learn how to perform regression testing for LLM outputs.\n",
      "\n",
      "You can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\n",
      "\n",
      "<Info>\n",
      "  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For \n"
     ]
    }
   ],
   "source": [
    "# Example: pick a long doc from evidently_docs\n",
    "doc = evidently_docs[45]   # change index if needed\n",
    "chunks = sliding_window(doc['content'], size=2000, step=1000)\n",
    "\n",
    "print(f\"Got {len(chunks)} chunks\")\n",
    "print(chunks[0]['chunk'][:500])  # preview first 500 chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41127e5f-a2a0-470d-a7ab-e4f42e985592",
   "metadata": {},
   "source": [
    "## ✅ Step 3. Process All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32ff973a-2298-4ef2-bea6-a19e5ae04300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 575\n"
     ]
    }
   ],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, size=2000, step=1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)   # keep metadata\n",
    "    evidently_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks: {len(evidently_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603068cb-68bb-4623-b36e-013c7a9b5d0b",
   "metadata": {},
   "source": [
    "## ✅ Step 4. Paragraph Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df629abc-7554-46b6-8bae-f45833734edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 13\n",
      "['When working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain:', '* Just the inputs, or\\n* Both inputs and expected outputs (ground truth).']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_by_paragraphs(text):\n",
    "    return re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "\n",
    "# Example\n",
    "paragraphs = split_by_paragraphs(doc['content'])\n",
    "print(f\"Paragraphs: {len(paragraphs)}\")\n",
    "print(paragraphs[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fdd592-d240-4361-94a7-23f4866c6bab",
   "metadata": {},
   "source": [
    "## ✅ Step 5. Section Splitting (Markdown Headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fa71a88-2ae6-4bbb-a5b2-bce42d248690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def split_markdown_by_level(text, level=2):\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    parts = pattern.split(text)\n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        header = parts[i] + parts[i+1]\n",
    "        header = header.strip()\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "        if content:\n",
    "            section = f\"{header}\\n\\n{content}\"\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    return sections\n",
    "\n",
    "# Example\n",
    "sections = split_markdown_by_level(doc['content'], level=2)\n",
    "print(f\"Sections: {len(sections)}\")\n",
    "print(sections[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f17d6-8269-427e-b005-49b4c91ff6e3",
   "metadata": {},
   "source": [
    "## ✅ Step 6. Apply to All Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c9d31e3-773d-4366-9227-9bf0630fe259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total section chunks: 262\n"
     ]
    }
   ],
   "source": [
    "evidently_sections = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_sections.append(section_doc)\n",
    "\n",
    "print(f\"Total section chunks: {len(evidently_sections)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f5bf9-89b6-4574-b8c7-1b594a46630f",
   "metadata": {},
   "source": [
    "## AI SPLITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0dd8053f-e41e-47f5-8700-3384f921d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tokenize.py:529: RuntimeWarning: coroutine 'AbstractAgent.run' was never awaited\n",
      "  pseudomatch = _compile(PseudoToken).match(line, pos)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# intelligent_chunking.py  (notebook-friendly)\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# create client from env var (OpenAI python client)\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# --- Prompt template (you can tune this) ---\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Split the provided document into logical, self-contained sections suitable for a Q&A/FAQ system.\n",
    "Each section must be self-contained and focused on one topic. Return the output in this EXACT format,\n",
    "separating sections with a line that contains three dashes (---) on its own.\n",
    "\n",
    "For each section, include a short section title line that starts with \"##\".\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Required output format example:\n",
    "\n",
    "## Section Title\n",
    "Section text with all relevant details.\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section\n",
    "Another section text.\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "# --- Helper: call LLM (Responses API) ---\n",
    "def llm_split_text(text: str, model: str = \"gpt-4o-mini\", timeout: int = 60) -> str:\n",
    "    \"\"\"\n",
    "    Call the OpenAI Responses API and return the text output.\n",
    "    \"\"\"\n",
    "    # input may be a string; using input= with instructions + input is supported\n",
    "    resp = client.responses.create(\n",
    "        model=model,\n",
    "        input=[{\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(document=text)}],\n",
    "        # optional: max_tokens, temperature, etc.\n",
    "    )\n",
    "    # responses.create returns an object with output_text (high-level convenience)\n",
    "    return getattr(resp, \"output_text\", None) or \"\".join([c.get(\"text\",\"\") for c in resp.output[0].content])\n",
    "\n",
    "# --- Helper: parse the LLM output into section strings ---\n",
    "def parse_sections_from_output(output_text: str) -> List[str]:\n",
    "    parts = [p.strip() for p in output_text.split(\"---\")]\n",
    "    # keep parts that look non-empty and start with a heading or text\n",
    "    sections = [p for p in parts if p]\n",
    "    return sections\n",
    "\n",
    "# --- Slice a long doc into large pre-slices (to keep LLM inputs bounded) ---\n",
    "def big_slices(text: str, slice_size: int = 12000, overlap: int = 1000):\n",
    "    if slice_size <= 0 or overlap < 0 or overlap >= slice_size:\n",
    "        raise ValueError(\"slice_size must be > overlap >= 0\")\n",
    "    n = len(text)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        yield text[i:i + slice_size]\n",
    "        if i + slice_size >= n:\n",
    "            break\n",
    "        i += (slice_size - overlap)\n",
    "\n",
    "# --- Top-level intelligent chunking function ---\n",
    "def intelligent_chunking(text: str,\n",
    "                         model: str = \"gpt-4o-mini\",\n",
    "                         slice_size: int = 12000,\n",
    "                         slice_overlap: int = 1000,\n",
    "                         max_slices: int = 20,\n",
    "                         sleep_between_calls: float = 0.6):\n",
    "    \"\"\"\n",
    "    Use an LLM to split text into semantically-coherent sections.\n",
    "    For very long documents we slice into large pre-slices to avoid one huge LLM call.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    seen_hashes = set()\n",
    "\n",
    "    # If text is small enough, do a single call\n",
    "    if len(text) <= slice_size:\n",
    "        out = llm_split_text(text, model=model)\n",
    "        parsed = parse_sections_from_output(out)\n",
    "        for s in parsed:\n",
    "            h = hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "            if h not in seen_hashes:\n",
    "                seen_hashes.add(h)\n",
    "                sections.append(s)\n",
    "        return sections\n",
    "\n",
    "    # If text is long: process by big slices\n",
    "    for idx, sl in enumerate(big_slices(text, slice_size=slice_size, overlap=slice_overlap)):\n",
    "        if idx >= max_slices:\n",
    "            # avoid infinite cost — stop after max_slices\n",
    "            break\n",
    "        out = llm_split_text(sl, model=model)\n",
    "        parsed = parse_sections_from_output(out)\n",
    "        for s in parsed:\n",
    "            h = hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "            if h not in seen_hashes:\n",
    "                seen_hashes.add(h)\n",
    "                sections.append(s)\n",
    "\n",
    "        # small sleep to avoid hitting rate limits; tune as needed\n",
    "        time.sleep(sleep_between_calls)\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79817ba4-f7f1-4e40-b838-21f6da19a378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b348bd730f5f49808901c4e576e42733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m text = doc[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     sections = \u001b[43mintelligent_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mslice_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mmax_slices\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msleep_between_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLM error on doc:\u001b[39m\u001b[33m\"\u001b[39m, doc_meta.get(\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m<no filename>\u001b[39m\u001b[33m\"\u001b[39m), e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mintelligent_chunking\u001b[39m\u001b[34m(text, model, slice_size, slice_overlap, max_slices, sleep_between_calls)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idx >= max_slices:\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# avoid infinite cost — stop after max_slices\u001b[39;00m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m out = \u001b[43mllm_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m parsed = parse_sections_from_output(out)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m parsed:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mllm_split_text\u001b[39m\u001b[34m(text, model, timeout)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03mCall the OpenAI Responses API and return the text output.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# input may be a string; using input= with instructions + input is supported\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROMPT_TEMPLATE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optional: max_tokens, temperature, etc.\u001b[39;49;00m\n\u001b[32m     47\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# responses.create returns an object with output_text (high-level convenience)\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(resp, \u001b[33m\"\u001b[39m\u001b[33moutput_text\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([c.get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m resp.output[\u001b[32m0\u001b[39m].content])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:828\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    792\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    793\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    826\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    827\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# assume evidently_docs is a list of dicts with 'content' and metadata\n",
    "from tqdm.auto import tqdm\n",
    "all_ai_sections = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_meta = {k:v for k,v in doc.items() if k != \"content\"}\n",
    "    text = doc[\"content\"]\n",
    "    try:\n",
    "        sections = intelligent_chunking(text, model=\"gpt-4o-mini\",\n",
    "                                        slice_size=12000, slice_overlap=1000,\n",
    "                                        max_slices=30, sleep_between_calls=0.6)\n",
    "    except Exception as e:\n",
    "        print(\"LLM error on doc:\", doc_meta.get(\"filename\",\"<no filename>\"), e)\n",
    "        sections = []\n",
    "\n",
    "    for sec in sections:\n",
    "        entry = doc_meta.copy()\n",
    "        entry[\"ai_section\"] = sec\n",
    "        all_ai_sections.append(entry)\n",
    "\n",
    "print(\"AI sections created:\", len(all_ai_sections))\n",
    "\n",
    "# Save to a file for later ingestion\n",
    "import json\n",
    "with open(\"evidently_ai_sections.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_ai_sections, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be90b703-10c6-46cf-90c6-6d43ffaa6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>', 'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx'}, {'start': 3000, 'chunk': ' Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in t', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 2000, 'chunk': 'ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  - Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definiti', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 1000, 'chunk': ' run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n## 2. Prepare the dataset\\n\\nLet\\'s create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    [\"When does water boil?\", \"Water\\'s boiling point is 100 degrees Celsius.\"],\\n    [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  -', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}, {'start': 8000, 'chunk': 'n LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What\\'s next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx'}]\n"
     ]
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "index.fit(evidently_chunks)\n",
    "\n",
    "# Try a query\n",
    "query = \"What should be in a test dataset for AI evaluation?\"\n",
    "results = index.search(query, num_results=5)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d82017b-800f-4801-9b1e-409b0a140bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde7a70a512f46b4a811820536fad37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'chunk': \"You may need evaluations at different stages of your AI product development:\\n\\n* **Ad hoc analysis.** Spot-check the quality of your data or AI outputs.\\n\\n* **Experiments**. Test different parameters, models, or prompts and compare outcomes.\\n\\n* **Safety and adversarial testing.** Evaluate how your system handles edge cases and adversarial inputs, including on synthetic data.\\n\\n* **Regression testing.** Ensure the performance does not degrade after updates or fixes.\\n\\n* **Monitoring**. Track the response quality for production systems.\\n\\nEvidently supports all these workflows. You can run evals locally or directly on the platform.\\n\\n## Evaluations via API\\n\\n<Check>\\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\nThis is perfect for experiments, CI/CD workflows, or custom evaluation pipelines.\\n\\n![](/images/evals_flow_python.png)\\n\\n**How it works**:\\n\\n* Run Python-based evaluations on your AI outputs by generating Reports.\\n\\n* Upload results to the Evidently Platform.\\n\\n* Use the Explore feature to compare and debug results between runs.\\n\\n**Next step:** check the Quickstart for [ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n\\n## No-code evaluations\\n\\n<Check>\\n  Supported in `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\nThis option lets you run evaluations directly in the user interface. This is great for non-technical users or when you prefer to run evaluations on Evidently infrastructure.\\n\\n![](/images/evals_flow_nocode.png)\\n\\n**How it works**:\\n\\n* **Analyze CSV datasets**. Drag and drop CSV files and evaluate their contents on the Platform.\\n\\n* **Evaluate uploaded datasets**. Assess collected [traces](/docs/platform/tracing_overview) from instrumented LLM applications or any [Datasets](/docs/platform/datasets_overview) you previously uploaded or generated.\\n\\nNo-code workflows create the same Reports or Test Suites you'd generate using Python. The rest of the workflow is the same. After you run your evals with any method, you can acces\", 'title': 'Overview', 'description': 'Running evals on the platform.', 'filename': 'docs-main/docs/platform/evals_overview.mdx'}, {'start': 0, 'chunk': 'The Evidently Python library is an open-source tool designed to evaluate, test and monitor the quality of AI systems, from experimentation to production. You can use the evaluation library on its own, or as part of the [Monitoring Platform](/docs/platform/overview) (self-hosted or Evidently Cloud).\\n\\nThis page provides a conceptual overview of the Evidently library.\\n\\n# At a glance\\n\\nEvidently library covers 4 core workflows. You can these features together or standalone.\\n\\n**1. AI/ML Evaluations**\\n\\n<Check>\\n  **TL;DR**: Lots of useful AI/ML/data metrics out of the box. Exportable as scores or visual reports.\\n</Check>\\n\\nEvidently’s core capability is running evaluations on AI system inputs and outputs. It includes 100\\\\+ built-in metrics and checks, and also useful configurable templates for custom evaluations.\\n\\nYou can get raw either metrics or pass/fail test results.\\n\\nWe support metrics that make sense both for predictive ML tasks and generative LLM system outputs. Example built-in checks:\\n\\n| **Type**                  | **Example checks**                                                        |\\n| ------------------------- | ------------------------------------------------------------------------- |\\n| **🔡 Text qualities**     | Length, sentiment, special symbols, pattern  matches, etc.                |\\n| **📝 LLM output quality** | Semantic similarity, relevance, RAG faithfulness, custom LLM judges, etc. |\\n| **🛢 Data quality**       | Missing values, duplicates, min-max ranges, correlations, etc.            |\\n| **📊 Data drift**         | 20\\\\+ tests and distance metrics to detect distribution drift.             |\\n| **🎯 Classification**     | Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.        |\\n| **📈 Regression**         | MAE, ME, RMSE, error distribution, error normality, error bias, etc.      |\\n| **🗂 Ranking (inc. RAG)** | NDCG, MAP, MRR, Hit Rate, etc.                                            |\\n\\nYou can get evaluation results in multiple formats:', 'title': 'Introduction', 'description': 'Core concepts and components of the Evidently Python library.', 'filename': 'docs-main/docs/library/overview.mdx'}, {'start': 0, 'chunk': \"We’re building Evidently AI to help teams ship reliable AI products: whether it’s an ML model, an LLM app, or a complex agent workflow.\\n\\nOur tools are model-, framework-, and application-agnostic, so you can build and evaluate AI systems your way without limitations.\\n\\n## We are open-source\\n\\n[**Evidently**](https://github.com/evidentlyai/evidently) is an open-source library with over 25 million downloads, 5000+ GitHub stars, and a thriving community. It's licensed under Apache 2.0. This gives full transparency - you can see exactly how every metric works and trust the implementation. It also delivers an intuitive API designed for a great developer experience.\\n\\nThe **Evidently Platform** builds on the library with additional UI features and workflows for team collaboration. For enterprise users, we offer both Cloud and self-hosted options for full data privacy and control.\\n\\n## Evidently is very modular\\xa0\\n\\nEvidently is built to adapt to your needs without lock-ins or complex setups. It’s modular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\\n\\n* Start with local ad hoc checks.\\xa0\\n\\n* Want to share results? Add a UI to track evaluations over time.\\xa0\\n\\n* When you run evals, choose to upload raw data or only evaluation results. It’s up to you.\\xa0\\n\\n* Add monitoring as you are ready to move to production workflows.\\n\\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \\n\\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\\xa0\\n\\n## 100+ built-in evaluations\\n\\nEvidently puts evaluations and quality testing first.\\xa0\\n\\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML an\", 'title': 'Why Evidently?', 'description': 'Why choose Evidently.', 'filename': 'docs-main/faq/why_evidently.mdx'}, {'start': 0, 'chunk': \"When working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain:\\n\\n* Just the inputs, or\\n* Both inputs and expected outputs (ground truth).\\n\\nYou can use this test dataset to:\\n\\n* Run **experiments** and track if changes improve or degrade system performance.\\n* Run **regression testing** to ensure updates don’t break what was already working.\\n* **Stress-test** your system with complex or adversarial inputs to check its resilience.\\n\\n![](/images/synthetic/synthetic_experiments_img.png)\\n\\nYou can create test datasets manually, collect them from real or historical data, or generate them synthetically. While real data is best, it is not always available or sufficient to cover all cases. Public LLM benchmarks help with general model comparisons but don’t reflect your specific use case. Manually writing test cases takes time and effort.\\n\\n**Synthetic data helps here**. It’s especially useful when you are:\\n\\n* You're starting from scratch and don’t have real data.\\n* You need to scale a manually designed dataset with more variation.\\n* You want to test edge cases, adversarial inputs, or system robustness.\\n* You're evaluating complex AI systems like RAG and AI agents.\\n\\n![](/images/synthetic/synthetic_adversarial_img.png)\\n\\nSynthetic data is not a replacement for real data or expert-designed tests — it’s a way to add variety and speed up the process. With synthetic data you can:\\n\\n* Quickly generate hundreds structured test cases.\\n* Fill gaps by adding missing scenarios and tricky inputs.\\n* Create controlled variations to evaluate specific weaknesses.\\n\\nIt’s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing.\\n\\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. I\", 'title': 'Why synthetic data?', 'description': 'When do you need synthetic data in LLM evaluations.', 'filename': 'docs-main/synthetic-data/why_synthetic.mdx'}, {'start': 14000, 'chunk': ' across all inputs\\n- analyze any tabular dataset (descriptive stats, quality, drift)\\n- evaluate AI system performance (regression, classification, ranking, etc.)\\n\\nEach Report runs a computation and visualizes a set of **Metrics** and conditional **Tests.** If you pass two datasets, you get a side-by-side comparison.\\xa0\\n\\nThe easiest way to start is by using **Presets**.\\n\\n### Metric Presets\\n\\nPresets are pre-configured evaluation templates.\\n\\nThey help compute multiple related Metrics using a single line of code. Evidently has a number of **comprehensive Presets** ([see all](/metrics/all_presets)) for specific evaluation scenarios: from exploratory data analysis to AI quality assessments. For example:\\n\\n<Tabs>\\n  <Tab title=\"TextEvals\">\\n    `TextEvals` summarizes the scores from all text descriptors.\\n\\n    ![](/images/examples/llm_quickstart_report.png)\\n  </Tab>\\n  <Tab title=\"Data Drift\">\\n    `DataDriftPreset` identifies shifts in data distribution for all dataset columns.\\n\\n    ![](/images/concepts/overview_drift_report-min.png)\\n  </Tab>\\n  <Tab title=\"Data Summary\">\\n    `DataSummaryPreset` summarizes all dataset columns, generating statistics and profiles for each.\\n\\n    ![](/images/metrics/preset_datasummary_example-min.png)\\n  </Tab>\\n  <Tab title=\"Classification\">\\n    `ClassificationPreset` breaks down classification metrics and includes debugging plots.\\n\\n    ![](/images/metrics/preset_classification_example-min.png)\\n  </Tab>\\n</Tabs>\\n\\n### Metrics\\n\\nEach Preset is made of individual Metrics. You can also create your own **custom Report** by listing the `Metrics` you want to include.\\n\\n- You can combine multiple Metrics and Presets in a Report.\\xa0\\n- You can include both built-in Metrics and custom Metrics.\\n\\nBuilt-in Metrics range from simple statistics like `MeanValue` or `MissingValueCount` to complex algorithmic evals like `DriftedColumnsCount`.\\n\\nEach **Metric** computes a single value and has an optional visual representation (or several to choose from). For convenience, there ', 'title': 'Introduction', 'description': 'Core concepts and components of the Evidently Python library.', 'filename': 'docs-main/docs/library/overview.mdx'}]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from minsearch import VectorSearch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")\n",
    "\n",
    "# Encode all chunks\n",
    "evidently_embeddings = []\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d[\"chunk\"])\n",
    "    evidently_embeddings.append(v)\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "# Build the vector index\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)\n",
    "\n",
    "# Try a query\n",
    "q = embedding_model.encode(\"How do I evaluate an AI model?\")\n",
    "vector_results = evidently_vindex.search(q, num_results=5)\n",
    "print(vector_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2395277-4d12-4e8b-9553-3ddf8ef9679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>', 'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx'}, {'start': 3000, 'chunk': 's/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quic', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'docs-main/examples/LLM_judge.mdx'}, {'start': 0, 'chunk': 'You can run text evaluations using descriptors directly in the user interface.\\n\\n## 1. Prepare the Dataset\\n\\nBefore you start, create a Project and prepare the Dataset to evaluate. There are two options:\\n\\n- **Upload a CSV**. Enter the \"Dataset\" menu, click on \"Create new dataset from CSV\". Drag and drop your Dataset. You must also specify the data definition when you upload it.\\n- **Use an existing Dataset**. Select a Dataset you previously uploaded to the platform or one collected through [Tracing](tracing_overview).\\n\\n<Note>\\n  **What are Datasets?** Learn how to manage and upload [Datasets](datasets_overview) to the platform.\\n</Note>\\n\\n<Note>\\n  **What is Data Definition?** Understand how to set your dataset schema in the [Data Definition](../library/data-definition).\\n</Note>\\n\\n## 2. Start an evaluation\\n\\nWhile you are viewing the Dataset, you can click on \"Add descriptors\" on the right.\\n\\n![](/images/evals_no_code_add_descriptors-min.png)\\n\\n**(Optional) Add the LLM provider API key.** Add a token in the “Secrets” menu section if you plan to use an LLM for evaluations. You can proceed without it, using other types of evals.\\n\\n## 3. Configure the evaluation\\n\\nYou must choose which column to evaluate and how. You can choose from the following methods:\\n\\n- **Model-based**: use built-in machine learning models, like sentiment analysis.\\n- **Regular expressions**: check for specific words or patterns.\\n- **Text stats**: measure stats like the number of symbols or sentences.\\n- **LLM-based**: use external LLMs to evaluate your text data.\\n\\nSelect specific checks one by one:\\n\\n![](/images/nocode_choose_evals-min.png)\\n\\nEach evaluation result is called a **Descriptor**. No matter the method, you’ll get a label or score for every evaluated text. Some, like “Sentiment,” work instantly, while others may need setup.\\n\\n<Note>\\n  **What other evaluators are there?** Check the list of [All Descriptors](../metrics/all_descriptors).\\n</Note>\\n\\nHere are few examples of Descriptors and how to configure th', 'title': 'No code evals', 'description': 'How to evaluate your data in a no-code interface.', 'filename': 'docs-main/docs/platform/evals_no_code.mdx'}, {'start': 0, 'chunk': '<Check>\\n  Datasets are available in **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\n## What is a Dataset?\\n\\n**Datasets** are collections of data from your application used for analysis and automated checks. You can bring in existing datasets, capture live data, or create synthetic datasets.\\n\\n![](/images/dataset_llm.png)\\n\\n## How to create a Dataset?\\n\\nYou can add Datasets to the platform in multiple ways:\\n\\n* **Upload directly**. Use the UI to upload CSV files or push datasets via the Python API.\\n\\n* **Upload with Reports**. Attach datasets to Reports when running local evaluations. This is optional — you can also upload only summary metrics.\\n\\n* **Generate synthetic data**. Use built-in platform features to generate synthetic evaluation datasets.\\n\\n* **Create from Traces**. During tracing, Evidently automatically generates tabular datasets that can be used for evaluations.\\n\\n<Tip>\\n  **Where do I find the data?** To view all datasets (uploaded, synthetic, or evaluation results), go to the \"Dataset\" page in your Project menu. For raw tracing datasets, check the Tracing section.\\n</Tip>\\n\\n## Synthetic Data\\n\\nYou can synthesize evaluation datasets directly in Evidently Platform:\\n\\n* **Generate from examples or description**. Describe specific test scenarios and generate matching datasets.\\n\\n* **Generate from source documents**. Generate Q\\\\&A pairs from source documents like PDF, CSV or markdown files (great for RAG evaluations).\\n\\nAfter creating or uploading datasets, you can edit or diversify them further using the \"more like this\" feature.\\n\\n## When do you need Datasets?\\n\\nHere are common use cases for datasets in Evidently:\\n\\n* **Organize evaluation datasets**. Save curated datasets with expected inputs and optional ground truth outputs. You can bring in domain experts to collaborate on these datasets in UI, and access them programmatically for CI/CD checks.\\n\\n* **Debug evaluation results**. After you run an evaluation, view the dataset to identify and debug specific fail', 'title': 'Overview', 'description': 'Introduction to Datasets.', 'filename': 'docs-main/docs/platform/datasets_overview.mdx'}, {'start': 1000, 'chunk': 'e role of specific columns and prepare your Dataset for future evaluations.\\n  </Tab>\\n</Tabs>\\n\\n<Note>\\n  **How to create an Evidently Dataset?** Read the [Data Definition docs](../library/data-definition).\\n</Note>\\n\\n## Download the Dataset\\n\\nYou can pull the Dataset stored or generated on the platform to your local environment. For example, call the evaluation or tracing dataset to use in a CI/CD testing script.\\n\\nUse the `load_dataset` method:\\n\\n```python\\neval_dataset = ws.load_dataset(dataset_id = \"YOUR_DATASET_ID\") \\n\\n#to create as pandas dataframe\\ndf = eval_dataset.as_dataframe()\\n```\\n\\n## Include the Dataset\\n\\nYou can include Datasets when you upload Reports to the platform. This way, after running an evaluation locally you simultaneously upload:\\n\\n- the Report with evaluation result,\\n- the Dataset it was generated for, with new added scores if applicable.\\n\\nBy default, you upload only the Report.\\n\\nTo include the Dataset, use the `include_data` parameter:\\n\\n```python\\nws.add_run(project.id, data_report, include_data=True)\\n```\\n\\nCheck the docs on [running evals via API](/docs/platform/evals_api) for details.', 'title': 'Work with datasets', 'description': 'How to create, upload and manage Datasets.', 'filename': 'docs-main/docs/platform/datasets_workflow.mdx'}, {'start': 0, 'chunk': 'To evaluate text data, like LLM inputs and outputs, you create **Descriptors**. This is a universal interface for all evals - from text statistics to LLM judges.\\n\\nEach descriptor computes a score or label per row of your dataset. You can combine multiple descriptors and set optional pass/fail conditions. You can use built-in descriptors or create custom ones using LLM prompts or Python.\\n\\nFor a general introduction, check [Core Concepts](/docs/library/overview). You can also refer to the [LLM quickstart](quickstart_llm) for a minimal example.\\n\\n## Basic flow\\n\\n<Accordion title=\"Generate toy data\" defaultOpen={false}>\\n  Use this code snippet to create sample data for testing:\\n\\n  ```python\\n  import pandas as pd\\n  \\n  data = [\\n      [\"What is the chemical symbol for gold?\", \"The chemical symbol for gold is Au.\"],\\n      [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n      [\"Tell me a joke.\", \"Why don\\'t programmers like nature? It has too many bugs!\"],\\n      [\"What is the boiling point of water?\", \"The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit).\"],\\n      [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n      [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n      [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework. You might want to consult your teacher for help.\"],\\n      [\"How many states are there in the USA?\", \"There are 50 states in the USA.\"],\\n      [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n      [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n  ]\\n  \\n  # Columns\\n  columns = [\"question\", \"answer\"]\\n  \\n  # Creating the DataFrame\\n  df = pd.DataFrame(data, column', 'title': 'Descriptors', 'description': 'How to run evaluations for text data.', 'filename': 'docs-main/docs/library/descriptors.mdx'}, {'start': 10000, 'chunk': 'Dataset` object. This allows attaching extra meta-information so that your data is processed correctly.\\n\\nThis is needed because some evaluations may require specific columns or data types present. For example, to evaluate classification quality, you need both predictions and actual labels. To specify where they are located in your table, you can map the data schema using [Data Definition](/docs/library/data_definition).\\n\\n3. **[Optional] Preparing two datasets**. Typically you evaluate a single (`current` ) dataset. Optionally, you can prepare a second (`reference`) dataset that will be used during the evaluation. Both must have identical structures.\\n\\n![](/images/datasets_input_data_two.png)\\n\\nWhen to use two datasets:\\n\\n- **Side-by-side comparison**. This lets you compare outputs or data quality across two periods, prompt/model versions, etc. in a single Report.\\n- **Data drift detection. (Required)**. You can detect distribution shifts by comparing datasets, such as this week’s data to the previous one.\\n- **Simplify test setup**. You can automatically generate test conditions (e.g., min-max ranges) from the reference dataset without manual configuration.\\n\\n<Info>\\n  **Data sampling**. For large datasets (millions of rows), evals can take some time. The depends on:\\n\\n  - the specific evaluation: some are more computationally intensive than others\\n  - your dataset: e.g., if you run column-level evals and have lots of columns\\n  - your infrastructure: data is processed in-memory.\\n\\n  If the computation takes too long, it’s often more efficient to use samples. For example, in data drift detection, you can apply random or stratified sampling.\\n</Info>\\n\\nOnce your `Dataset` is ready, you can run evaluations. You can either:\\n\\n- Add `descriptors` to your dataset, and then compute a summary Report.\\n- Compute a Report directly over raw data.\\n\\n## Descriptors\\n\\nTo evaluate text data and LLM outputs, you need `Descriptors`.\\n\\nA **Descriptor** is a _row-level_ score or label that assesses a', 'title': 'Introduction', 'description': 'Core concepts and components of the Evidently Python library.', 'filename': 'docs-main/docs/library/overview.mdx'}]\n"
     ]
    }
   ],
   "source": [
    "def text_search(query):\n",
    "    return index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return evidently_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    seen = set()\n",
    "    final = []\n",
    "    for r in text_results + vector_results:\n",
    "        fid = r.get(\"filename\")\n",
    "        if fid not in seen:\n",
    "            seen.add(fid)\n",
    "            final.append(r)\n",
    "    return final\n",
    "\n",
    "print(hybrid_search(\"How can I evaluate my dataset?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00dba30a-5026-480b-8fb4-d34f315140ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query: str):\n",
    "    return faq_index.search(query, num_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "823d8d87-d709-4c68-a809-6aee06e24316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c26795b-26c3-45db-a958-27d31d7dcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.\n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If search gives results, base your answer on them.  \n",
    "If search doesn’t help, give general guidance.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ce66c05-33b5-407d-9288-7679fdda2ade",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faq_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mI just discovered the course, can I join now?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(user_prompt=question)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\abstract.py:218\u001b[39m, in \u001b[36mAbstractAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, deferred_tool_results, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, event_stream_handler)\u001b[39m\n\u001b[32m    204\u001b[39m event_stream_handler = event_stream_handler \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_stream_handler\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    207\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    208\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m     toolsets=toolsets,\n\u001b[32m    217\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    220\u001b[39m             \u001b[38;5;28mself\u001b[39m.is_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_call_tools_node(node)\n\u001b[32m    221\u001b[39m         ):\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m node.stream(agent_run.ctx) \u001b[38;5;28;01mas\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\run.py:149\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    147\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n\u001b[32m    148\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(node=next_node):\n\u001b[32m    151\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\graph.py:758\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\graph.py:731\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    729\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    730\u001b[39m         ctx = GraphRunContext(state=\u001b[38;5;28mself\u001b[39m.state, deps=\u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    734\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:524\u001b[39m, in \u001b[36mCallToolsNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    522\u001b[39m     \u001b[38;5;28mself\u001b[39m, ctx: GraphRunContext[GraphAgentState, GraphAgentDeps[DepsT, NodeRunEndT]]\n\u001b[32m    523\u001b[39m ) -> ModelRequestNode[DepsT, NodeRunEndT] | End[result.FinalResult[NodeRunEndT]]:\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(ctx):\n\u001b[32m    525\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._next_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mthe stream should set `self._next_node` before it ends\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:217\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aexit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:538\u001b[39m, in \u001b[36mCallToolsNode.stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m stream\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# Run the stream to completion if it was not finished:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _event \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    539\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:623\u001b[39m, in \u001b[36mCallToolsNode._run_stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    619\u001b[39m             \u001b[38;5;28mself\u001b[39m._next_node = ModelRequestNode[DepsT, NodeRunEndT](_messages.ModelRequest(parts=[e.tool_retry]))\n\u001b[32m    621\u001b[39m     \u001b[38;5;28mself\u001b[39m._events_iterator = _run_stream()\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events_iterator:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:577\u001b[39m, in \u001b[36mCallToolsNode._run_stream.<locals>._run_stream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_calls:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_tool_calls(ctx, tool_calls):\n\u001b[32m    578\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m text:\n\u001b[32m    580\u001b[39m         \u001b[38;5;66;03m# No events are emitted during the handling of text responses, so we don't need to yield anything\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:639\u001b[39m, in \u001b[36mCallToolsNode._handle_tool_calls\u001b[39m\u001b[34m(self, ctx, tool_calls)\u001b[39m\n\u001b[32m    636\u001b[39m output_parts: \u001b[38;5;28mlist\u001b[39m[_messages.ModelRequestPart] = []\n\u001b[32m    637\u001b[39m output_final_result: deque[result.FinalResult[NodeRunEndT]] = deque(maxlen=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m process_tool_calls(\n\u001b[32m    640\u001b[39m     tool_manager=ctx.deps.tool_manager,\n\u001b[32m    641\u001b[39m     tool_calls=tool_calls,\n\u001b[32m    642\u001b[39m     tool_call_results=\u001b[38;5;28mself\u001b[39m.tool_call_results,\n\u001b[32m    643\u001b[39m     final_result=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    644\u001b[39m     ctx=ctx,\n\u001b[32m    645\u001b[39m     output_parts=output_parts,\n\u001b[32m    646\u001b[39m     output_final_result=output_final_result,\n\u001b[32m    647\u001b[39m ):\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_final_result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:817\u001b[39m, in \u001b[36mprocess_tool_calls\u001b[39m\u001b[34m(tool_manager, tool_calls, tool_call_results, final_result, ctx, output_parts, output_final_result)\u001b[39m\n\u001b[32m    814\u001b[39m deferred_calls: \u001b[38;5;28mdict\u001b[39m[Literal[\u001b[33m'\u001b[39m\u001b[33mexternal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33munapproved\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mlist\u001b[39m[_messages.ToolCallPart]] = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m calls_to_run:\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m _call_tools(\n\u001b[32m    818\u001b[39m         tool_manager=tool_manager,\n\u001b[32m    819\u001b[39m         tool_calls=calls_to_run,\n\u001b[32m    820\u001b[39m         tool_call_results=calls_to_run_results,\n\u001b[32m    821\u001b[39m         tracer=ctx.deps.tracer,\n\u001b[32m    822\u001b[39m         usage_limits=ctx.deps.usage_limits,\n\u001b[32m    823\u001b[39m         output_parts=output_parts,\n\u001b[32m    824\u001b[39m         output_deferred_calls=deferred_calls,\n\u001b[32m    825\u001b[39m     ):\n\u001b[32m    826\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    828\u001b[39m \u001b[38;5;66;03m# Finally, we handle deferred tool calls (unless they were already included in the run because results were provided)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:931\u001b[39m, in \u001b[36m_call_tools\u001b[39m\u001b[34m(tool_manager, tool_calls, tool_call_results, tracer, usage_limits, output_parts, output_deferred_calls)\u001b[39m\n\u001b[32m    929\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m    930\u001b[39m                 index = tasks.index(task)\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m event := \u001b[38;5;28;01mawait\u001b[39;00m handle_call_or_result(coro_or_task=task, index=index):\n\u001b[32m    932\u001b[39m                     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    934\u001b[39m \u001b[38;5;66;03m# We append the results at the end, rather than as they are received, to retain a consistent ordering\u001b[39;00m\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# This is mostly just to simplify testing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:896\u001b[39m, in \u001b[36m_call_tools.<locals>.handle_call_or_result\u001b[39m\u001b[34m(coro_or_task, index)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_call_or_result\u001b[39m(\n\u001b[32m    888\u001b[39m     coro_or_task: Awaitable[\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mtuple\u001b[39m[_messages.ToolReturnPart | _messages.RetryPromptPart, _messages.UserPromptPart | \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m   (...)\u001b[39m\u001b[32m    892\u001b[39m     index: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    893\u001b[39m ) -> _messages.HandleResponseEvent | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    895\u001b[39m         tool_part, tool_user_part = (\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m             (\u001b[38;5;28;01mawait\u001b[39;00m coro_or_task) \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(coro_or_task) \u001b[38;5;28;01melse\u001b[39;00m coro_or_task.result()\n\u001b[32m    897\u001b[39m         )\n\u001b[32m    898\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.CallDeferred:\n\u001b[32m    899\u001b[39m         deferred_calls_by_index[index] = \u001b[33m'\u001b[39m\u001b[33mexternal\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:951\u001b[39m, in \u001b[36m_call_tool\u001b[39m\u001b[34m(tool_manager, tool_call, tool_call_result, usage_limits)\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    950\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_call_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m         tool_result = \u001b[38;5;28;01mawait\u001b[39;00m tool_manager.handle_call(tool_call, usage_limits=usage_limits)\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_call_result, ToolApproved):\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m tool_call_result.override_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_tool_manager.py:112\u001b[39m, in \u001b[36mToolManager.handle_call\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors, usage_limits)\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_tool(call, allow_partial, wrap_validation_errors, count_tool_usage=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_tool_traced(\n\u001b[32m    113\u001b[39m         call,\n\u001b[32m    114\u001b[39m         allow_partial,\n\u001b[32m    115\u001b[39m         wrap_validation_errors,\n\u001b[32m    116\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx.tracer,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx.trace_include_content,\n\u001b[32m    118\u001b[39m         usage_limits,\n\u001b[32m    119\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_tool_manager.py:237\u001b[39m, in \u001b[36mToolManager._call_tool_traced\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors, tracer, include_content, usage_limits)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\u001b[33m'\u001b[39m\u001b[33mrunning tool\u001b[39m\u001b[33m'\u001b[39m, attributes=span_attributes) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         tool_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_tool(call, allow_partial, wrap_validation_errors, usage_limits)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ToolRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    239\u001b[39m         part = e.tool_retry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_tool_manager.py:163\u001b[39m, in \u001b[36mToolManager._call_tool\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors, usage_limits, count_tool_usage)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m usage_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m count_tool_usage:\n\u001b[32m    161\u001b[39m     usage_limits.check_before_tool_call(\u001b[38;5;28mself\u001b[39m.ctx.usage)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.toolset.call_tool(name, args_dict, ctx, tool)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m count_tool_usage:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m.ctx.usage.tool_calls += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\toolsets\\combined.py:90\u001b[39m, in \u001b[36mCombinedToolset.call_tool\u001b[39m\u001b[34m(self, name, tool_args, ctx, tool)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_tool\u001b[39m(\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, tool_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]\n\u001b[32m     88\u001b[39m ) -> Any:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, _CombinedToolsetTool)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m tool.source_toolset.call_tool(name, tool_args, ctx, tool.source_tool)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\toolsets\\function.py:344\u001b[39m, in \u001b[36mFunctionToolset.call_tool\u001b[39m\u001b[34m(self, name, tool_args, ctx, tool)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_tool\u001b[39m(\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, tool_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]\n\u001b[32m    342\u001b[39m ) -> Any:\n\u001b[32m    343\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, FunctionToolsetTool)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m tool.call_func(tool_args, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_function_schema.py:55\u001b[39m, in \u001b[36mFunctionSchema.call\u001b[39m\u001b[34m(self, args_dict, ctx)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     function = cast(Callable[[Any], \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mself\u001b[39m.function)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(function, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_utils.py:47\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_in_executor\u001b[39m(func: Callable[_P, _R], *args: _P.args, **kwargs: _P.kwargs) -> _R:\n\u001b[32m     46\u001b[39m     wrapped_func = partial(func, *args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_sync(wrapped_func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\anyio\\to_thread.py:56\u001b[39m, in \u001b[36mrun_sync\u001b[39m\u001b[34m(func, abandon_on_cancel, cancellable, limiter, *args)\u001b[39m\n\u001b[32m     48\u001b[39m     abandon_on_cancel = cancellable\n\u001b[32m     49\u001b[39m     warn(\n\u001b[32m     50\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `cancellable=` keyword argument to `anyio.to_thread.run_sync` is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdeprecated since AnyIO 4.1.0; use `abandon_on_cancel=` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m     53\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m     54\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().run_sync_in_worker_thread(\n\u001b[32m     57\u001b[39m     func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:2485\u001b[39m, in \u001b[36mAsyncIOBackend.run_sync_in_worker_thread\u001b[39m\u001b[34m(cls, func, args, abandon_on_cancel, limiter)\u001b[39m\n\u001b[32m   2482\u001b[39m     worker_scope = scope._parent_scope\n\u001b[32m   2484\u001b[39m worker.queue.put_nowait((context, func, args, future, worker_scope))\n\u001b[32m-> \u001b[39m\u001b[32m2485\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:976\u001b[39m, in \u001b[36mWorkerThread.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    974\u001b[39m threadlocals.current_cancel_scope = cancel_scope\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m     result = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    978\u001b[39m     exception = exc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtext_search\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtext_search\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) -> List[Any]:\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    Perform a text-based search on the FAQ index.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[33;03m        List[Any]: A list of up to 5 search results.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfaq_index\u001b[49m.search(query, num_results=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'faq_index' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "138c150e-236a-4677-b82c-582e1b68211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "# Extract to data/faq\n",
    "extract_path = \"data/faq\"\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "zip_file.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01721e7e-714a-46b1-a3a3-9ab321f6094e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader\n",
    "\n",
    "# Load documents\n",
    "docs = SimpleDirectoryReader(\"data/faq/faq-main\").load_data()\n",
    "\n",
    "# Build index\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "# Persist\n",
    "index.storage_context.persist(\"storage/faq_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "884145e8-59d2-4d7d-ab71-655a4b8cc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3d17727-61db-461c-9140-5c6fd2ba1fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Directory data/docs does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Load documents from your docs folder\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m docs = \u001b[43mSimpleDirectoryReader\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/docs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.load_data()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 2. Build index\u001b[39;00m\n\u001b[32m      7\u001b[39m docs_index = VectorStoreIndex.from_documents(docs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\llama_index\\core\\readers\\file\\base.py:295\u001b[39m, in \u001b[36mSimpleDirectoryReader.__init__\u001b[39m\u001b[34m(self, input_dir, input_files, exclude, exclude_hidden, exclude_empty, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata, raise_on_error, fs)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m input_dir:\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fs.isdir(input_dir):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_dir = _Path(input_dir)\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m.exclude = exclude\n",
      "\u001b[31mValueError\u001b[39m: Directory data/docs does not exist."
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n",
    "\n",
    "# 1. Load documents from your docs folder\n",
    "docs = SimpleDirectoryReader(\"data/docs\").load_data()\n",
    "\n",
    "# 2. Build index\n",
    "docs_index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "# 3. (Optional) Persist index\n",
    "docs_index.storage_context.persist(persist_dir=\"storage/docs_index\")\n",
    "\n",
    "# 4. Reload later if already saved\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage/docs_index\")\n",
    "docs_index = load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e164ea7-2232-422d-b2ea-610d0b4eba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Search through the documentation index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: Up to 5 relevant results from the documentation index.\n",
    "    \"\"\"\n",
    "    return docs_index.search(query, num_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a703662-0fbc-439f-b019-1fb24071eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that answers user questions\n",
    "based on the documentation provided. Use the doc_search tool when needed.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"docs_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[doc_search],   # tools you defined\n",
    "    model=\"gpt-4o-mini\"   # or \"gpt-4o\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b426b70d-f93e-41fa-8adc-7dc23a40b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Where to store docs\n",
    "docs_dir = \"data/docs\"\n",
    "\n",
    "# Download FAQ repo if not already there\n",
    "if not os.path.exists(docs_dir):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    url = \"https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Unzip into data/docs\n",
    "    with zipfile.ZipFile(BytesIO(resp.content)) as zf:\n",
    "        zf.extractall(\"data\")\n",
    "    \n",
    "    # Move extracted content into data/docs\n",
    "    extracted_folder = \"data/faq-main\"   # GitHub repo unzips as faq-main\n",
    "    if os.path.exists(extracted_folder):\n",
    "        os.rename(extracted_folder, docs_dir)\n",
    "\n",
    "# Load documents\n",
    "docs = SimpleDirectoryReader(docs_dir).load_data()\n",
    "\n",
    "# Build index\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0d8697d-0c18-4512-a3c9-5f5c90a80918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mHow do I set up the environment for this project?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(user_prompt=question)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\abstract.py:218\u001b[39m, in \u001b[36mAbstractAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, deferred_tool_results, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, event_stream_handler)\u001b[39m\n\u001b[32m    204\u001b[39m event_stream_handler = event_stream_handler \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_stream_handler\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    207\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    208\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m     toolsets=toolsets,\n\u001b[32m    217\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    220\u001b[39m             \u001b[38;5;28mself\u001b[39m.is_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_call_tools_node(node)\n\u001b[32m    221\u001b[39m         ):\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m node.stream(agent_run.ctx) \u001b[38;5;28;01mas\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\run.py:149\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    147\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n\u001b[32m    148\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(node=next_node):\n\u001b[32m    151\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\graph.py:758\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_graph\\graph.py:731\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    729\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    730\u001b[39m         ctx = GraphRunContext(state=\u001b[38;5;28mself\u001b[39m.state, deps=\u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    734\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:524\u001b[39m, in \u001b[36mCallToolsNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    522\u001b[39m     \u001b[38;5;28mself\u001b[39m, ctx: GraphRunContext[GraphAgentState, GraphAgentDeps[DepsT, NodeRunEndT]]\n\u001b[32m    523\u001b[39m ) -> ModelRequestNode[DepsT, NodeRunEndT] | End[result.FinalResult[NodeRunEndT]]:\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(ctx):\n\u001b[32m    525\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._next_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mthe stream should set `self._next_node` before it ends\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:217\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aexit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:538\u001b[39m, in \u001b[36mCallToolsNode.stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m stream\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# Run the stream to completion if it was not finished:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _event \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    539\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:623\u001b[39m, in \u001b[36mCallToolsNode._run_stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    619\u001b[39m             \u001b[38;5;28mself\u001b[39m._next_node = ModelRequestNode[DepsT, NodeRunEndT](_messages.ModelRequest(parts=[e.tool_retry]))\n\u001b[32m    621\u001b[39m     \u001b[38;5;28mself\u001b[39m._events_iterator = _run_stream()\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events_iterator:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:577\u001b[39m, in \u001b[36mCallToolsNode._run_stream.<locals>._run_stream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_calls:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_tool_calls(ctx, tool_calls):\n\u001b[32m    578\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m text:\n\u001b[32m    580\u001b[39m         \u001b[38;5;66;03m# No events are emitted during the handling of text responses, so we don't need to yield anything\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:639\u001b[39m, in \u001b[36mCallToolsNode._handle_tool_calls\u001b[39m\u001b[34m(self, ctx, tool_calls)\u001b[39m\n\u001b[32m    636\u001b[39m output_parts: \u001b[38;5;28mlist\u001b[39m[_messages.ModelRequestPart] = []\n\u001b[32m    637\u001b[39m output_final_result: deque[result.FinalResult[NodeRunEndT]] = deque(maxlen=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m process_tool_calls(\n\u001b[32m    640\u001b[39m     tool_manager=ctx.deps.tool_manager,\n\u001b[32m    641\u001b[39m     tool_calls=tool_calls,\n\u001b[32m    642\u001b[39m     tool_call_results=\u001b[38;5;28mself\u001b[39m.tool_call_results,\n\u001b[32m    643\u001b[39m     final_result=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    644\u001b[39m     ctx=ctx,\n\u001b[32m    645\u001b[39m     output_parts=output_parts,\n\u001b[32m    646\u001b[39m     output_final_result=output_final_result,\n\u001b[32m    647\u001b[39m ):\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_final_result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:817\u001b[39m, in \u001b[36mprocess_tool_calls\u001b[39m\u001b[34m(tool_manager, tool_calls, tool_call_results, final_result, ctx, output_parts, output_final_result)\u001b[39m\n\u001b[32m    814\u001b[39m deferred_calls: \u001b[38;5;28mdict\u001b[39m[Literal[\u001b[33m'\u001b[39m\u001b[33mexternal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33munapproved\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mlist\u001b[39m[_messages.ToolCallPart]] = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m calls_to_run:\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m _call_tools(\n\u001b[32m    818\u001b[39m         tool_manager=tool_manager,\n\u001b[32m    819\u001b[39m         tool_calls=calls_to_run,\n\u001b[32m    820\u001b[39m         tool_call_results=calls_to_run_results,\n\u001b[32m    821\u001b[39m         tracer=ctx.deps.tracer,\n\u001b[32m    822\u001b[39m         usage_limits=ctx.deps.usage_limits,\n\u001b[32m    823\u001b[39m         output_parts=output_parts,\n\u001b[32m    824\u001b[39m         output_deferred_calls=deferred_calls,\n\u001b[32m    825\u001b[39m     ):\n\u001b[32m    826\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    828\u001b[39m \u001b[38;5;66;03m# Finally, we handle deferred tool calls (unless they were already included in the run because results were provided)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:931\u001b[39m, in \u001b[36m_call_tools\u001b[39m\u001b[34m(tool_manager, tool_calls, tool_call_results, tracer, usage_limits, output_parts, output_deferred_calls)\u001b[39m\n\u001b[32m    929\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m    930\u001b[39m                 index = tasks.index(task)\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m event := \u001b[38;5;28;01mawait\u001b[39;00m handle_call_or_result(coro_or_task=task, index=index):\n\u001b[32m    932\u001b[39m                     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    934\u001b[39m \u001b[38;5;66;03m# We append the results at the end, rather than as they are received, to retain a consistent ordering\u001b[39;00m\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# This is mostly just to simplify testing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:896\u001b[39m, in \u001b[36m_call_tools.<locals>.handle_call_or_result\u001b[39m\u001b[34m(coro_or_task, index)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_call_or_result\u001b[39m(\n\u001b[32m    888\u001b[39m     coro_or_task: Awaitable[\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mtuple\u001b[39m[_messages.ToolReturnPart | _messages.RetryPromptPart, _messages.UserPromptPart | \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m   (...)\u001b[39m\u001b[32m    892\u001b[39m     index: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    893\u001b[39m ) -> _messages.HandleResponseEvent | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    895\u001b[39m         tool_part, tool_user_part = (\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m             (\u001b[38;5;28;01mawait\u001b[39;00m coro_or_task) \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(coro_or_task) \u001b[38;5;28;01melse\u001b[39;00m coro_or_task.result()\n\u001b[32m    897\u001b[39m         )\n\u001b[32m    898\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.CallDeferred:\n\u001b[32m    899\u001b[39m         deferred_calls_by_index[index] = \u001b[33m'\u001b[39m\u001b[33mexternal\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_agent_graph.py:951\u001b[39m, in \u001b[36m_call_tool\u001b[39m\u001b[34m(tool_manager, tool_call, tool_call_result, usage_limits)\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    950\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_call_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m         tool_result = \u001b[38;5;28;01mawait\u001b[39;00m tool_manager.handle_call(tool_call, usage_limits=usage_limits)\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_call_result, ToolApproved):\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m tool_call_result.override_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_tool_manager.py:112\u001b[39m, in \u001b[36mToolManager.handle_call\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors, usage_limits)\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_tool(call, allow_partial, wrap_validation_errors, count_tool_usage=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_tool_traced(\n\u001b[32m    113\u001b[39m         call,\n\u001b[32m    114\u001b[39m         allow_partial,\n\u001b[32m    115\u001b[39m         wrap_validation_errors,\n\u001b[32m    116\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx.tracer,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx.trace_include_content,\n\u001b[32m    118\u001b[39m         usage_limits,\n\u001b[32m    119\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_tool_manager.py:237\u001b[39m, in \u001b[36mToolManager._call_tool_traced\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors, tracer, include_content, usage_limits)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\u001b[33m'\u001b[39m\u001b[33mrunning tool\u001b[39m\u001b[33m'\u001b[39m, attributes=span_attributes) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         tool_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_tool(call, allow_partial, wrap_validation_errors, usage_limits)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ToolRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    239\u001b[39m         part = e.tool_retry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_tool_manager.py:163\u001b[39m, in \u001b[36mToolManager._call_tool\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors, usage_limits, count_tool_usage)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m usage_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m count_tool_usage:\n\u001b[32m    161\u001b[39m     usage_limits.check_before_tool_call(\u001b[38;5;28mself\u001b[39m.ctx.usage)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.toolset.call_tool(name, args_dict, ctx, tool)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m count_tool_usage:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m.ctx.usage.tool_calls += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\toolsets\\combined.py:90\u001b[39m, in \u001b[36mCombinedToolset.call_tool\u001b[39m\u001b[34m(self, name, tool_args, ctx, tool)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_tool\u001b[39m(\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, tool_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]\n\u001b[32m     88\u001b[39m ) -> Any:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, _CombinedToolsetTool)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m tool.source_toolset.call_tool(name, tool_args, ctx, tool.source_tool)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\toolsets\\function.py:344\u001b[39m, in \u001b[36mFunctionToolset.call_tool\u001b[39m\u001b[34m(self, name, tool_args, ctx, tool)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_tool\u001b[39m(\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, tool_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]\n\u001b[32m    342\u001b[39m ) -> Any:\n\u001b[32m    343\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, FunctionToolsetTool)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m tool.call_func(tool_args, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_function_schema.py:55\u001b[39m, in \u001b[36mFunctionSchema.call\u001b[39m\u001b[34m(self, args_dict, ctx)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     function = cast(Callable[[Any], \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mself\u001b[39m.function)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(function, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\pydantic_ai\\_utils.py:47\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_in_executor\u001b[39m(func: Callable[_P, _R], *args: _P.args, **kwargs: _P.kwargs) -> _R:\n\u001b[32m     46\u001b[39m     wrapped_func = partial(func, *args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_sync(wrapped_func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\anyio\\to_thread.py:56\u001b[39m, in \u001b[36mrun_sync\u001b[39m\u001b[34m(func, abandon_on_cancel, cancellable, limiter, *args)\u001b[39m\n\u001b[32m     48\u001b[39m     abandon_on_cancel = cancellable\n\u001b[32m     49\u001b[39m     warn(\n\u001b[32m     50\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `cancellable=` keyword argument to `anyio.to_thread.run_sync` is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdeprecated since AnyIO 4.1.0; use `abandon_on_cancel=` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m     53\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m     54\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().run_sync_in_worker_thread(\n\u001b[32m     57\u001b[39m     func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:2485\u001b[39m, in \u001b[36mAsyncIOBackend.run_sync_in_worker_thread\u001b[39m\u001b[34m(cls, func, args, abandon_on_cancel, limiter)\u001b[39m\n\u001b[32m   2482\u001b[39m     worker_scope = scope._parent_scope\n\u001b[32m   2484\u001b[39m worker.queue.put_nowait((context, func, args, future, worker_scope))\n\u001b[32m-> \u001b[39m\u001b[32m2485\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\aihero\\course\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:976\u001b[39m, in \u001b[36mWorkerThread.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    974\u001b[39m threadlocals.current_cancel_scope = cancel_scope\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m     result = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    978\u001b[39m     exception = exc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mdoc_search\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdoc_search\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) -> List[Any]:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Search through the documentation index.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33;03m        List[Any]: Up to 5 relevant results from the documentation index.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdocs_index\u001b[49m.search(query, num_results=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'docs_index' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"How do I set up the environment for this project?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd598d8-e9b2-48e1-a187-646f0ae8166c",
   "metadata": {},
   "source": [
    "## Day 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9737ab94-bba7-4524-9d9f-6eb6e7e5fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghada\\aihero\\course\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "325a9d5a-b026-404b-ae44-2e0b33a4bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "# Create a query engine once from your index\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the documentation index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of search results from the documentation index.\n",
    "    \"\"\"\n",
    "    response = query_engine.query(query)\n",
    "    return [str(response)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4ca85ec-38b8-4588-98e1-a678878d770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You can join the bootcamp by visiting the DataTalks.Club website and enrolling in the specific course you are interested in, such as Data Engineering Zoomcamp, Machine Learning Zoomcamp, or MLOps Zoomcamp.']\n"
     ]
    }
   ],
   "source": [
    "# Example: run a search\n",
    "results = text_search(\"How do I join the bootcamp?\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9aa27568-a2b8-4d20-ad56-41c6d74dc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that answers questions about the DataTalks.Club documentation.\n",
    "Use the text_search tool when you need to look up information.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"docs_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],   # 👈 give the agent your tool\n",
    "    model=\"gpt-4o-mini\"    # or another model you prefer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "afb6f88f-0ae9-4857-9056-74480ff9991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: Yes, you can join the course by enrolling in the available courses listed on the DataTalks.Club FAQ page.\n",
      "\n",
      "--- Messages ---\n",
      "ModelRequest(parts=[UserPromptPart(content='I just discovered the course, can I join now?', timestamp=datetime.datetime(2025, 9, 30, 15, 12, 19, 561129, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\nUse the text_search tool when you need to look up information.')\n",
      "ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"joining the course\"}', tool_call_id='call_HFI3AJg5dNEWaKr2cz8LMoOZ')], usage=RequestUsage(input_tokens=127, output_tokens=16, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 30, 15, 12, 20, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CLWJYIFLwBTZmqgN5AxxfNcgtmIrJ', finish_reason='tool_call')\n",
      "ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=['You can join the DataTalks.Club courses by enrolling in the available courses listed on the DataTalks.Club FAQ page.'], tool_call_id='call_HFI3AJg5dNEWaKr2cz8LMoOZ', timestamp=datetime.datetime(2025, 9, 30, 15, 12, 22, 797875, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\nUse the text_search tool when you need to look up information.')\n",
      "ModelResponse(parts=[TextPart(content='Yes, you can join the course by enrolling in the available courses listed on the DataTalks.Club FAQ page.')], usage=RequestUsage(input_tokens=180, output_tokens=25, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 30, 15, 12, 23, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-CLWJb1GYGsorP2nBCMOrJmUUyGu7Z', finish_reason='stop')\n"
     ]
    }
   ],
   "source": [
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "# If you're in Jupyter/Notebook, don't use asyncio.run()\n",
    "result = await agent.run(user_prompt=question)\n",
    "\n",
    "# In v1.0.11, the final text is available at .output\n",
    "print(\"Final Answer:\", result.output)\n",
    "\n",
    "# To debug the internal flow:\n",
    "print(\"\\n--- Messages ---\")\n",
    "for msg in result.new_messages():\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a0194625-51a5-4095-8536-22314eaf80b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: I couldn't find any specific information about the homework deadlines in the documentation. It might be helpful to check the course materials or announcements directly related to your class.\n"
     ]
    }
   ],
   "source": [
    "question = \"When are the deadlines for the homework?\"\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(\"Final Answer:\", result.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1f66e-72e3-4724-ac0d-ef1ea9874326",
   "metadata": {},
   "source": [
    "## Day 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4663b13b-65c8-42b9-8435-e28ed292025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import secrets\n",
    "\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source=\"user\"):\n",
    "    # Convert Pydantic AI messages to plain dicts\n",
    "    serialized_messages = []\n",
    "    for m in messages:\n",
    "        if hasattr(m, \"dict\"):  # Pydantic model\n",
    "            serialized_messages.append(m.dict())\n",
    "        else:\n",
    "            serialized_messages.append(str(m))  # fallback to string\n",
    "\n",
    "    # Extract tools\n",
    "    tools = []\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    entry = {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": getattr(agent, \"_instructions\", \"\"),\n",
    "        \"model_name\": getattr(agent.model, \"model_name\", str(agent.model)),\n",
    "        \"tools\": tools,\n",
    "        \"messages\": serialized_messages,\n",
    "        \"source\": source,\n",
    "    }\n",
    "\n",
    "    ts_str = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, ensure_ascii=False, default=serializer)\n",
    "\n",
    "    print(f\"✅ Logged to {filepath}\")\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f7308637-05f0-422d-a00c-04cdd11bd099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: The documentation does not provide specific dates for homework deadlines. It suggests using a Makefile to manage and verify your code before submission. If you need exact deadlines, you may want to check the course schedule or announcements directly related to the homework.\n",
      "✅ Logged to logs\\docs_agent_20250930_152313_495ae2.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('logs/docs_agent_20250930_152313_495ae2.json')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"When are the deadlines for the homework?\"\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(\"Final Answer:\", result.output)\n",
    "\n",
    "log_interaction_to_file(agent, result.new_messages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e26c8737-1d0f-4f0c-9e5a-92dbd3daf113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "            if kind == 'user-prompt': del part['timestamp']\n",
    "            if kind == 'tool-call': del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']; del part['metadata']; del part['timestamp']\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text': del part['id']\n",
    "            parts.append(part)\n",
    "        message = {'kind': m['kind'], 'parts': parts}\n",
    "        log_simplified.append(message)\n",
    "    return log_simplified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a1c24dac-ebf4-4f37-8768-33e0ac9acdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "Checklist:\n",
    "- instructions_follow: Did the agent follow the user's instructions?\n",
    "- instructions_avoid: Did the agent avoid forbidden actions?\n",
    "- answer_relevant: Does the answer address the question?\n",
    "- answer_clear: Is it clear and correct?\n",
    "- answer_citations: Are references included?\n",
    "- completeness: Does it cover all key aspects?\n",
    "- tool_call_search: Was the search tool used?\n",
    "\n",
    "Output true/false for each check and provide a short justification.\n",
    "\"\"\".strip()\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n",
    "\n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-nano',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n",
    "\n",
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1d92e76a-72d8-4adc-8ee9-983f8a301a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data\n",
    "\n",
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f263d854-b97e-48f2-b6da-4d228674288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250930_152048.json\n",
      "20250930_152136.json\n",
      "20250930_152226.json\n",
      "docs_agent_20250930_152313_495ae2.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path('./logs')\n",
    "for f in LOG_DIR.glob('*.json'):\n",
    "    print(f.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ed8ba4a-66d2-4bd1-940c-9f0b513ea6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = sorted(LOG_DIR.glob('*.json'), reverse=True)  # latest first\n",
    "log_record = load_log_file(log_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "351fcd13-8937-47a8-94e1-0dde788af7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged to logs\\docs_agent_20250930_152635_777980.json\n",
      "Logged to: logs\\docs_agent_20250930_152635_777980.json\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)\n",
    "log_path = log_interaction_to_file(agent, result.new_messages())\n",
    "print(\"Logged to:\", log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c5f7bde2-ef53-4679-b87e-d5128877a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = './logs/docs_agent_20250930_152635_777980.json'\n",
    "log_record = load_log_file(log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bd1fe12b-a2bd-46e0-836a-19580bbf1764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ModelRequest(parts=[UserPromptPart(content='How do I install Kafka in Python?', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 29, 812108, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\nUse the text_search tool when you need to look up information.')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "log_file = './logs/docs_agent_20250930_152635_777980.json'\n",
    "with open(log_file, 'r', encoding='utf-8') as f:\n",
    "    log_record = json.load(f)\n",
    "\n",
    "# Check the first message\n",
    "print(type(log_record['messages']))\n",
    "print(log_record['messages'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b12b57d9-c1ee-4f45-a46c-2edade3b96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert datetime objects to ISO format\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "messages_dict = ModelMessagesTypeAdapter.dump_python(result.new_messages())\n",
    "\n",
    "with open(\"log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(messages_dict, f, indent=2, ensure_ascii=False, default=serializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6c23d3e6-349f-45bd-91aa-8c413be925cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"log.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    log_record = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "819eaee7-de38-4d48-a314-e740e90156af",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = log_record\n",
    "\n",
    "# Extract question and answer\n",
    "question = messages[0]['parts'][0]['content']\n",
    "answer = messages[-1]['parts'][0]['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c9918b2e-7d4c-4485-ae08-537596576482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your agent object directly, not result.agent\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=agent._instructions,  # system prompt from the agent\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=json.dumps(messages)  # or a simplified version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "21ced491-a96c-493d-90ee-e174dacf3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool-assisted evaluation suggested the answer is correct and concise, but it lacks citations and broader guidance (e.g., usage examples, alternatives like kafka-python).\n",
      "check_name='instructions_follow' justification=\"We attempted to adhere to the user's instruction to answer about DataTalks.Club documentation and used the text_search lookup as part of preparing a precise answer.\" check_pass=True\n",
      "check_name='instructions_avoid' justification='No disallowed actions detected; content is technical guidance.' check_pass=True\n",
      "check_name='answer_relevant' justification='The answer provides information on installing Kafka in Python and mentions confluent-kafka as the library.' check_pass=True\n",
      "check_name='answer_clear' justification='The answer is clear and actionable (pip install confluent-kafka).' check_pass=True\n",
      "check_name='answer_citations' justification='No external citations are included in the answer. If DataTalks.Club docs require citations, they are not included in this short answer.' check_pass=False\n",
      "check_name='completeness' justification='Provides the basic installation command but could mention usage and alternatives.' check_pass=True\n",
      "check_name='tool_call_search' justification='A text_search was used to fetch information prior to composing the answer.' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation agent\n",
    "eval_result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "# Access the Pydantic output\n",
    "checklist = eval_result.output\n",
    "\n",
    "# Print summary and checks\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "17d5c42d-574c-4d63-982e-bddaeecec1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Summary: Tool-assisted evaluation suggested the answer is correct and concise, but it lacks citations and broader guidance (e.g., usage examples, alternatives like kafka-python).\n",
      "instructions_follow: True - We attempted to adhere to the user's instruction to answer about DataTalks.Club documentation and used the text_search lookup as part of preparing a precise answer.\n",
      "instructions_avoid: True - No disallowed actions detected; content is technical guidance.\n",
      "answer_relevant: True - The answer provides information on installing Kafka in Python and mentions confluent-kafka as the library.\n",
      "answer_clear: True - The answer is clear and actionable (pip install confluent-kafka).\n",
      "answer_citations: False - No external citations are included in the answer. If DataTalks.Club docs require citations, they are not included in this short answer.\n",
      "completeness: True - Provides the basic installation command but could mention usage and alternatives.\n",
      "tool_call_search: True - A text_search was used to fetch information prior to composing the answer.\n"
     ]
    }
   ],
   "source": [
    "# Access structured output\n",
    "checklist = eval_result.output\n",
    "\n",
    "# Print overall summary\n",
    "print(\"Evaluation Summary:\", checklist.summary)\n",
    "\n",
    "# Print each individual check\n",
    "for check in checklist.checklist:\n",
    "    print(f\"{check.check_name}: {check.check_pass} - {check.justification}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6026ef73-9721-4020-9cad-c7f5eb7577ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 3 column 20 (char 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m eval_results = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m log_file \u001b[38;5;129;01min\u001b[39;00m LOG_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33m*.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     log_record = \u001b[43mload_log_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     user_prompt = create_user_prompt_from_log(log_record)  \u001b[38;5;66;03m# use your formatting function\u001b[39;00m\n\u001b[32m      6\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mload_log_file\u001b[39m\u001b[34m(log_file)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_log_file\u001b[39m(log_file):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         log_data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m         log_data[\u001b[33m'\u001b[39m\u001b[33mlog_file\u001b[39m\u001b[33m'\u001b[39m] = log_file\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m log_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 3 column 20 (char 66)"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_file in LOG_DIR.glob(\"*.json\"):\n",
    "    log_record = load_log_file(log_file)\n",
    "    user_prompt = create_user_prompt_from_log(log_record)  # use your formatting function\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    eval_results.append((log_file.name, result.output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "218927a8-7e9f-449a-b6cf-df285052189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for log_name, checklist in eval_results:\n",
    "    row = {\"file\": log_name}\n",
    "    for check in checklist.checklist:\n",
    "        row[check.check_name] = check.check_pass\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n",
    "print(df.mean(numeric_only=True))  # average pass rate for each check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "de76d1b6-0e6c-4478-bb9b-746243f31a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"timestamp\": \"2025-09-30T15:20:48.857088\",\n",
      "  \"system_prompt\": \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "835d5cab-96cf-4f55-8f99-a584effb7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_dict = ModelMessagesTypeAdapter.dump_python(result.new_messages())\n",
    "log_data = {\n",
    "    \"messages\": messages_dict,\n",
    "    \"system_prompt\": result.agent.instructions if hasattr(result, \"agent\") else \"No instructions\"\n",
    "}\n",
    "\n",
    "with open(\"log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(log_data, f, indent=2, ensure_ascii=False, default=serializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a12b9fd8-4ed8-4e03-971e-07e62b2b1a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"timestamp\": \"2025-09-30T15:20:48.857088\",\n",
      "  \"system_prompt\": \n"
     ]
    }
   ],
   "source": [
    "with open(log_file, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "print(content[:500])  # preview first 500 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5e08d679-cdbc-47f1-ad4a-d9021d8d6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ModelRequest(parts=[UserPromptPart(content='How do I install Kafka in Python?', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 29, 812108, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\nUse the text_search tool when you need to look up information.')\n"
     ]
    }
   ],
   "source": [
    "log_record = load_log_file(\"logs/docs_agent_20250930_152635_777980.json\")\n",
    "messages = log_record['messages']\n",
    "print(type(messages))       # should be list\n",
    "print(messages[0])          # see what the first item actually is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c4fc8ad8-4ad1-4605-b645-64ef236691e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string messages to proper format\n",
    "fixed_messages = []\n",
    "for m in messages:\n",
    "    if isinstance(m, str):\n",
    "        fixed_messages.append({\n",
    "            \"source\": \"user\",\n",
    "            \"parts\": [{\"content\": m, \"timestamp\": None}]\n",
    "        })\n",
    "    else:\n",
    "        fixed_messages.append(m)\n",
    "\n",
    "messages = fixed_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ae3e3338-fc11-4db7-b292-a4e64f17cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = messages[0]['parts'][0]['content']\n",
    "answer = messages[-1]['parts'][0]['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "50cb3a30-33c4-4d54-bc05-03e00c05711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ModelRequest(parts=[UserPromptPart(content='How do I install Kafka in Python?', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 29, 812108, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\nUse the text_search tool when you need to look up information.')\n",
      "Answer: ModelResponse(parts=[TextPart(content='To install Kafka in Python, you can use the `confluent-kafka` library. You can install it using pip with the following command:\\n\\n```bash\\npip install confluent-kafka\\n``` \\n\\nThis library provides a Python client for Apache Kafka.')], usage=RequestUsage(input_tokens=168, output_tokens=53, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 34, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-CLWXKmGOPWsxhuazyWmufplSRPaJW', finish_reason='stop')\n"
     ]
    }
   ],
   "source": [
    "log_record = load_log_file(\"logs/docs_agent_20250930_152635_777980.json\")\n",
    "messages = log_record['messages']\n",
    "\n",
    "# Fix messages if they are strings\n",
    "fixed_messages = []\n",
    "for m in messages:\n",
    "    if isinstance(m, str):\n",
    "        fixed_messages.append({\n",
    "            \"source\": \"user\",\n",
    "            \"parts\": [{\"content\": m, \"timestamp\": None}]\n",
    "        })\n",
    "    else:\n",
    "        fixed_messages.append(m)\n",
    "\n",
    "messages = fixed_messages\n",
    "\n",
    "# Now safely get question and answer\n",
    "question = messages[0]['parts'][0]['content']\n",
    "answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "188aeb8f-a4be-4104-945d-ab6bc3f92436",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"\"\"\n",
    "Instructions: {log_record['system_prompt']}\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Log: {json.dumps(messages, indent=2, ensure_ascii=False)}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5588011b-b929-4a54-8ff6-c88c043d2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bfbb4670-b085-46df-bbc6-529ae19159b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__class_getitem__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_is_protocol', '_new_message_index', '_output_tool_name', '_set_output_tool_return', '_state', '_traceparent', '_traceparent_value', 'all_messages', 'all_messages_json', 'new_messages', 'new_messages_json', 'output', 'timestamp', 'usage']\n",
      "{'_new_message_index': 0,\n",
      " '_output_tool_name': 'final_result',\n",
      " '_state': GraphAgentState(message_history=[ModelRequest(parts=[UserPromptPart(content='\\nInstructions: You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\nUse the text_search tool when you need to look up information.\\n\\nQuestion: ModelRequest(parts=[UserPromptPart(content=\\'How do I install Kafka in Python?\\', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 29, 812108, tzinfo=datetime.timezone.utc))], instructions=\\'You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\\\nUse the text_search tool when you need to look up information.\\')\\nAnswer: ModelResponse(parts=[TextPart(content=\\'To install Kafka in Python, you can use the `confluent-kafka` library. You can install it using pip with the following command:\\\\n\\\\n```bash\\\\npip install confluent-kafka\\\\n``` \\\\n\\\\nThis library provides a Python client for Apache Kafka.\\')], usage=RequestUsage(input_tokens=168, output_tokens=53, details={\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}), model_name=\\'gpt-4o-mini-2024-07-18\\', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 34, tzinfo=TzInfo(UTC)), provider_name=\\'openai\\', provider_details={\\'finish_reason\\': \\'stop\\'}, provider_response_id=\\'chatcmpl-CLWXKmGOPWsxhuazyWmufplSRPaJW\\', finish_reason=\\'stop\\')\\n\\nLog: [\\n  {\\n    \"source\": \"user\",\\n    \"parts\": [\\n      {\\n        \"content\": \"ModelRequest(parts=[UserPromptPart(content=\\'How do I install Kafka in Python?\\', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 29, 812108, tzinfo=datetime.timezone.utc))], instructions=\\'You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\\\\\\\nUse the text_search tool when you need to look up information.\\')\",\\n        \"timestamp\": null\\n      }\\n    ]\\n  },\\n  {\\n    \"source\": \"user\",\\n    \"parts\": [\\n      {\\n        \"content\": \"ModelResponse(parts=[ToolCallPart(tool_name=\\'text_search\\', args=\\'{\\\\\"query\\\\\":\\\\\"install Kafka in Python\\\\\"}\\', tool_call_id=\\'call_mKevgziJrxbmmzsZqcWrhR0d\\')], usage=RequestUsage(input_tokens=124, output_tokens=17, details={\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}), model_name=\\'gpt-4o-mini-2024-07-18\\', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 31, tzinfo=TzInfo(UTC)), provider_name=\\'openai\\', provider_details={\\'finish_reason\\': \\'tool_calls\\'}, provider_response_id=\\'chatcmpl-CLWXHQnCKB7Vw9znLaU2sX3I7B5Ia\\', finish_reason=\\'tool_call\\')\",\\n        \"timestamp\": null\\n      }\\n    ]\\n  },\\n  {\\n    \"source\": \"user\",\\n    \"parts\": [\\n      {\\n        \"content\": \"ModelRequest(parts=[ToolReturnPart(tool_name=\\'text_search\\', content=[\\'Kafka can be installed in Python using the `confluent-kafka` library.\\'], tool_call_id=\\'call_mKevgziJrxbmmzsZqcWrhR0d\\', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 33, 172419, tzinfo=datetime.timezone.utc))], instructions=\\'You are a helpful assistant that answers questions about the DataTalks.Club documentation.\\\\\\\\nUse the text_search tool when you need to look up information.\\')\",\\n        \"timestamp\": null\\n      }\\n    ]\\n  },\\n  {\\n    \"source\": \"user\",\\n    \"parts\": [\\n      {\\n        \"content\": \"ModelResponse(parts=[TextPart(content=\\'To install Kafka in Python, you can use the `confluent-kafka` library. You can install it using pip with the following command:\\\\\\\\n\\\\\\\\n```bash\\\\\\\\npip install confluent-kafka\\\\\\\\n``` \\\\\\\\n\\\\\\\\nThis library provides a Python client for Apache Kafka.\\')], usage=RequestUsage(input_tokens=168, output_tokens=53, details={\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}), model_name=\\'gpt-4o-mini-2024-07-18\\', timestamp=datetime.datetime(2025, 9, 30, 15, 26, 34, tzinfo=TzInfo(UTC)), provider_name=\\'openai\\', provider_details={\\'finish_reason\\': \\'stop\\'}, provider_response_id=\\'chatcmpl-CLWXKmGOPWsxhuazyWmufplSRPaJW\\', finish_reason=\\'stop\\')\",\\n        \"timestamp\": null\\n      }\\n    ]\\n  }\\n]\\n', timestamp=datetime.datetime(2025, 9, 30, 15, 41, 1, 562541, tzinfo=datetime.timezone.utc))], instructions=\"Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\\nWe also include the entire log (<LOG>) for analysis.\\n\\nChecklist:\\n- instructions_follow: Did the agent follow the user's instructions?\\n- instructions_avoid: Did the agent avoid forbidden actions?\\n- answer_relevant: Does the answer address the question?\\n- answer_clear: Is it clear and correct?\\n- answer_citations: Are references included?\\n- completeness: Does it cover all key aspects?\\n- tool_call_search: Was the search tool used?\\n\\nOutput true/false for each check and provide a short justification.\"),\n",
      "                                            ModelResponse(parts=[ToolCallPart(tool_name='final_result', args='{\"checklist\": [{\"check_name\": \"instructions_follow\", \"justification\": \"The assistant will call a search tool as required by the developer instruction; after tool results, it will provide an answer. The final content is prepared to answer the user\\'s question.\", \"check_pass\": true}], \"summary\": \"Triggered a text search as required by the developer instruction to fetch DataTalks.Club docs on Kafka installation for Python. Awaiting tool results.\"}', tool_call_id='call_RUk1LxwAy8IMaqMpf22BFNOb')], usage=RequestUsage(input_tokens=1346, output_tokens=1530, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1408, 'rejected_prediction_tokens': 0}), model_name='gpt-5-nano-2025-08-07', timestamp=datetime.datetime(2025, 9, 30, 15, 41, 4, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CLWlMHoak0LmWH4wHXMwHcDqT2UwG', finish_reason='tool_call'),\n",
      "                                            ModelRequest(parts=[ToolReturnPart(tool_name='final_result', content='Final result processed.', tool_call_id='call_RUk1LxwAy8IMaqMpf22BFNOb', timestamp=datetime.datetime(2025, 9, 30, 15, 41, 12, 380777, tzinfo=datetime.timezone.utc))])],\n",
      "                           usage=RunUsage(input_tokens=1346, output_tokens=1530, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1408, 'rejected_prediction_tokens': 0}, requests=1),\n",
      "                           retries=0,\n",
      "                           run_step=1),\n",
      " '_traceparent_value': None,\n",
      " 'output': EvaluationChecklist(checklist=[EvaluationCheck(check_name='instructions_follow', justification=\"The assistant will call a search tool as required by the developer instruction; after tool results, it will provide an answer. The final content is prepared to answer the user's question.\", check_pass=True)], summary='Triggered a text search as required by the developer instruction to fetch DataTalks.Club docs on Kafka installation for Python. Awaiting tool results.')}\n"
     ]
    }
   ],
   "source": [
    "# Just print its attributes\n",
    "print(dir(eval_result))\n",
    "# Or a nicer view\n",
    "import pprint\n",
    "pprint.pprint(eval_result.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e42b89e5-e83d-46df-b28f-337cb5dc797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Triggered a text search as required by the developer instruction to fetch DataTalks.Club docs on Kafka installation for Python. Awaiting tool results.\n",
      "\n",
      "Checklist:\n",
      "- instructions_follow: True (The assistant will call a search tool as required by the developer instruction; after tool results, it will provide an answer. The final content is prepared to answer the user's question.)\n"
     ]
    }
   ],
   "source": [
    "# Access the checklist and summary properly\n",
    "evaluation = eval_result.output  # This is an EvaluationChecklist\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary:\")\n",
    "print(evaluation.summary)\n",
    "\n",
    "# Print each check\n",
    "print(\"\\nChecklist:\")\n",
    "for check in evaluation.checklist:\n",
    "    print(f\"- {check.check_name}: {check.check_pass} ({check.justification})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d779a-bbe8-441b-82d6-ac258bdf8f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
